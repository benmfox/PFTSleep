{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations\n",
    "\n",
    "> Because this will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np, torch, torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from torch.distributions.beta import Beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def create_patch(xb, patch_len, stride, return_patch_num=False, constant_pad=False, constant_pad_value=0, max_seq_len=None):\n",
    "    \"\"\"\n",
    "    xb: [bs x n_vars x seq_len]\n",
    "    \"\"\"\n",
    "    seq_len = xb.shape[-1] if max_seq_len is None else max_seq_len\n",
    "    xb = xb[...,:seq_len]\n",
    "    patch_num = (max(seq_len, patch_len)-patch_len) // stride + 1  # in supervised tsai model - patch_num = int((seq_len - patch_len) / stride + 1) + 1\n",
    "    patch_len = seq_len if patch_len > seq_len else patch_len\n",
    "    if constant_pad:\n",
    "        # seq_len > patch_len and stride <= seq_len - patch_len\n",
    "        if ((seq_len-patch_len) % stride != 0):\n",
    "            # only pad if remainder\n",
    "            xb = F.pad(xb, (0, stride), 'constant', value=constant_pad_value) # pad at the end with value\n",
    "            patch_num += 1\n",
    "    dim = xb.dim()\n",
    "    dimension = 0\n",
    "    if dim > 3:\n",
    "        raise ValueError('Tensor has dimension gt 3, not sure how to patch.')\n",
    "    elif dim == 3:\n",
    "        xb = xb.permute(0,2,1) # xb: [bs x tgt_len x nvars]\n",
    "        dimension = 1\n",
    "    elif dim == 2:\n",
    "        xb = xb.permute(1,0)\n",
    "    xb = xb.unfold(dimension=dimension, size=patch_len, step=stride) # xb: [bs x num_patch x n_vars x patch_len]\n",
    "    if dim == 2:\n",
    "        xb = xb.transpose(0,1) #[bx x num_patch x patch_len]]\n",
    "    if return_patch_num:\n",
    "        return xb, patch_num\n",
    "    else:\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1000]), torch.Size([4, 1, 505]), torch.Size([4, 2, 500]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4, 1000)\n",
    "\n",
    "# test seq_len > patch len == stride \n",
    "xb = create_patch(x, patch_len=505, stride=500, constant_pad=False)\n",
    "xb_rep = create_patch(x, patch_len=500, stride=500, constant_pad=True)\n",
    "x.shape, xb.shape, xb_rep.shape\n",
    "#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 1350000]),\n",
       " torch.Size([1, 1318, 7, 1024]),\n",
       " torch.Size([1, 1319, 7, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(1,7,1350000)\n",
    "\n",
    "# test seq_len > patch len == stride \n",
    "xb = create_patch(x, patch_len=1024, stride=1024, constant_pad=False)\n",
    "xb_rep = create_patch(x, patch_len=1024, stride=1024, constant_pad=True)\n",
    "x.shape, xb.shape, xb_rep.shape\n",
    "#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unpatch(x, seq_len, remove_padding=True):\n",
    "    \"\"\"\n",
    "    x: [bs/None x patch_num x n_vars x patch_len]\n",
    "    returns x: [bs x n_vars x seq_len]\n",
    "    \"\"\"\n",
    "    if x.dim() == 3:\n",
    "        x = x.transpose(0,1)\n",
    "    else:\n",
    "        x = x.transpose(1,2)\n",
    "    x = x.flatten(start_dim=-2, end_dim=-1)\n",
    "    if remove_padding:\n",
    "        x = x[...,:seq_len]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 50]), torch.Size([1, 9, 1, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(1,1,50)\n",
    "\n",
    "# test seq_len > patch len == stride \n",
    "xb = create_patch(x, patch_len=6, stride=6, constant_pad=True)\n",
    "xb = unpatch(xb, seq_len=50, remove_padding=False)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mask_patches_simple(xb, # input tensor of size 3 or 4 to be masked\n",
    "                        mask_ratio # ratio of masking of patches\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Function that masks patches in a simple way\n",
    "\n",
    "    xb: [bs x patch_num x n_vars x patch_len]\n",
    "    padding_mask [bs x patch_num x 1|num_vars x patch_len]\n",
    "    \"\"\"\n",
    "    if xb.dim() == 3:\n",
    "        xb = xb.unsqueeze(0)\n",
    "    assert xb.dim() == 4, \"The simple masking function expects a dim == 4 input tensor with [bs x patch_num x n_vars x patch_len]\"\n",
    "    x = xb.clone()\n",
    "    mask_to_multiply = torch.from_numpy(np.random.choice([0,1], size=x.shape[:3], p=[mask_ratio,1-mask_ratio])).to(xb.device)\n",
    "    x = x * mask_to_multiply[...,None]\n",
    "    mask = (mask_to_multiply == 0).int()\n",
    "    return x,mask\n",
    "\n",
    "def random_masking(xb, mask_ratio):\n",
    "    # xb: [bs x num_patch x n_vars x patch_len]\n",
    "    bs, L, nvars, D = xb.shape\n",
    "    x = xb.clone()\n",
    "    \n",
    "    len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "    noise = torch.rand(bs, L, nvars,device=xb.device)  # noise in [0, 1], bs x L x nvars\n",
    "        \n",
    "    # sort noise for each sample\n",
    "    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)                                     # ids_restore: [bs x L x nvars]\n",
    "\n",
    "    # keep the first subset\n",
    "    ids_keep = ids_shuffle[:, :len_keep, :]                                              # ids_keep: [bs x len_keep x nvars]         \n",
    "    x_kept = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, 1, D))     # x_kept: [bs x len_keep x nvars  x patch_len]\n",
    "   \n",
    "    # removed x\n",
    "    x_removed = torch.zeros(bs, L-len_keep, nvars, D, device=xb.device)                 # x_removed: [bs x (L-len_keep) x nvars x patch_len]\n",
    "    x_ = torch.cat([x_kept, x_removed], dim=1)                                          # x_: [bs x L x nvars x patch_len]\n",
    "\n",
    "    # combine the kept part and the removed one\n",
    "    x_masked = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1,1,1,D)) # x_masked: [bs x num_patch x nvars x patch_len]\n",
    "\n",
    "    # generate the binary mask: 0 is keep, 1 is remove\n",
    "    mask = torch.ones([bs, L, nvars], device=x.device)                                  # mask: [bs x num_patch x nvars]\n",
    "    mask[:, :len_keep, :] = 0\n",
    "    # unshuffle to get the binary mask\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)                                  # [bs x num_patch x nvars]\n",
    "    return x_masked, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "x = torch.randn(50,16,7,50)\n",
    "mask_ratio = 0.4\n",
    "\n",
    "x_new, mask = mask_patches_simple(x,mask_ratio=mask_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_values(x, mask_ratio):\n",
    "    mask = torch.rand(x.shape, device=x.device) > mask_ratio # maskout_ratio are False, note that this is per channel masking\n",
    "    x = x*mask\n",
    "    return x\n",
    "\n",
    "def jitter_augmentation(x, mask_ratio=0.05, jitter_ratio=0.05):\n",
    "    x = x.clone()  \n",
    "    max_amplitude = x.abs().max()  # Use absolute max for amplitude scale\n",
    "\n",
    "    zero_mask = torch.rand(x.shape, device=x.device) > mask_ratio\n",
    "    jitter_mask = torch.rand(x.shape, device=x.device) > (1-jitter_ratio)\n",
    "    jitter_values = torch.randn(x.shape, device=x.device) * max_amplitude * 0.1  # Normal distribution centered at 0 times 10% of max amplitude\n",
    "    \n",
    "    x = x*zero_mask + jitter_mask*jitter_values\n",
    "    n_masks =  (~zero_mask).sum() + jitter_mask.sum()\n",
    "    return x, n_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "## note that the random number generator advances state...\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(4,7,1000)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x_new, n_masks = jitter_augmentation(x)\n",
    "n_masks /(4* 7*1000)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x_new2, n_masks2 = jitter_augmentation(x)\n",
    "torch.equal(x_new, x_new2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def shuffle_dim(x, dim=1, p=0.5):\n",
    "    \"\"\"\n",
    "    shuffles a dimension randomly along dim\n",
    "    x: [bs x n channels x n patches x patch len]\n",
    "    \"\"\"\n",
    "    if torch.rand(1, device=x.device) > (1-p):\n",
    "        idx = torch.randperm(x.size(dim), device=x.device)\n",
    "        return torch.index_select(x, dim, idx)\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reverse_sequence(x, seq_dim=(-1,), p=0.5):\n",
    "    if torch.rand(1, device=x.device) > (1-p):\n",
    "        return torch.flip(x, dims=seq_dim)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,1,5,5).to('cuda')\n",
    "\n",
    "torch.equal(shuffle_dim(x), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IntraClassCutMix1d(Callback):\n",
    "    \"\"\"\n",
    "    Intra-class CutMix for 1D data (e.g., time-series). \n",
    "\n",
    "    This is a callback that can be used to apply CutMix to the training data.\n",
    "    It is used to mix segments within the same class.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 mix_prob=0.5, # probability of applying cutmix\n",
    "                 return_y_every_sec=30, # length of segment to mix, if one value of y corresponds to 30 seconds of signal data, this should be set to 30. \n",
    "                 frequency=125, # frequency of the data\n",
    "                 return_sequence_padding_mask=True # whether to return the sequence padding mask\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.mix_prob = mix_prob # probability of applying cutmix\n",
    "        self.segment_length = return_y_every_sec # length of segment to mix\n",
    "        self.frequency = frequency # frequency of the data\n",
    "        self.return_sequence_padding_mask = return_sequence_padding_mask # whether to return the sequence padding mask\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "        if self.return_sequence_padding_mask:\n",
    "            x, y, padding_mask = batch  # x: [batch_size, channels, time_steps], y: [batch_size, time_steps]\n",
    "        else:\n",
    "            x, y = batch  # x: [batch_size, channels, time_steps], y: [batch_size, time_steps]\n",
    "\n",
    "        if torch.rand(1) < self.mix_prob:\n",
    "            # Group indices by class\n",
    "            #class_indices = {cls.item(): (y == cls).nonzero(as_tuple=True) for cls in y.unique()}\n",
    "\n",
    "            # Number of segments\n",
    "            n_segments = y.shape[-1]\n",
    "\n",
    "            for seg in range(n_segments):\n",
    "                start = seg\n",
    "                end = seg + 1\n",
    "\n",
    "                start_x = start*self.segment_length*self.frequency\n",
    "                end_x = end*self.segment_length*self.frequency\n",
    "                # Get the classes for this segment across all batch items\n",
    "                seg_classes = y[:, start].cpu()  # Assuming the class is consistent within a segment\n",
    "\n",
    "                for cls in seg_classes.unique():\n",
    "                    # Find batch items with this class in the current segment\n",
    "                    batch_indices = (seg_classes == cls).nonzero().squeeze(-1)\n",
    "                    if len(batch_indices) > 1:  # We need at least 2 samples to mix\n",
    "                        # Shuffle these indices\n",
    "                        shuffled_indices = batch_indices[torch.randperm(len(batch_indices))]\n",
    "                        \n",
    "                        # Mix the segments\n",
    "                        x[batch_indices, :, start_x:end_x] = x[shuffled_indices, :, start_x:end_x]\n",
    "       \n",
    "        return (x, y) if not self.return_sequence_padding_mask else (x, y, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,7,90)\n",
    "x_c = x.clone()\n",
    "y = torch.randint(0, 5, size=(4,90//30))\n",
    "xxt = IntraClassCutMix1d(mix_prob=1, frequency=1, return_y_every_sec=30, return_sequence_padding_mask=False)\n",
    "batch = (x,y)\n",
    "xxt.on_train_batch_start(None, None, batch, 0)\n",
    "torch.equal(x_c, batch[0]) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IntraClassCutMixBatch(Callback):\n",
    "    \"\"\"\n",
    "    Intra-class CutMix for 1D data (e.g., time-series). \n",
    "\n",
    "    This is a callback that can be used to apply CutMix to the training data.\n",
    "    It is used to mix segments within the same class.\n",
    "\n",
    "    This is different to IntraClassCutMix1d in that it mixes segments of the same class across batches of data, rather than just at the same segment\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 mix_prob=0.5, # probability of applying cutmix\n",
    "                 return_y_every_sec=30, # length of segment to mix, if one value of y corresponds to 30 seconds of signal data, this should be set to 30. \n",
    "                 frequency=125, # frequency of the data\n",
    "                 return_sequence_padding_mask=True, # whether to return the sequence padding mask\n",
    "                 intra_class_only=True # whether to mix only within same class (True) or across all classes (False)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.mix_prob = mix_prob # probability of applying cutmix\n",
    "        self.frequency = frequency # frequency of the data\n",
    "        self.segment_length = return_y_every_sec*self.frequency # length of segment to mix\n",
    "        self.return_sequence_padding_mask = return_sequence_padding_mask # whether to return the sequence padding mask\n",
    "        self.intra_class_only = intra_class_only # whether to mix only within same class (True) or across all classes (False)\n",
    "    \n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "        if self.return_sequence_padding_mask:\n",
    "            x, y, padding_mask = batch  # x: [batch_size, channels, time_steps], y: [batch_size, time_steps]\n",
    "        else:\n",
    "            x, y = batch  # x: [batch_size, channels, time_steps], y: [batch_size, time_steps]\n",
    "        seq_len = x.shape[-1]\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            mode = \"intra-class\" if self.intra_class_only else \"cross-class\"\n",
    "            print(f\"{mode} CutMixBatch is being applied!\")\n",
    "\n",
    "        if torch.rand(1) < self.mix_prob:\n",
    "            # Group indices by class\n",
    "            #class_indices = {cls.item(): (y == cls).nonzero(as_tuple=True) for cls in y.unique()}\n",
    "\n",
    "            # create patches of length segment_length, [bs x n_patches x n_vars x patch_len]\n",
    "            x = create_patch(x, patch_len=self.segment_length, stride=self.segment_length, return_patch_num=False, constant_pad=False, constant_pad_value=0)\n",
    "            if self.intra_class_only:\n",
    "                unique_classes = y.unique()\n",
    "                for cls in unique_classes:\n",
    "                    class_indices = (y == cls).nonzero()\n",
    "                    if len(class_indices) > 1:\n",
    "                        # shuffle the indices\n",
    "                        shuffled_indices = class_indices[torch.randperm(len(class_indices))]\n",
    "                        # Extract the batch and patch indices\n",
    "                        batch_indices = class_indices[:, 0]\n",
    "                        patch_indices = class_indices[:, 1]\n",
    "                        # mix the segments\n",
    "                        x[batch_indices, patch_indices] = x[shuffled_indices[:, 0], shuffled_indices[:, 1]]\n",
    "            else:\n",
    "                # Cross-class mixing logic\n",
    "                all_indices = torch.nonzero(torch.ones_like(y))  # Get all valid indices\n",
    "                if len(all_indices) > 1:\n",
    "                    shuffled_indices = all_indices[torch.randperm(len(all_indices))]\n",
    "                    batch_indices = all_indices[:, 0]\n",
    "                    patch_indices = all_indices[:, 1]\n",
    "                    x[batch_indices, patch_indices] = x[shuffled_indices[:, 0], shuffled_indices[:, 1]]\n",
    "                    y[batch_indices, patch_indices] = y[shuffled_indices[:, 0], shuffled_indices[:, 1]]\n",
    "            # unpatch the data\n",
    "            x = unpatch(x, seq_len)\n",
    "        return (x, y) if not self.return_sequence_padding_mask else (x, y, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intra-class CutMixBatch is being applied!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,7,90)\n",
    "x_c = x.clone()\n",
    "y = torch.randint(0, 5, size=(4,90//30))\n",
    "xxt = IntraClassCutMixBatch(mix_prob=1, frequency=1, return_y_every_sec=30, return_sequence_padding_mask=False)\n",
    "batch = (x,y)\n",
    "batch = xxt.on_train_batch_start(None, None, batch, 0)\n",
    "torch.equal(x_c, batch[0]) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MixupCallback(Callback):\n",
    "    \"\"\"\n",
    "    Mixup for 1D data (e.g., time-series).\n",
    "\n",
    "    This callback applies Mixup to the training data, blending both the input data and the labels.\n",
    "\n",
    "    See tsai implementation here: https://github.com/timeseriesAI/tsai/blob/bdff96cc8c4c8ea55bc20d7cffd6a72e402f4cb2/tsai/data/mixed_augmentation.py#L43\n",
    "\n",
    "    Note that this creates non-integer labels/soft labels. Loss functions should be able to handle this.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_classes,\n",
    "                 mixup_alpha=0.4, # alpha parameter for the beta distribution\n",
    "                 return_sequence_padding_mask=True, # whether to return the sequence padding mask\n",
    "                 ignore_index=-100 # ignore index\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.distrib = Beta(mixup_alpha, mixup_alpha)\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.return_sequence_padding_mask = return_sequence_padding_mask\n",
    "        self.ignore_index = ignore_index\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "        if self.return_sequence_padding_mask:\n",
    "            x, y, padding_mask = batch  # x: [batch_size, channels, time_steps], y: [batch_size, time_steps]\n",
    "        else:\n",
    "            x, y = batch  # x: [batch_size, channels, time_steps], y: [batch_size, time_steps]\n",
    "\n",
    "        # Sample lambda from a beta distribution\n",
    "        if batch_idx == 0:\n",
    "            print(\"Mixup is being applied!\")\n",
    "        lam = self.distrib.sample((x.size(0), )).to(x.device) # [bs]\n",
    "        # our mixing coefficient is always â‰¥ 0.5\n",
    "        lam = torch.max(lam, 1 - lam)\n",
    "        lam = lam.view(-1, 1, 1)  # for input shape [bs, channels, seq_len]\n",
    "\n",
    "        # Shuffle the batch\n",
    "        indices = torch.randperm(x.size(0), device=x.device)\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # create ignore masks\n",
    "        ignore_mask = (y == self.ignore_index)\n",
    "        ignore_mask_shuffled = (y_shuffled == self.ignore_index)\n",
    "        combined_ignore_mask = torch.logical_or(ignore_mask, ignore_mask_shuffled)\n",
    "\n",
    "        y_clean = torch.where(ignore_mask, torch.zeros_like(y), y)\n",
    "        y_shuffled_clean = torch.where(ignore_mask_shuffled, torch.zeros_like(y_shuffled), y_shuffled)\n",
    "        # Create one-hot encodings\n",
    "        y_onehot = F.one_hot(y_clean.long(), num_classes=self.num_classes).float()\n",
    "        y_shuffled_onehot = F.one_hot(y_shuffled_clean.long(), num_classes=self.num_classes).float()\n",
    "        # Zero out the one-hot vectors for ignored indices\n",
    "        y_onehot = torch.where(ignore_mask.unsqueeze(-1), torch.zeros_like(y_onehot), y_onehot)\n",
    "        y_shuffled_onehot = torch.where(ignore_mask_shuffled.unsqueeze(-1), torch.zeros_like(y_shuffled_onehot), y_shuffled_onehot)\n",
    "\n",
    "        # Mixup the inputs and labels\n",
    "        x_mixed = torch.lerp(x_shuffled, x, lam) # x = lam * x + (1 - lam) * x_shuffled\n",
    "        y_mixed = torch.lerp(y_shuffled_onehot, y_onehot, lam) # y = lam * y + (1 - lam) * y_shuffled\n",
    "\n",
    "        # finally assign all probabilities with a -100 mixing to 0\n",
    "        y_mixed = torch.where(combined_ignore_mask.unsqueeze(-1), torch.zeros_like(y_mixed), y_mixed)\n",
    "        # # add back in the ignore index where it was mixed with other labels\n",
    "        # combined_ignore_mask = torch.logical_or(ignore_mask, ignore_mask_shuffled)\n",
    "        # if combined_ignore_mask.any():\n",
    "        #     # y_mixed = torch.where(combined_ignore_mask, \n",
    "        #     #                         torch.tensor(self.ignore_index, dtype=y_mixed.dtype, device=y_mixed.device), \n",
    "        #     #                     y_mixed)\n",
    "        #     y_mixed = torch.where(\n",
    "        #         combined_ignore_mask.unsqueeze(-1),\n",
    "        #         torch.zeros_like(y_mixed),\n",
    "        #         y_mixed\n",
    "        #     )\n",
    "        y_mixed = y_mixed.permute(0,2,1) # [bs x n_classes x n_patches]\n",
    "        if self.return_sequence_padding_mask:\n",
    "            # Mix the padding masks using OR operation (1 means padded)\n",
    "            padding_mask_shuffled = padding_mask[indices]\n",
    "            # If either mask indicates padding (1), the result should be padded\n",
    "            padding_mask = torch.logical_or(padding_mask, padding_mask_shuffled)\n",
    "            batch[0], batch[1], batch[2] = x_mixed, y_mixed, padding_mask\n",
    "            #return (x_mixed, y_mixed, padding_mask)\n",
    "        else:\n",
    "            batch[0], batch[1] = x_mixed, y_mixed\n",
    "            #return (x_mixed, y_mixed)\n",
    "        #return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixup is being applied!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,7,90)\n",
    "x_c = x.clone()\n",
    "y_og = torch.randint(0, 5, size=(4,90//30))\n",
    "y_og[1,2] = -100\n",
    "y_og[2,1] = -100\n",
    "y_c = y_og.clone()\n",
    "xxt = MixupCallback(num_classes=5, mixup_alpha=0.4, return_sequence_padding_mask=False)\n",
    "batch = (x,y_og)\n",
    "batch = xxt.on_train_batch_start(None, None, batch, 0)\n",
    "torch.equal(x_c, batch[0]) == False, torch.equal(y_c, batch[1]) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
