{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79652227",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "> A good use of time, no doubt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0510a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da62865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import glob, os\n",
    "from nbdev.showdoc import *\n",
    "from pftsleep.slumber import SelfSupervisedTimeFrequencyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aedfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import warnings\n",
    "from pftsleep.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522284f6",
   "metadata": {},
   "source": [
    "## Time Series Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSTEncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, # dimension of patch embeddings\n",
    "                 n_heads, # number of attention heads per layer\n",
    "                 d_ff=256, # dimension of feedforward layer in each transformer layer\n",
    "                 store_attn=False, # indicator of whether or not to store attention\n",
    "                 norm='BatchNorm',\n",
    "                 relative_attn_type='vanilla', # options include vaniall or eRPE\n",
    "                 use_flash_attn=False, # indicator to use flash attention\n",
    "                 num_patches=None, # num patches required for eRPE attn\n",
    "                 attn_dropout=0, \n",
    "                 dropout=0., \n",
    "                 bias=True, \n",
    "                 activation=\"gelu\", \n",
    "                 res_attention=False, \n",
    "                 pre_norm=False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads\n",
    "        d_v = d_model // n_heads\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.use_flash_attn = use_flash_attn\n",
    "        self.store_attn = store_attn\n",
    "        if self.use_flash_attn and self.store_attn:\n",
    "            warnings.warn(\"Flash attention does not support storing attention, setting store_attn to False\")\n",
    "            self.store_attn = False\n",
    "        if relative_attn_type == 'eRPE':\n",
    "            assert num_patches is not None, \"You must provide a num_patches for eRPE\"\n",
    "            self.self_attn = Attention_Rel_Scl(d_model, n_heads=n_heads, seq_len=num_patches, res_attention=res_attention, attn_dropout=attn_dropout, proj_dropout=dropout)\n",
    "        else:\n",
    "            self.self_attn = MultiheadAttentionCustom(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        if use_flash_attn:\n",
    "            self.self_attn = MultiheadFlashAttention(d_model, n_heads, attn_dropout=attn_dropout, proj_dropout=dropout)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = nn.Dropout(dropout) \n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias), \n",
    "                                get_activation_fn(activation), # note do not put functions in sequential, it makes things non-deterministic\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        src: tensor [bs x q_len x d_model]\n",
    "        \"\"\"\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, prev=prev, key_padding_mask=key_padding_mask)\n",
    "        elif not self.use_flash_attn:\n",
    "            src2, attn = self.self_attn(src, key_padding_mask=key_padding_mask)\n",
    "        else:\n",
    "            src2 = self.self_attn(src, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        if self.store_attn:\n",
    "            self.attn = attn\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "\n",
    "        src2 = self.ff(src)\n",
    "\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        if self.res_attention:\n",
    "            return src, scores\n",
    "        else:\n",
    "            return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b6756",
   "metadata": {},
   "source": [
    "## Patch Time Series and Frequency Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchTFTSimple(nn.Module):\n",
    "     def __init__(self,\n",
    "                  c_in:int, # the number of input channels\n",
    "                  win_length, # the length of the patch of time/interval or short time ft windown length (when time_domain=False)\n",
    "                  hop_length, # the length of the distance between each patch/fft\n",
    "                  max_seq_len, # maximum sequence len\n",
    "                  time_domain=True,\n",
    "                  pos_encoding_type='learned', # options include learned or tAPE\n",
    "                  relative_attn_type='vanilla', # options include vanilla or eRPE\n",
    "                  use_flash_attn=False, # indicator to use flash attention\n",
    "                  use_revin=True, # if time_domain is true, whether or not to instance normalize time data\n",
    "                  dim1reduce=False, # indicator to normalize by timepoint in revin\n",
    "                  affine=True, # if time_domain is true, whether or not to learn revin normalization parameters \n",
    "                  mask_ratio=0.1, # amount of signal to mask\n",
    "                  augmentations=['patch_mask', 'jitter_zero_mask', 'reverse_sequence', 'shuffle_channels'], # the type of mask to use, options are patch or jitter_zero\n",
    "                  n_layers:int=2, # the number of transformer encoder layers to use\n",
    "                  d_model=512, # the dimension of the input to the transofmrer encoder\n",
    "                  n_heads=2, # the number of heads in each layer\n",
    "                  shared_embedding=False, # indicator for whether or not each channel should be projected with its own set of linear weights to the encoder dimension\n",
    "                  d_ff:int=2048, # the feedforward layer size in the transformer\n",
    "                  norm:str='BatchNorm', # BatchNorm or LayerNorm during trianing\n",
    "                  attn_dropout:float=0., # dropout in attention\n",
    "                  dropout:float=0.1, # dropout for linear layers\n",
    "                  act:str=\"gelu\", # activation function\n",
    "                  res_attention:bool=True, # whether to use residual attention\n",
    "                  pre_norm:bool=False, # indicator to pre batch or layer norm \n",
    "                  store_attn:bool=False, # indicator to store attention\n",
    "                  pretrain_head=True, # indicator to include a pretraining head\n",
    "                  pretrain_head_n_layers=1, # how many linear layers on the pretrained head\n",
    "                  pretrain_head_dropout=0., # dropout applied to pretrain head\n",
    "                  ):\n",
    "          super().__init__()\n",
    "          self.c_in = c_in # original c_in without convolution\n",
    "          self.shared_embedding = shared_embedding\n",
    "          self.d_model = d_model # original d_model\n",
    "          self.use_revin = use_revin\n",
    "          self.affine = affine\n",
    "          self.time_domain = time_domain\n",
    "          self.pretrain_head = pretrain_head\n",
    "          self.use_flash_attn = use_flash_attn\n",
    "          if use_flash_attn and res_attention:\n",
    "               warnings.warn(\"Flash attention is not yet implemented for residual attention, setting res_attention=False\")\n",
    "               res_attention = False\n",
    "          # Instance Normalization (full sequence)\n",
    "          if self.use_revin:\n",
    "               self.revin = RevIN(num_features=self.c_in, affine=self.affine, dim_to_reduce=1 if dim1reduce else -1)\n",
    "          self.num_patch = int((max(max_seq_len, win_length)-win_length) // hop_length + 1)\n",
    "          if ((max_seq_len-win_length) % hop_length != 0):\n",
    "               # add one for padding if above is true, see create_patch fxn for more details\n",
    "               self.num_patch += 1\n",
    "          self.patch_len = win_length\n",
    "          self.patch_layer = Patch(patch_len=win_length, stride=hop_length, max_seq_len=max_seq_len)\n",
    "          if not self.time_domain:\n",
    "               self.fft = FFT(dim=-1)\n",
    "          \n",
    "          # Patch Embedding\n",
    "          self.patch_encoder = PatchEncoder(c_in=self.c_in, patch_len=self.patch_len, d_model = self.d_model, shared_embedding=shared_embedding)\n",
    "          # Positional Encoding\n",
    "          if pos_encoding_type.lower() == 'tape':\n",
    "               self.pe = tAPE(d_model=self.d_model, seq_len=self.num_patch, scale_factor=1.0)\n",
    "          else:\n",
    "               self.pe = PositionalEncoding(num_patch=self.num_patch, d_model=self.d_model)\n",
    "          # residual dropout\n",
    "          self.dropout = nn.Dropout(dropout)\n",
    "          # time series transformer layers/Encoder\n",
    "          self.layers = nn.ModuleList([TSTEncoderLayer(d_model=self.d_model, n_heads=n_heads, d_ff=d_ff, norm=norm,\n",
    "                                                       attn_dropout=attn_dropout, dropout=dropout, relative_attn_type=relative_attn_type, num_patches=self.num_patch,\n",
    "                                                       activation=act, res_attention=res_attention, use_flash_attn=use_flash_attn,\n",
    "                                                       pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "          self.res_attention = res_attention\n",
    "\n",
    "          # Head\n",
    "          self.mask = PatchAugmentations(augmentations=augmentations, patch_mask_ratio=mask_ratio, jitter_zero_mask_ratio=mask_ratio)\n",
    "          if self.pretrain_head:\n",
    "               self.head = MaskedAutogressionFeedForward(c_in = self.c_in, patch_len = self.patch_len, d_model = self.d_model, shared_recreation=self.shared_embedding)\n",
    "         \n",
    "     def forward(self, z, sequence_padding_mask=None):\n",
    "          \"\"\"\n",
    "          input from ds is [bs x n_vars x max_seq_len]\n",
    "          z: tensor [bs x num_patch x n_vars x patch_len]\n",
    "          \"\"\"\n",
    "          bs = z.shape[0]\n",
    "          # REVIN\n",
    "          if self.use_revin:\n",
    "               z = self.revin(z, mode=True) # z: [bs x n_vars x max_seq_len] dont passs sequence pad mask to revin if dim=(1,) - it doesnt matter\n",
    "\n",
    "          z = self.patch_layer(z, constant_pad=True, constant_pad_value=0) # z: [bs x num_patch x n_vars x patch_len] pad with 0, same as input padded values\n",
    "          if not self.time_domain:\n",
    "               z = self.fft(z)\n",
    "          Y_true = z.clone().detach()\n",
    "          # patching for pad mask\n",
    "          if sequence_padding_mask is not None:\n",
    "               # create a patched version of the mask for attention and loss masking\n",
    "               ## pad it as well, with 1 (which is the same padding as the padded input values)\n",
    "               if self.time_domain:\n",
    "                    patch_padding_mask = self.patch_layer(sequence_padding_mask, constant_pad=True, constant_pad_value=1).detach() # patch_padding_mask: [bs x num_patch x 1 x patch_len]\n",
    "                    key_padding_mask = torch.all(patch_padding_mask, -1).squeeze(-1)  # key_padding_mask: [bs x num_patch] calculate mask over patch len for each patch\n",
    "                    key_padding_mask = key_padding_mask.unsqueeze(1).expand(-1, self.c_in, -1) # key_padding_mask: [bs x n_vars x num_patch]\n",
    "               else:\n",
    "                    # fft will be 0 for zero values / values with no frequency\n",
    "                    key_padding_mask = torch.all(z, -1) # calculate mask over patch len for each patch\n",
    "                    key_padding_mask = (key_padding_mask[:,:,:1]==0).unsqueeze(1).expand(-1, self.c_in, -1) # key padding mask is true for patches with non-zero,, only need one channel dimension since channels same\n",
    "\n",
    "               key_padding_mask = torch.reshape(key_padding_mask, (-1,self.num_patch)) # key_padding_mask: [bs * nvars x num_patch]\n",
    "          else:\n",
    "               patch_padding_mask,key_padding_mask = None, None\n",
    "          # MASKING\n",
    "          if self.training and self.pretrain_head:\n",
    "               z = self.mask(z) # z: [bs x num_patch x n_vars x patch_len] not implementing padding mask for masking, it shouldnt matter expects: [bs x num_patch x n_vars x patch_len]\n",
    "\n",
    "          # EMBEDDING\n",
    "          z = self.patch_encoder(z) # z: [bs x num_patch x nvars x d_model]\n",
    "          z = z.transpose(1,2) # z: [bs x nvars x num_patch x d_model]\n",
    "          # positional encoding\n",
    "          z = torch.reshape(z, (bs * self.c_in, self.num_patch, self.d_model)) # u: [bs * nvars x num_patch x d_model]\n",
    "\n",
    "          z = self.pe(z) # z: [bs * nvars x num_patch x d_model]\n",
    "\n",
    "          # residual dropout\n",
    "          z = self.dropout(z) # z: [bs * nvars x num_patch x d_model] \n",
    "          \n",
    "          # encoder layers\n",
    "          if self.res_attention:\n",
    "               scores = None\n",
    "               for mod in self.layers:\n",
    "                    z, scores = mod(z, prev=scores, key_padding_mask=key_padding_mask) # z: [bs * n_vars x num_patch x d_model], scores: [bs * n_vars x n_heads x num_patch x num_patch]\n",
    "          else:\n",
    "               for mod in self.layers: \n",
    "                    z = mod(z, key_padding_mask=key_padding_mask) # z: [bs * n_vars x num_patch x d_model]\n",
    "          if key_padding_mask is not None:\n",
    "               key_padding_mask = key_padding_mask.reshape(-1, self.c_in, self.num_patch)[:,0,:] # key_padding_mask: [bs x num_patch]\n",
    "          z = torch.reshape(z, (-1, self.c_in, self.num_patch, self.d_model)) # z: [bs x nvars x num_patch x d_model]\n",
    "          z = z.permute(0,1,3,2) # z: [bs x nvars x d_model x num_patch]\n",
    "          if self.pretrain_head:\n",
    "               Y_pred = self.head(z)\n",
    "          mask=None\n",
    "          # PRETRAIN HEAD\n",
    "          if self.training and self.pretrain_head:\n",
    "               return Y_pred, Y_true, mask, key_padding_mask\n",
    "          elif not self.training and self.pretrain_head:\n",
    "               return z, Y_pred, Y_true, mask, key_padding_mask # z: [bs x nvars x d_model x num_patch]\n",
    "          else:\n",
    "               return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 480])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 480, 7, 750]),\n",
       " torch.Size([4, 480, 7, 750]),\n",
       " torch.Size([4, 480]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "XX = torch.randn(4,7,1*3600*100)\n",
    "pad = torch.zeros(4,1*3600*100)\n",
    "pad[:,0:100] = 1\n",
    "model = PatchTFTSimple(c_in=7,\n",
    "                        win_length=750,\n",
    "                        hop_length=750,\n",
    "                        max_seq_len=(1*3600*100),\n",
    "                        use_revin=True,\n",
    "                        time_domain=True,\n",
    "                        affine=False,\n",
    "                        dim1reduce=False,\n",
    "                        act='gelu',\n",
    "                        use_flash_attn=True,\n",
    "                        relative_attn_type='vanilla',\n",
    "                        pos_encoding_type='learned',\n",
    "                        mask_ratio=0.1,\n",
    "                        augmentations=['jitter_zero_mask'],\n",
    "                        n_layers=1,\n",
    "                        n_heads=1,\n",
    "                        d_model=512,\n",
    "                        d_ff=2048,\n",
    "                        dropout=0.,\n",
    "                        attn_dropout=0.,\n",
    "                        pre_norm=False,\n",
    "                        res_attention=False,\n",
    "                        shared_embedding=False,\n",
    "                        pretrain_head=True\n",
    "                        )\n",
    "r = model(XX, sequence_padding_mask=pad)\n",
    "r[0].shape, r[1].shape, r[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a01932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
