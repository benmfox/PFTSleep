{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "> I'm lost too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, numpy as np, torch.nn.functional as F, torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def masked_mse_loss(preds, target, mask, use_mask=False, padding_mask=None):\n",
    "    \"\"\"\n",
    "    preds:   [bs x num_patch x n_vars x patch_len]\n",
    "    targets: [bs x num_patch x n_vars x patch_len] \n",
    "    mask: [bs x num_patch x n_vars]\n",
    "    padding_mask: [bs x num_patch]\n",
    "    \"\"\"\n",
    "    bs, num_patch, n_vars, patch_len = preds.shape\n",
    "    loss = (preds - target) ** 2\n",
    "    loss = loss.sum(dim=-1) # [bs x num_patch x n_vars]\n",
    "    n_elements = torch.numel(preds)\n",
    "    if padding_mask is not None: \n",
    "        loss = loss*(padding_mask==0).unsqueeze(-1) # padding mask is 0 when it is not a mask\n",
    "        n_elements -= (padding_mask).sum() * patch_len * n_vars  # how many values were padded\n",
    "        # this is a padding mask and should definitely not be included in calculatign the loss\n",
    "    if use_mask:\n",
    "        # use mask is only implemenmted for full patch masking \n",
    "        n_elements = mask.sum()\n",
    "        loss = loss * mask\n",
    "    loss = loss.sum() / torch.as_tensor(n_elements, device=preds.device)   # 1 number\n",
    "    return loss\n",
    "\n",
    "def masked_mae_loss(preds, target, mask, use_mask=False, padding_mask=None):\n",
    "    \"\"\"\n",
    "    preds:   [bs x num_patch x n_vars x patch_len]\n",
    "    targets: [bs x num_patch x n_vars x patch_len] \n",
    "    mask: [bs x num_patch x n_vars]\n",
    "    padding_mask: [bs x num_patch]\n",
    "    \"\"\"\n",
    "    bs, num_patch, n_vars, patch_len = preds.shape\n",
    "    loss = (preds - target).abs()\n",
    "    loss = loss.sum(dim=-1) # [bs x num_patch x n_vars] \n",
    "    n_elements = torch.numel(preds)\n",
    "    if padding_mask is not None:\n",
    "        loss = loss*(padding_mask==0).unsqueeze(-1)\n",
    "        n_elements -= (padding_mask).sum() * patch_len * n_vars \n",
    "    if use_mask:\n",
    "        n_elements = mask.sum()\n",
    "        loss = loss * mask\n",
    "    loss = loss.sum() / torch.as_tensor(n_elements, device=preds.device)  # 1 number\n",
    "    return loss\n",
    "\n",
    "\n",
    "def r2_score(preds, target, mask, use_mask=False):\n",
    "    if use_mask:\n",
    "        ss_res = (((target - preds)**2).mean(dim=-1) * mask).sum()\n",
    "        ss_tot = (((target - target.mean())**2).mean(dim=-1) * mask).sum()\n",
    "    else:\n",
    "        ss_res = (((target - preds)**2).mean(dim=-1)).sum()\n",
    "        ss_tot = (((target - target.mean())**2).mean(dim=-1)).sum()\n",
    "    return 1 - (ss_res/ss_tot)\n",
    "\n",
    "def mse(preds, target, mask, use_mask=False, padding_mask=None):\n",
    "    return masked_mse_loss(preds, target, mask, use_mask=use_mask, padding_mask=padding_mask)\n",
    "\n",
    "\n",
    "def rmse(preds, target, mask, use_mask=False, padding_mask=None):\n",
    "    return torch.sqrt(masked_mse_loss(preds, target, mask, use_mask=use_mask, padding_mask=padding_mask))\n",
    "\n",
    "def mae(preds, target, mask, use_mask=False, padding_mask=None):\n",
    "    return masked_mae_loss(preds, target, mask, use_mask=use_mask, padding_mask=padding_mask)\n",
    "\n",
    "def mape(preds, target, mask, use_mask=False):\n",
    "    epsilon = np.finfo(np.float64).eps # from sklearn\n",
    "    if use_mask:\n",
    "        return (((target - preds).mean(dim=-1).abs() * mask) / (target.mean(dim=-1).abs() * mask).clamp(min=epsilon)).sum() / mask.sum()\n",
    "    else:\n",
    "        return (((target - preds).mean(dim=-1).abs()) / (target.mean(dim=-1).abs()).clamp(min=epsilon)).sum()\n",
    "    \n",
    "def cosine_similarity(preds,target, mask, use_mask=False, padding_mask=None):\n",
    "    bs, num_patch, n_vars, patch_len = preds.shape\n",
    "    sim = F.cosine_similarity(preds,target, dim=-1)\n",
    "    n_elements = torch.numel(preds)\n",
    "    if padding_mask is not None:\n",
    "        sim = sim * (padding_mask==0).unsqueeze(-1)\n",
    "        n_elements -= (padding_mask).sum() * patch_len * n_vars \n",
    "    if use_mask:\n",
    "        n_elements = mask.sum()\n",
    "        sim = sim * mask\n",
    "    sim = sim.sum() / torch.as_tensor(n_elements, device=preds.device)\n",
    "    return sim\n",
    "\n",
    "def cosine_similarity_loss(preds,target, mask, use_mask=False, padding_mask=None):\n",
    "    \"\"\"\n",
    "    preds:   [bs x num_patch x n_vars x patch_len]\n",
    "    targets: [bs x num_patch x n_vars x patch_len] \n",
    "    mask: [bs x num_patch x n_vars]\n",
    "    \"\"\"\n",
    "    bs, num_patch, n_vars, patch_len = preds.shape\n",
    "    sim = F.cosine_similarity(preds,target, dim=-1)\n",
    "    n_elements = torch.numel(preds)\n",
    "    if padding_mask is not None:\n",
    "        sim = sim * (padding_mask==0).unsqueeze(-1)\n",
    "        n_elements -= (padding_mask).sum() * patch_len * n_vars\n",
    "    if use_mask:\n",
    "        n_elements = mask.sum()\n",
    "        sim = sim * mask\n",
    "    sim = sim.sum() / torch.as_tensor(n_elements, device=preds.device)\n",
    "    return -sim\n",
    "\n",
    "def huber_loss(preds, target, mask, use_mask=False, padding_mask=None, delta=1):\n",
    "    \"\"\"\n",
    "    preds:   [bs x num_patch x n_vars x patch_len]\n",
    "    targets: [bs x num_patch x n_vars x patch_len] \n",
    "    mask: [bs x num_patch x n_vars]\n",
    "    padding_mask: [bs x num_patch]\n",
    "    \"\"\"\n",
    "    bs, num_patch, n_vars, patch_len = preds.shape\n",
    "    loss = F.huber_loss(preds, target, delta=delta, reduction='none')\n",
    "    loss = loss.sum(dim=-1) # [bs x num_patch x n_vars] \n",
    "    n_elements = torch.numel(preds)\n",
    "    if padding_mask is not None:\n",
    "        loss = loss*(padding_mask==0).unsqueeze(-1)\n",
    "        n_elements -= (padding_mask).sum() * patch_len * n_vars \n",
    "    if use_mask:\n",
    "        n_elements = mask.sum()\n",
    "        loss = loss * mask\n",
    "    loss = loss.sum() / torch.as_tensor(n_elements, device=preds.device)  # 1 number\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def patch_continuity_loss(preds):\n",
    "    \"\"\"\n",
    "    preds: [bs x num_patch x n_vars x patch_len]\n",
    "    targets: [bs x num_patch x n_vars x patch_len]\n",
    "    \"\"\"\n",
    "    bs, num_patch, n_vars, patch_len = preds.shape\n",
    "    # Calculate difference between end of each patch and start of next\n",
    "    diff = preds[:, 1:, :, 0] - preds[:, :-1, :, -1]\n",
    "    \n",
    "    # Mean squared difference\n",
    "    continuity_loss = torch.mean(diff**2)\n",
    "    \n",
    "    return continuity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2936)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(2,2, 2, 10)\n",
    "\n",
    "patch_continuity_loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    adapted from tsai, weighted multiclass focal loss\n",
    "    https://github.com/timeseriesAI/tsai/blob/bdff96cc8c4c8ea55bc20d7cffd6a72e402f4cb2/tsai/losses.py#L116C1-L140C20\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 weight=None, \n",
    "                 gamma=2., \n",
    "                 reduction='mean',\n",
    "                 ignore_index=-100\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    __name__ = 'focalloss'\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [bs x n classes x n patches]\n",
    "        y: [bs x n patches]\n",
    "        \"\"\"\n",
    "        log_prob = F.log_softmax(x, dim=1)\n",
    "        prob = log_prob.exp()\n",
    "        weight = self.weight.to(x.device) if self.weight is not None else None\n",
    "        if y.dim() == 2:\n",
    "            # hard labels\n",
    "            ce = F.nll_loss(log_prob, y, weight=weight, reduction='none', ignore_index=self.ignore_index)\n",
    "            loss = (1 - prob) ** self.gamma * ce.unsqueeze(1)\n",
    "            mask = (y != self.ignore_index).float().unsqueeze(1)\n",
    "        else:  # soft labels\n",
    "            ce = -(y * log_prob)  # [bs x n_classes x n_patches]\n",
    "            loss = (1 - prob) ** self.gamma * ce\n",
    "            # Positions to ignore will have all zeros\n",
    "            mask = (y.sum(dim=1) > 0).float().unsqueeze(1)\n",
    "            \n",
    "            if weight is not None:\n",
    "                loss = loss * weight.view(1, -1, 1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.sum() / mask.sum().clamp(min=1e-5)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.4217)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "criterion = FocalLoss(gamma=0.7, weight=None, ignore_index=0)\n",
    "batch_size = 10\n",
    "\n",
    "n_patch = 721\n",
    "n_class = 5\n",
    "#m = torch.nn.Softmax(dim=-1)\n",
    "logits = torch.randn(batch_size, n_class, n_patch)\n",
    "target = torch.randint(0, n_class, size=(batch_size, n_patch))\n",
    "criterion(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class KLDivLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler Divergence Loss with masking for ignore_index.\n",
    "    Handles soft labels with ignore_index marked as -100.\n",
    "    \n",
    "    Args:\n",
    "        logits: [bs x n_classes x pred_labels] - model predictions\n",
    "        targets: [bs x n_classes x soft_labels] - soft labels, with ignore_index positions marked as -100\n",
    "        or [bs x n_labels] - hard labels\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='none')\n",
    "    \n",
    "    def _to_one_hot(self, targets, n_classes):\n",
    "        \"\"\"Convert hard labels to one-hot format\"\"\"\n",
    "        valid_mask = (targets != self.ignore_index)\n",
    "        # Set ignore_index positions to 0 temporarily for one-hot conversion\n",
    "        valid_targets = torch.where(valid_mask, targets, torch.zeros_like(targets))\n",
    "        # Convert to one-hot\n",
    "        one_hot = F.one_hot(valid_targets, num_classes=n_classes).permute(0, 2, 1).float()\n",
    "        # Set ignore_index positions to ignore_index\n",
    "        one_hot = torch.where(valid_mask.unsqueeze(1), one_hot, \n",
    "                            torch.full_like(one_hot, self.ignore_index))\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        if targets.dim() == 2:\n",
    "            targets = self._to_one_hot(targets, n_classes=logits.shape[1])\n",
    "        # Create mask for valid positions (where target is not ignore_index)\n",
    "        mask = (targets != self.ignore_index)  # [bs x n classes x soft_labels]\n",
    "        \n",
    "        # Replace ignore_index positions with zeros\n",
    "        targets = torch.where(targets == self.ignore_index, \n",
    "                            torch.zeros_like(targets), \n",
    "                            targets)\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        # Compute KL divergence loss\n",
    "        loss = self.kl_loss(log_probs, targets)\n",
    "        if self.weight is not None:\n",
    "            # Reshape weight to match loss dimensions [1 x n_classes x 1]\n",
    "            weight = self.weight.view(1, -1, 1).to(loss.device)\n",
    "            loss = loss * weight\n",
    "        # Sum across class dimension and apply mask\n",
    "        loss = (loss * mask).sum() / mask.sum().clamp(min=1e-6)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4065)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,5,10)\n",
    "y = torch.randint(0,5, size=(4,10))\n",
    "y_og = y.clone()\n",
    "y[0,0] = -100\n",
    "\n",
    "KLDivLoss(ignore_index=-100)(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
