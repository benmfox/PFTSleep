{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Potentially helpful layers for your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, numpy as np, torch.nn.functional as F\n",
    "\n",
    "from typing import Optional\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.fft import fft\n",
    "from pftsleep.augmentations import create_patch, mask_patches_simple, jitter_augmentation, shuffle_dim, reverse_sequence\n",
    "from pftsleep.signal import stft\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers for Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, # the number of input channels\n",
    "                 patch_len, # the length of the patches (either stft or interval length)\n",
    "                 d_model, # the dimension of the initial linear layers for inputting patches into transformer\n",
    "                 shared_embedding, # indicator of whether to project each channel individually or together\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_embedding = shared_embedding\n",
    "        self.n_vars = c_in\n",
    "\n",
    "        # Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        ## note that this could be an MLP too, if you want\n",
    "        if not shared_embedding:\n",
    "            self.W_P = nn.ModuleList()\n",
    "            for _ in range(self.n_vars): self.W_P.append(nn.Linear(patch_len, d_model))\n",
    "        else:\n",
    "            self.W_P = nn.Linear(patch_len, d_model)\n",
    "\n",
    "    def forward(self, x) -> Tensor:          \n",
    "        \"\"\"\n",
    "        input: x: tensor [bs x num_patch x nvars x patch_len]\n",
    "        returns: x: tensor [bs x num_patch x nvars x d_model]\n",
    "        \"\"\"\n",
    "        # Input embedding\n",
    "        if not self.shared_embedding:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars): \n",
    "                z = self.W_P[i](x[:,:,i,:])\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=2)\n",
    "        else:\n",
    "            x = self.W_P(x) # x: [bs x num_patch x nvars x d_model]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_patch, # number of patches of time series or stft in input\n",
    "                 d_model, # dimension of patch embeddings\n",
    "                 #dropout=0.1 # dropout value\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Positional encoding - learned\n",
    "        self.W_pos =  nn.Parameter(torch.empty((num_patch, d_model)))\n",
    "        nn.init.uniform_(self.W_pos, -0.02, 0.02)\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: x: [bs * nvars x num_patch x d_model]\n",
    "        returns: x: [bs * nvars x num_patch x d_model]\n",
    "        \"\"\"\n",
    "        x = x + self.W_pos\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class tAPE(nn.Module):\n",
    "    \"\"\"\n",
    "    time Absolute Position Encoding\n",
    "    Adapted from tsai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        d_model:int, # the embedding dimension\n",
    "        seq_len:int, # the max. length of the incoming sequence or num patches\n",
    "        #dropout:float=0., # dropout value\n",
    "        scale_factor=1.0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model)  # positional encoding\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin((position * div_term)*(d_model/seq_len)) # this is the difference between normal PE and tAPE, scaling (d_model/seq_len)\n",
    "        pe[:, 1::2] = torch.cos((position * div_term)*(d_model/seq_len))\n",
    "        pe = scale_factor * pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)  # this stores the variable in the state_dict (used for non-trainable variables)\n",
    "\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x): # [batch size, sequence length, embed dim]\n",
    "        x = x + self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask and Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, mask_type, mask_ratio, return_mask=True):\n",
    "        super().__init__()\n",
    "        assert mask_type in ['patch', 'jitter_zero']\n",
    "        self.mask_type = mask_type\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.return_mask = return_mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mask_type == 'jitter_zero':\n",
    "            # mask is a number with the number of masks applied in this function\n",
    "            x_masked, mask = jitter_augmentation(x, mask_ratio=(self.mask_ratio/2), jitter_ratio=(self.mask_ratio/2))\n",
    "        else:\n",
    "            # padding mask is currently not implemented here.. tbh not sure if its needed\n",
    "            x_masked, mask = mask_patches_simple(x, mask_ratio=self.mask_ratio)\n",
    "        if self.return_mask:\n",
    "            return x_masked, mask\n",
    "        else:\n",
    "            return x_masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 0.0000,  0.0000,  0.0000,  1.0975,  0.0000,  1.4248,  0.0000, -0.1104,\n",
       "           1.1865]),\n",
       "  tensor(8)),\n",
       " (tensor([ 1.1462, -1.8379, -0.2368,  2.0488,  0.0000,  0.0000, -0.3927, -1.0523,\n",
       "           0.1294]),\n",
       "  tensor(4)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "torch.manual_seed(125)\n",
    "m = Mask(mask_type='jitter_zero', mask_ratio=0.5)\n",
    "x = torch.randn((9))\n",
    "\n",
    "m(x), m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0000,  1.0000,  1.9390, -0.0338]),\n",
       " tensor([-1.7385, -0.5780,  1.9390, -0.0338]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn((4))\n",
    "x_aug = x.clone()\n",
    "mask = torch.tensor([True,True,False,False])\n",
    "\n",
    "x_aug[mask] = 1\n",
    "x_aug, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchAugmentations(nn.Module):\n",
    "    def __init__(self, augmentations=['patch_mask', 'jitter_zero_mask', 'reverse_sequence', 'shuffle_channels'], patch_mask_ratio=0., jitter_zero_mask_ratio=0.):\n",
    "        super().__init__()\n",
    "        if 'patch_mask' in augmentations:\n",
    "            assert patch_mask_ratio >= 0.\n",
    "        if 'jitter_zero_mask' in augmentations:\n",
    "            assert jitter_zero_mask_ratio >= 0.\n",
    "        self.augmentations = augmentations\n",
    "        self.patch_mask_ratio = patch_mask_ratio\n",
    "        self.jitter_zero_mask_ratio = jitter_zero_mask_ratio\n",
    "    def forward(self, x):\n",
    "        if 'patch_mask' in self.augmentations and 'jitter_zero_mask' in self.augmentations:\n",
    "            mask_choice = np.random.choice(['patch_mask', 'jitter_zero_mask'], replace=True)\n",
    "        else:\n",
    "            mask_choice = None\n",
    "        augs = self.augmentations.copy()\n",
    "        np.random.shuffle(augs)\n",
    "        for augmentation in augs:\n",
    "            if augmentation == 'jitter_zero_mask' and (mask_choice == 'jitter_zero_mask' or mask_choice is None):\n",
    "                # mask is a number with the number of masks applied in this function\n",
    "                x, _ = jitter_augmentation(x, mask_ratio=(self.jitter_zero_mask_ratio/2), jitter_ratio=(self.jitter_zero_mask_ratio/2))\n",
    "            if augmentation == 'patch_mask' and (mask_choice == 'patch_mask' or mask_choice is None):\n",
    "                # padding mask is currently not implemented here.. tbh not sure if its needed\n",
    "                x, _ = mask_patches_simple(x, mask_ratio=self.patch_mask_ratio)\n",
    "            if augmentation == 'shuffle_channels':\n",
    "                x = shuffle_dim(x, dim=2, p=0.5)\n",
    "            if augmentation == 'reverse_sequence':\n",
    "                x = reverse_sequence(x, seq_dim=(-1,), p=0.5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3600, 7, 750])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,3600,7,750)\n",
    "\n",
    "s=PatchAugmentations(patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\n",
    "s(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EmbeddingAugmentations(nn.Module):\n",
    "    def __init__(self, augmentations=['shuffle_dims', 'jitter_zero_mask', 'patch_mask'], dims_to_shuffle = [1,2,3], patch_mask_ratio=0., jitter_zero_mask_ratio=0.):\n",
    "        super().__init__()\n",
    "        assert set(augmentations) - set(['shuffle_dims', 'jitter_zero_mask', 'patch_mask']) == set(), f\"Augmentations must be in {set(['shuffle_dims', 'jitter_zero_mask', 'patch_mask'])}.\"\n",
    "        if 'patch_mask' in augmentations:\n",
    "            assert patch_mask_ratio >= 0.\n",
    "        if 'jitter_zero_mask' in augmentations:\n",
    "            assert jitter_zero_mask_ratio >= 0.\n",
    "        self.augmentations = augmentations\n",
    "        self.patch_mask_ratio = patch_mask_ratio\n",
    "        self.jitter_zero_mask_ratio = jitter_zero_mask_ratio\n",
    "        self.dims_to_shuffle = dims_to_shuffle\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input is an embedding: \n",
    "        x: [bs x n channels x d model x n patches]\n",
    "        dims correspond to 1 = channels 2 = n patches 3 = d model\n",
    "        returns: [bs x n channels x d model x n patches]\n",
    "        \"\"\"\n",
    "        x = x.permute(0,1,3,2)\n",
    "        if 'patch_mask' in self.augmentations and 'jitter_zero_mask' in self.augmentations:\n",
    "            mask_choice = np.random.choice(['patch_mask', 'jitter_zero_mask'], replace=True)\n",
    "        else:\n",
    "            mask_choice = None\n",
    "        augs = self.augmentations\n",
    "        np.random.shuffle(augs)\n",
    "        for augmentation in augs:\n",
    "            if augmentation == 'jitter_zero_mask' and (mask_choice == 'jitter_zero_mask' or mask_choice is None):\n",
    "                # mask is a number with the number of masks applied in this function\n",
    "                x, _ = jitter_augmentation(x, mask_ratio=(self.jitter_zero_mask_ratio/2), jitter_ratio=(self.jitter_zero_mask_ratio/2))\n",
    "            if augmentation == 'patch_mask' and (mask_choice == 'patch_mask' or mask_choice is None):\n",
    "                # padding mask is currently not implemented here.. tbh not sure if its needed\n",
    "                x, _ = mask_patches_simple(x, mask_ratio=self.patch_mask_ratio)\n",
    "            if augmentation == 'shuffle_dims':\n",
    "                # shuffle channels, patches, features\n",
    "                dim_shuffle_order = self.dims_to_shuffle\n",
    "                np.random.shuffle(dim_shuffle_order)\n",
    "                for dim in dim_shuffle_order:\n",
    "                    x = shuffle_dim(x, dim=dim, p=0.5)\n",
    "        x = x.permute(0,1,3,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 512, 3600])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,7,512,3600)\n",
    "\n",
    "s = EmbeddingAugmentations(augmentations=['jitter_zero_mask'], dims_to_shuffle=[1], patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\n",
    "s(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch and Fourier Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Patch(nn.Module):\n",
    "    def __init__(self, patch_len, stride, max_seq_len=None):\n",
    "        super().__init__()\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def forward(self, x, constant_pad=False, constant_pad_value=0):\n",
    "        x = create_patch(x, patch_len=self.patch_len, stride=self.stride, constant_pad=constant_pad, constant_pad_value=constant_pad_value, max_seq_len=self.max_seq_len)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class STFT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_fft, \n",
    "                 win_length,\n",
    "                 hop_length, \n",
    "                 stft_norm, \n",
    "                 decibel_scale, \n",
    "                 channel_stft_means=None, \n",
    "                 channel_stft_stds=None, \n",
    "                 pad_win_length_to_nfft=True, \n",
    "                 pad_mode='reflect', \n",
    "                 center=False, \n",
    "                 return_complex=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.pad_mode = pad_mode\n",
    "        self.pad_win_length_to_nfft = pad_win_length_to_nfft\n",
    "        self.center = center\n",
    "        self.return_complex = return_complex\n",
    "        self.stft_norm = stft_norm\n",
    "        self.decibel_scale = decibel_scale\n",
    "        self.channel_stft_means = channel_stft_means\n",
    "        self.channel_stft_stds = channel_stft_stds\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [bs x n_vars x max_seq_len]\n",
    "        out: [bs x n_vars x n_fft // 2 + 1 x stft_len]\n",
    "        \"\"\"\n",
    "        x_fft = stft(x, \n",
    "                     n_fft=self.n_fft, \n",
    "                     win_length=self.win_length,\n",
    "                     normalized=self.stft_norm,\n",
    "                     pad_win_length_to_nfft=self.pad_win_length_to_nfft,\n",
    "                     decibel_scale=self.decibel_scale, \n",
    "                     channel_stft_means=self.channel_stft_means, \n",
    "                     channel_stft_stds=self.channel_stft_stds, \n",
    "                     hop_length=self.hop_length, \n",
    "                     pad_mode=self.pad_mode, \n",
    "                     center=self.center, \n",
    "                     return_complex=self.return_complex\n",
    "                     )\n",
    "        return x_fft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class FFT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim=-1, # dimension to calculate fft over\n",
    "                 norm='backward' #  \"forward\" - normalize by 1/n, \"backward\" - no normalization, \"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is equivalent to an stft with onesided set to False, hop length of 1, and win_length == n_fft.\n",
    "\n",
    "        x: [bs x num_patch x n_vars x patch_len]\n",
    "        out: [bs x num_patch x n_vars x patch_len(freq domain)]\n",
    "        \"\"\"\n",
    "        x_fft = fft(x, dim=self.dim, norm=self.norm).abs()\n",
    "        return x_fft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reversible Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_features: int, # the number of channels or features in the input\n",
    "                 eps=1e-5, # added to avoid division by zero errors\n",
    "                 dim_to_reduce=-1, # the dimension to reduce, \n",
    "                 affine=True # learning affine parameters bias and weight per channel\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.dim_to_reduce = dim_to_reduce\n",
    "\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,num_features,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,num_features,1))\n",
    "\n",
    "    def forward(self, x, mode:bool):\n",
    "        \"\"\"\n",
    "        x: [bs x n_vars x max_seq_len]\n",
    "        \"\"\"\n",
    "        if mode:\n",
    "            return self._normalize(x)\n",
    "        else:\n",
    "            return self._denormalize(x)\n",
    "\n",
    "    def _normalize(self, x):      \n",
    "        self.mean = torch.mean(x, dim=self.dim_to_reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.std(x, dim=self.dim_to_reduce, keepdim=True, unbiased=False).detach() + self.eps\n",
    "\n",
    "        x = x.sub(self.mean)\n",
    "        x = x.div(self.stdev)\n",
    "        \n",
    "        if self.affine:\n",
    "            x = x.mul(self.affine_weight)\n",
    "            x = x.add(self.affine_bias)\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x.sub(self.affine_bias)\n",
    "            x = x.div(self.affine_weight)# + self.eps*self.eps)\n",
    "        x = x.mul(self.stdev)\n",
    "        x = x.add(self.mean)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 1000]),\n",
       " torch.Size([4, 7, 1000]),\n",
       " torch.Size([4, 7, 1000]),\n",
       " torch.Size([4, 7, 1]),\n",
       " torch.Size([4, 7, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn(4,7,1000)\n",
    "\n",
    "revin = RevIN(x.shape[1], dim_to_reduce=-1, affine=True)\n",
    "x_norm = revin(x, mode=True)\n",
    "x_denorm = revin(x_norm, mode=False)\n",
    "\n",
    "x.shape, x_norm.shape, x_denorm.shape, revin.mean.shape, revin.stdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiheadFlashAttention(nn.Module):\n",
    "    \"\"\"Multihead attention layer with optional causal masking.\n",
    "    Uses flash attention when available in PyTorch 2.0+.\n",
    "    \n",
    "    Args:\n",
    "        n_heads (int): Number of attention heads\n",
    "        d_model (int): Embedding dimension\n",
    "        qkv_bias (bool, optional): Use bias in linear layers. Defaults to False.\n",
    "        is_causal (bool, optional): Use causal masking. Defaults to False.\n",
    "        attn_dropout (float, optional): Attention dropout probability. Defaults to 0.0.\n",
    "        proj_dropout (float, optional): Dropout probability. Defaults to 0.0.\n",
    "\n",
    "    Note that when passing in a key paddings mask, it should be a boolean tensor of shape [bs x seq_len]\n",
    "    where a True value indicates that the key at that position should be ignored for the purposes of attention.\n",
    "    This is contrary to what the PyTorch documentation suggests, but is correct in this module.\n",
    "    \"\"\", \n",
    "    def __init__(self, d_model: int, n_heads: int, qkv_bias: bool=True, \n",
    "                 is_causal: bool=False, attn_dropout: float=0.0, proj_dropout: float=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        # # key, query, value projections for all heads, but in a batch, Combined Q,K,V projections\n",
    "        self.c_attn = nn.Linear(d_model, 3 * d_model, bias=qkv_bias)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        \n",
    "        # Regularization\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.resid_dropout = nn.Dropout(proj_dropout)\n",
    "        \n",
    "        # Architecture\n",
    "        self.num_heads = n_heads\n",
    "        self.embed_dimension = d_model\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "    def forward(self, x: Tensor, \n",
    "                key_padding_mask: Optional[Tensor] = None\n",
    "                ) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: [bs x seq_len x d_model]\n",
    "        key_padding_mask: [bs x seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Project to Q,K,V\n",
    "        qkv = self.c_attn(x)\n",
    "        embed_dim = qkv.size(2)\n",
    "        head_dim = embed_dim // (self.num_heads * 3)\n",
    "\n",
    "        # Split and reshape\n",
    "        query, key, value = qkv.chunk(3, -1)\n",
    "        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Set dropout and causality based on training mode\n",
    "        attn_dropout = self.attn_dropout if self.training else 0.0\n",
    "        is_causal = self.is_causal if self.training else False\n",
    "        if key_padding_mask is not None:\n",
    "            if key_padding_mask.dim() == 2:\n",
    "                key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(1)  # [bs x 1 x 1 x seq_len]\n",
    "            if key_padding_mask.dtype == torch.bool:\n",
    "                key_padding_mask = key_padding_mask.float().masked_fill(key_padding_mask, float('-inf'))\n",
    "            key_padding_mask = key_padding_mask.expand(-1, self.num_heads, query.size(-2), -1)\n",
    "            if key_padding_mask.stride(-1) != 1:\n",
    "                # see https://github.com/pytorch/pytorch/issues/127523\n",
    "                key_padding_mask = torch.empty_like(key_padding_mask, memory_format=torch.contiguous_format).copy_(key_padding_mask)\n",
    "\n",
    "        # Attention (with flash attention when available)\n",
    "        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "            y = F.scaled_dot_product_attention(\n",
    "                query, key, value, \n",
    "                attn_mask=key_padding_mask,  # PyTorch handles mask preprocessing internally, can pass attn_mask or key_padding_mask\n",
    "                dropout_p=attn_dropout, \n",
    "                is_causal=is_causal,\n",
    "                scale=self.scale\n",
    "            )\n",
    "        \n",
    "        # Reshape and project output\n",
    "        y = y.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * head_dim)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 100, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "mha = MultiheadFlashAttention(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0.)\n",
    "x = torch.randn(2*7,100,512) # [bs * n_vars x n_patches x d_model]\n",
    "key_padding_mask = torch.zeros(2*7, 100, dtype=torch.bool)\n",
    "key_padding_mask[:, -2:] = True  # mask last 2 positions\n",
    "output = mha(x, key_padding_mask=key_padding_mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
    "    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
    "    by Lee et al, 2021)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        '''\n",
    "        Input shape:\n",
    "            q               : [bs x n_heads x max_q_len x d_k]\n",
    "            k               : [bs x n_heads x d_k x seq_len]\n",
    "            v               : [bs x n_heads x seq_len x d_v]\n",
    "            prev            : [bs x n_heads x q_len x seq_len]\n",
    "            key_padding_mask: [bs x seq_len]\n",
    "            attn_mask       : [1 x seq_len x seq_len]\n",
    "        Output shape:\n",
    "            output:  [bs x n_heads x q_len x d_v]\n",
    "            attn   : [bs x n_heads x q_len x seq_len]\n",
    "            scores : [bs x n_heads x q_len x seq_len]\n",
    "        '''\n",
    "\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        ## this is the b ottleneck in your code\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        ## if implementing attention or key padding mask - https://github.com/pytorch/pytorch/issues/41508#issuecomment-1723119580\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:# and torch.any(key_padding_mask): - not sure if this is needed                         # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        # this is another bottleneck (softmax)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
    "            attn_weights = attn_weights.masked_fill(attn_mask, 0.0)\n",
    "       \n",
    "        if key_padding_mask is not None:# and torch.any(key_padding_mask):\n",
    "            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), 0.0)\n",
    "       \n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiheadAttentionCustom(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
    "        \"\"\"Multi Head Attention Layer\n",
    "        Input shape:\n",
    "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
    "            K, V:    [batch_size (bs) x q_len x d_model]\n",
    "            mask:    [q_len x q_len]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        # Scaled Dot-Product Attention (multiple heads)\n",
    "        self.res_attention = res_attention\n",
    "        self.sdp_attn = ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
    "\n",
    "        # Poject output\n",
    "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
    "\n",
    "\n",
    "    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None,\n",
    "                key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "\n",
    "        bs = Q.size(0)\n",
    "        if K is None: K = Q\n",
    "        if V is None: V = Q\n",
    "\n",
    "        # Linear (+ split in multiple heads)\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
    "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
    "        if self.res_attention:\n",
    "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        output = self.to_out(output)\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttentionCustom(\n",
       "  (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (sdp_attn): ScaledDotProductAttention(\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (to_out): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "mha_attn = MultiheadAttentionCustom(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0., res_attention=False)\n",
    "mha_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom output shape: torch.Size([2, 10, 64])\n",
      "Flash output shape: torch.Size([2, 10, 64])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "def test_attention_equivalence():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "    \n",
    "    # Create input tensor (only need one since we're using self-attention)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create key padding mask\n",
    "    key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)\n",
    "    key_padding_mask[:, -2:] = True  # mask last 2 positions\n",
    "\n",
    "    # Initialize both implementations\n",
    "    custom_mha = MultiheadAttentionCustom(d_model=d_model, n_heads=n_heads)\n",
    "    flash_mha = MultiheadFlashAttention(d_model=d_model, n_heads=n_heads)\n",
    "    \n",
    "    # Set both models to eval mode to disable dropout\n",
    "    custom_mha.eval()\n",
    "    flash_mha.eval()\n",
    "    \n",
    "    # Copy weights to ensure identical parameters\n",
    "    # Combine QKV weights from custom implementation into single matrix for flash attention\n",
    "    combined_weight = torch.cat([\n",
    "        custom_mha.W_Q.weight,\n",
    "        custom_mha.W_K.weight,\n",
    "        custom_mha.W_V.weight\n",
    "    ], dim=0)\n",
    "    combined_bias = torch.cat([\n",
    "        custom_mha.W_Q.bias,\n",
    "        custom_mha.W_K.bias,\n",
    "        custom_mha.W_V.bias\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Copy combined weights to flash attention\n",
    "    flash_mha.c_attn.weight.data = combined_weight\n",
    "    flash_mha.c_attn.bias.data = combined_bias\n",
    "    \n",
    "    # Output projection weights\n",
    "    flash_mha.c_proj.weight.data = custom_mha.to_out[0].weight.data.clone()\n",
    "    flash_mha.c_proj.bias.data = custom_mha.to_out[0].bias.data.clone()\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        custom_output, custom_attn = custom_mha(x, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        flash_output = flash_mha(x, attn_mask=key_padding_mask)\n",
    "    \n",
    "    # Compare outputs\n",
    "    print(f\"Custom output shape: {custom_output.shape}\")\n",
    "    print(f\"Flash output shape: {flash_output.shape}\")\n",
    "    \n",
    "    output_close = torch.allclose(custom_output, flash_output, rtol=0, atol=0)\n",
    "    print(f\"Outputs match: {output_close}\")\n",
    "    \n",
    "    if not output_close:\n",
    "        print(\"\\nOutput differences:\")\n",
    "        print(f\"Max difference: {(custom_output - flash_output).abs().max().item()}\")\n",
    "        print(f\"Mean difference: {(custom_output - flash_output).abs().mean().item()}\")\n",
    "    \n",
    "    return custom_output, flash_output\n",
    "\n",
    "custom_output, flash_output = test_attention_equivalence()\n",
    "#: 8.940696716308594e-08\n",
    "#Mean difference: 1.0550138540565968e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input shape: torch.Size([1, 7, 10799, 500])\n",
      "Projected X shape to d_model: torch.Size([1, 7, 10799, 512])\n",
      "Reshape for attention: torch.Size([7, 10799, 512])\n",
      "\n",
      "Testing MHA and SDA attention, with just 50 elements.\n",
      "MHA attention output shape: torch.Size([7, 50, 512]), mha attn weight shape: torch.Size([7, 8, 50, 50])\n",
      "Q shape: torch.Size([1, 8, 75593, 64]), K shape: torch.Size([1, 8, 64, 75593]), V shape: torch.Size([1, 8, 75593, 64])\n",
      "Attn output shape torch.Size([1, 50, 512]), attn weight shape: torch.Size([1, 8, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "d_model=512\n",
    "n_heads=8\n",
    "d_k = d_v = d_model // n_heads\n",
    "attn = ScaledDotProductAttention(d_model=d_model, n_heads=n_heads)\n",
    "mha_attn = MultiheadAttentionCustom(d_model, n_heads)\n",
    "\n",
    "W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "X,_,_ = ds[0]\n",
    "\n",
    "X = create_patch(X, patch_len=(10*50), stride=(5*50), constant_pad=True)\n",
    "\n",
    "patch_len = X.shape[-1]\n",
    "\n",
    "X = X[None, ...].permute(0,2,1,3)  # simulate batch size of 1 [bs x n_vars x num_patch x patch_len]\n",
    "\n",
    "print(f'X input shape: {X.shape}')\n",
    "W_P = nn.Linear(patch_len, d_model)\n",
    "\n",
    "X = W_P(X) # project to d_model\n",
    "print(f\"Projected X shape to d_model: {X.shape}\")\n",
    "\n",
    "X = torch.reshape(X, (X.shape[0]*X.shape[1],X.shape[2],X.shape[3]))\n",
    "print(f\"Reshape for attention: {X.shape}\")\n",
    "\n",
    "# test multihead attention\n",
    "print(\"\\nTesting MHA and SDA attention, with just 50 elements.\")\n",
    "mha_output, mha_attn_weights = mha_attn(Q=X[:,:50,:])\n",
    "print(f\"MHA attention output shape: {mha_output.shape}, mha attn weight shape: {mha_attn_weights.shape}\")\n",
    "\n",
    "# test scaled dot product attn\n",
    "K = Q = V = X\n",
    "\n",
    "# # Linear (+ split in multiple heads)\n",
    "bs = 1 # 1 * 16\n",
    "q_s = W_Q(Q).reshape(bs, -1, n_heads, d_k).transpose(1, 2)\n",
    "k_s = W_K(K).reshape(bs, -1, n_heads, d_k).permute(0, 2, 3, 1)\n",
    "v_s = W_V(V).reshape(bs, -1, n_heads, d_v).transpose(1, 2)\n",
    "print(f\"Q shape: {q_s.shape}, K shape: {k_s.shape}, V shape: {v_s.shape}\")\n",
    "\n",
    "to_out = nn.Linear(n_heads * d_v, d_model)\n",
    "output, attn_weights = attn(q_s[:,:,:50,:],k_s[:,:,:,:50], v_s[:,:,:50,:])\n",
    "output = output.transpose(1, 2).contiguous().view(bs, -1, n_heads * d_v)\n",
    "print(f\"Attn output shape {output.shape}, attn weight shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Attention_Rel_Scl(nn.Module):\n",
    "    def __init__(self, \n",
    "        d_model:int, # Embedding dimension\n",
    "        n_heads:int, # number of attention heads\n",
    "        seq_len:int, # sequence length or num patches\n",
    "        d_k:int=None, # key dimension\n",
    "        d_v:int=None, # value dimension\n",
    "        res_attention:bool=False, # whether to use residual attention\n",
    "        attn_dropout:float=0., # dropout for attention\n",
    "        lsa:bool=False, # whether to use LSA, trainable paramater for scaling\n",
    "        proj_dropout:float=0., # dropout for projection\n",
    "        qkv_bias:bool=True, # bias for q, k, v\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Adapted from tsai\n",
    "        Added residual attention and dropout\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.value = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), n_heads))\n",
    "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)), indexing=\"xy\")\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "        relative_coords[1] += self.seq_len - 1\n",
    "        relative_coords = relative_coords.permute(1, 2, 0)\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
    "\n",
    "    def forward(self, x, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        q = self.query(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, -1, self.n_heads, self.d_k).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k) * self.scale # [seq_len, seq_len]\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:# and torch.any(key_padding_mask): - not sure if this is needed                         # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), -np.inf)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, self.n_heads))\n",
    "        relative_bias = relative_bias.reshape(self.seq_len, self.seq_len, -1).permute(2, 0, 1).unsqueeze(0)\n",
    "        attn_weights = attn_weights + relative_bias\n",
    "\n",
    "        if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
    "            attn_weights = attn_weights.masked_fill(attn_mask, 0.0)\n",
    "       \n",
    "        if key_padding_mask is not None:# and torch.any(key_padding_mask):\n",
    "            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), 0.0)\n",
    "\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        out = torch.matmul(attn_weights, v) # [batch_size, n_heads, seq_len, d_head]\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        out = self.to_out(out)\n",
    "        if self.res_attention: \n",
    "            return out, attn_weights, attn_scores\n",
    "        else: \n",
    "            return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "d_model=16\n",
    "c_in = 2\n",
    "seq_len = 1*360*10\n",
    "x = torch.randn(4,c_in,seq_len)\n",
    "embed_layer = nn.Sequential(nn.Conv2d(1, d_model*4, kernel_size=[1, 7], padding='same'), nn.BatchNorm2d(d_model*4), nn.GELU())\n",
    "embed_layer2 = nn.Sequential(nn.Conv2d(d_model*4, d_model, kernel_size=[c_in, 1], padding='valid'), nn.BatchNorm2d(d_model), nn.GELU())\n",
    "abs_position = tAPE(d_model, seq_len=seq_len)\n",
    "x_emb = embed_layer2(embed_layer(x.unsqueeze(1))).squeeze(2)\n",
    "x_emb = x_emb.permute(0,2,1)\n",
    "x_emb_pos = abs_position(x_emb)\n",
    "\n",
    "model = Attention_Rel_Scl(d_model=d_model,\n",
    "        n_heads=2, # number of attention heads\n",
    "        seq_len=seq_len, # sequence length or num patches\n",
    "        )\n",
    "\n",
    "out, attn_weights = model(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "## test w patches [bs *c_in x num_patches x d_model]\n",
    "d_model=512\n",
    "c_in = 2\n",
    "num_patches = 10\n",
    "x_emb = torch.randn(4*c_in,num_patches, d_model)\n",
    "abs_position = tAPE(d_model, seq_len=num_patches)\n",
    "x_emb_pos = abs_position(x_emb)\n",
    "\n",
    "model = Attention_Rel_Scl(d_model=d_model,\n",
    "        n_heads=2, # number of attention heads\n",
    "        seq_len=num_patches, # sequence length or num patches\n",
    "        )\n",
    "\n",
    "out, attn_weights = model(x_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskedAutogressionFeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, # the number of input channels\n",
    "                 patch_len, # the length of the patches (either stft or interval length)\n",
    "                 d_model, # the dimension of the initial linear layers for inputting patches into transformer\n",
    "                 shared_recreation=True, # indicator of whether to project each channel individually or together\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_recreation = shared_recreation\n",
    "        self.n_vars = c_in\n",
    "\n",
    "        # Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        ## note that this could be an MLP too, if you want\n",
    "        if not shared_recreation:\n",
    "            self.W_P = nn.ModuleList()\n",
    "            for _ in range(self.n_vars): self.W_P.append(nn.Linear(d_model, patch_len))\n",
    "        else:\n",
    "            self.W_P = nn.Linear(d_model, patch_len)\n",
    "\n",
    "    def forward(self, x) -> Tensor:          \n",
    "        \"\"\"\n",
    "        input: x: tensor [bs x nvars x d_model x num_patch]\n",
    "        returns: x: tensor [bs x num_patch x nvars x d_model]\n",
    "        \"\"\"\n",
    "        # Input embedding\n",
    "        x = x.permute(0,3,1,2) # [bs x num_patch x nvars x d_model]\n",
    "        if not self.shared_recreation:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars): \n",
    "                z = self.W_P[i](x[:,:,i,:])\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=2)\n",
    "        else:\n",
    "            x = self.W_P(x) # x: [bs x num_patch x nvars x patch_len]\n",
    "        return x\n",
    "    \n",
    "class MaskedAutogressionFeedForward2(nn.Module):\n",
    "    def __init__(self, d_model, patch_len, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers \n",
    "        self.layers = []\n",
    "        n_nuerons = int((patch_len - d_model)/self.n_layers)\n",
    "        if self.n_layers == 1:\n",
    "            self.layers.append(nn.Sequential(nn.Linear(d_model, patch_len), nn.Dropout(dropout)))\n",
    "        elif self.n_layers == 0:\n",
    "            # do nothing\n",
    "            self.layers.append(nn.Identity())\n",
    "        else:\n",
    "            for i in range(self.n_layers-1):\n",
    "                self.layers.append(nn.Sequential(nn.Linear(n_nuerons*i+d_model, n_nuerons*(i+1)+d_model), nn.ReLU(), nn.Dropout(dropout)))\n",
    "            self.layers.append(nn.Sequential(nn.Linear(n_nuerons*(i+1)+d_model, patch_len)))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,3)\n",
    "        x = self.layers(x)\n",
    "        x = x.permute(0,2,1,3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):        \n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "\n",
    "def get_activation_fn(activation):\n",
    "    if callable(activation): return activation()\n",
    "    elif activation.lower() == \"relu\": return nn.ReLU()\n",
    "    elif activation.lower() == \"gelu\": return nn.GELU()\n",
    "    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
