{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> oh no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import zarr, numpy as np, pandas as pd, multiprocessing as mp, torch\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_class_weights(dataloader, ignore_index=-100, returns_padded_mask=True, return_ratio=True):\n",
    "    if returns_padded_mask:\n",
    "        target_values = [y.unique(return_counts=True) for _,y,_ in dataloader]\n",
    "    else:\n",
    "        target_values = [y.unique(return_counts=True) for _,y in dataloader]\n",
    "    all_counts = []\n",
    "    for i,c in target_values:\n",
    "        if ignore_index is not None:\n",
    "            all_counts.append(c[i != ignore_index])\n",
    "        else:\n",
    "            all_counts.append(c)\n",
    "    idx = i[i != ignore_index] if ignore_index is not None else i\n",
    "    total_per_class = torch.stack(all_counts).sum(dim=0)\n",
    "    total = total_per_class.sum()\n",
    "    if return_ratio:\n",
    "        ratio = total_per_class/total\n",
    "    else:\n",
    "        ratio = total_per_class\n",
    "    return dict(zip(idx.tolist(), ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def interpolate_nan_clip(x_in, physiological_range_clip=None, percentile_clip=None, return_mask_only=False):\n",
    "    \"\"\"\n",
    "    Function to clip outliers based on percentiles or physiological range and then interpolate nearby values\n",
    "    \"\"\"\n",
    "    x = np.copy(x_in)\n",
    "    if physiological_range_clip is not None:\n",
    "        # if a physiological range, clip, set to nan, and interpolate nearby values\n",
    "        assert len(physiological_range_clip) == 2, \"physiological_range_clip expects a tuple or list of 1 or 2 values. Supply none to clip only one end.\"\n",
    "        max_ = physiological_range_clip[1] if physiological_range_clip[1] is not None else None\n",
    "        min_ = physiological_range_clip[0] if physiological_range_clip[0] is not None else None\n",
    "        if max_ is not None:\n",
    "            x[x>=max_] = np.nan\n",
    "        if min_ is not None:\n",
    "            x[x<=min_] = np.nan\n",
    "    if percentile_clip is not None:\n",
    "        # if percentiles, clip at percentiles, set to nana, and interpolate nearby\n",
    "        assert len(percentile_clip) == 2, \"percentile_clip expects a tuple or list of 1 or 2 values. Supply none to clip only one end.\"\n",
    "        max_ = np.quantile(x, q=percentile_clip[1]) if percentile_clip[1] is not None else None\n",
    "        min_ = np.quantile(x, q=percentile_clip[0]) if percentile_clip[0] is not None else None\n",
    "        if max_ is not None:\n",
    "            x[x>=max_] = np.nan\n",
    "        if min_ is not None:\n",
    "            x[x<=min_] = np.nan\n",
    "    mask = np.isnan(x)\n",
    "    if return_mask_only:\n",
    "        return mask\n",
    "    if all(mask):\n",
    "        return x_in\n",
    "    else:\n",
    "        x[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), x[~mask])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_stats(idx, zarr_file, channels, clip_interpolations=None, channel_magnitude_multiple=None):\n",
    "    \"\"\"\n",
    "    Function to caluclate stats on an individual zarr array, including a clip interpolate range\n",
    "    \"\"\"\n",
    "    root_grp = zarr.open(zarr_file)\n",
    "    updated_channels = []\n",
    "    if any(isinstance(i, list) for i in channels):\n",
    "        avail_channels = list(root_grp.array_keys())\n",
    "        for p in channels:\n",
    "            updated_channels.append(next(x for x in p if x in avail_channels))\n",
    "    else:\n",
    "        updated_channels = channels\n",
    "    channel_tracker = {channel:{'std':0,'len':0,'mean':0, 'n':0, 'n_zeros':0, 'n_nan':0, '50%':0, '95%':0, '99%':0, '5%':0, '1%':0} for channel in updated_channels}\n",
    "    for channel in updated_channels:\n",
    "        if channel in root_grp.array_keys():\n",
    "            channel_tracker[channel]['n'] += 1\n",
    "            temp = root_grp[channel][:].astype(np.float32)\n",
    "            if clip_interpolations is not None and clip_interpolations != {} and channel in clip_interpolations:\n",
    "                temp = interpolate_nan_clip(temp, physiological_range_clip=clip_interpolations[channel]['phys_range'], percentile_clip=clip_interpolations[channel]['percentiles'])\n",
    "            channel_tracker[channel]['n_zeros'] = (temp == 0).sum()\n",
    "            channel_tracker[channel]['n_nan'] = np.isnan(temp).sum()\n",
    "            channel_tracker[channel]['file'] = zarr_file\n",
    "            channel_tracker[channel]['mean'] += np.nanmean(temp)\n",
    "            channel_tracker[channel]['len'] += len(temp)\n",
    "            channel_tracker[channel]['std'] += np.nanstd(temp)\n",
    "            quantiles = np.nanquantile(temp, q=[0.01,0.05,0.5,0.95,0.99])\n",
    "            for idx, quantile in enumerate(['1%','5%','50%', '95%','99%']):\n",
    "                channel_tracker[channel][quantile] += quantiles[idx]\n",
    "    if channel_magnitude_multiple is not None:\n",
    "        for channel in updated_channels:\n",
    "            if channel in channel_magnitude_multiple:\n",
    "                for stat in ['mean', 'std', '1%','5%','50%', '95%','99%']:\n",
    "                    channel_tracker[channel][stat] *= channel_magnitude_multiple[channel]\n",
    "    return channel_tracker\n",
    "\n",
    "def calculate_stats_all(zarr_files, channels, sample_wise=True, clip_interpolations=None, channel_magnitude_multiple=None):\n",
    "    with mp.Pool() as pool:\n",
    "        f = partial(calculate_stats, channels=channels, clip_interpolations=clip_interpolations, channel_magnitude_multiple=channel_magnitude_multiple)\n",
    "        results = pool.starmap_async(f, enumerate(zarr_files))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    if not sample_wise:\n",
    "        all_channels = [v for i in channels for v in i]\n",
    "        total_stats = {channel:{'std':0,'len':0,'mean':0, 'n':1, 'n_zeros':0, 'n_nan':0, '50%':0, '95%':0, '99%':0, '5%':0, '1%':0} for channel in all_channels}\n",
    "        for result in results.get():\n",
    "            for channel in result:\n",
    "                for stat in ['n','len','n_zeros','n_nan', 'std','mean','1%','5%','50%','95%','99%']:\n",
    "                    total_stats[channel][stat] += result[channel][stat]\n",
    "        for channel in total_stats:\n",
    "            for stat in ['std','mean','1%','5%','50%','95%','99%']:\n",
    "                total_stats[channel][stat] = total_stats[channel][stat] / total_stats[channel]['n']\n",
    "        return pd.DataFrame(total_stats).T\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "        for result in results.get():\n",
    "            final_df = pd.concat([final_df, pd.DataFrame(result).T])\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_samples(idx, zarr_file, channels, frequency, sample_seq_len_sec, stride_sec, start_offset_sec=None, max_seq_len_sec=None, include_partial_samples=True, nan_tolerance=0.0):\n",
    "    \"\"\"\n",
    "    Function to create a dataframe of samples and their sequence indices\n",
    "    \"\"\"\n",
    "    if start_offset_sec is None:\n",
    "        start_offset_sec = 0\n",
    "    start_offset = start_offset_sec * frequency\n",
    "    if max_seq_len_sec is not None:\n",
    "        assert max_seq_len_sec >= sample_seq_len_sec, \"The maximum sequence length should be >= the sample sequence length. The maximum sequence length is the end cutoff point of a sample. A sample cannot be longer than that value.\"\n",
    "    sample_seq_len = sample_seq_len_sec * frequency\n",
    "\n",
    "    stride = stride_sec * frequency\n",
    "    root_grp = zarr.open(zarr_file)\n",
    "    updated_channels = []\n",
    "    avail_channels = list(root_grp.array_keys())\n",
    "    if all(isinstance(i, list) for i in channels):\n",
    "        for p in channels:\n",
    "            updated_channels.append(next((x for x in p if x in avail_channels), None))\n",
    "    else:\n",
    "        updated_channels = [p if p in avail_channels else None for p in channels]\n",
    "    if None not in updated_channels:\n",
    "        # all channels are present\n",
    "        ## all channels should be the same length\n",
    "        if 'header' in root_grp.attrs:\n",
    "            duration = int(root_grp.attrs['header']['Duration']) # duration in seconds\n",
    "        else:\n",
    "            duration = int(root_grp.attrs['Duration']) # duration in seconds\n",
    "        if duration > start_offset_sec:\n",
    "            if include_partial_samples:\n",
    "                max_seq_len = max_seq_len_sec*frequency if max_seq_len_sec is not None else duration*frequency+sample_seq_len-1\n",
    "                sample_indices = [(i, i+sample_seq_len) for i in range(start_offset, max_seq_len+start_offset, stride) if (i-start_offset)+sample_seq_len <= max_seq_len]#duration*frequency+sample_seq_len]\n",
    "            else:\n",
    "                max_seq_len =  max_seq_len_sec*frequency if max_seq_len_sec is not None and max_seq_len_sec < duration else duration*frequency\n",
    "                sample_indices = [(i, i+sample_seq_len) for i in range(start_offset, max_seq_len+start_offset, stride) if (i-start_offset)+sample_seq_len <= max_seq_len]\n",
    "            sample_index_df = pd.DataFrame([{'start_idx':i[0], 'end_idx':i[1]} for i in sample_indices])\n",
    "            sample_index_df['file'] = zarr_file\n",
    "            sample_index_df['zarr_index'] = idx # assign unique uuid to each sample\n",
    "            if not sample_index_df.empty:\n",
    "                for channel in updated_channels:\n",
    "                    channel_data = root_grp[channel][:]\n",
    "                    sample_index_df[f'{channel}_sum_nan'] = sample_index_df.apply(\n",
    "                        lambda row: np.isnan(channel_data[row['start_idx']:row['end_idx']]).sum(), axis=1\n",
    "                    )\n",
    "                \n",
    "                # the maximum channel nan sum should be less than the nan tolerance\n",
    "                sample_index_df['all_channels_nan_max'] = sample_index_df[[f'{channel}_sum_nan' for channel in updated_channels]].max(axis=1)\n",
    "                cutoff = nan_tolerance * sample_seq_len \n",
    "                sample_index_df = sample_index_df[sample_index_df['all_channels_nan_max'] <= cutoff]\n",
    "                            \n",
    "                sample_index_df['n_samples'] = len(sample_indices)\n",
    "                return sample_index_df\n",
    "\n",
    "def calculate_samples_mp(zarr_files, channels, frequency, sample_seq_len_sec, stride_sec, start_offset_sec = None, max_seq_len_sec=None, include_partial_samples=True, nan_tolerance=0.0):\n",
    "    \"\"\"\n",
    "    Multiprocessing function to generate samples\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame(columns=['file', 'start_idx','end_idx','n_samples'])\n",
    "    total_samples = 0\n",
    "    with mp.Pool() as pool:\n",
    "        f = partial(calculate_samples, channels=channels, frequency=frequency, start_offset_sec=start_offset_sec, max_seq_len_sec=max_seq_len_sec, sample_seq_len_sec=sample_seq_len_sec, stride_sec=stride_sec, include_partial_samples=include_partial_samples, nan_tolerance=nan_tolerance)\n",
    "        results = pool.starmap_async(f, enumerate(zarr_files))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    for result in results.get():\n",
    "        final_df = pd.concat([final_df, result])\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    total_samples = len(final_df)\n",
    "    return final_df, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
