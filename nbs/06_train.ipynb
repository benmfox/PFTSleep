{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "> Pew pew pew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, torch.nn as nn, lightning.pytorch as pl\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "\n",
    "from pftsleep.transformers import PatchTFTSimple\n",
    "from pftsleep.loss import masked_mae_loss, cosine_similarity_loss, masked_mse_loss, FocalLoss, KLDivLoss, huber_loss, patch_continuity_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Supervised PatchTFT Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchTFTSimpleLightning(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 learning_rate,\n",
    "                 train_size,\n",
    "                 batch_size,\n",
    "                 channels,\n",
    "                 metrics,\n",
    "                 precalculate_onebatch_stft_stats=False,\n",
    "                 use_sequence_padding_mask=False,\n",
    "                 loss_func='mse',\n",
    "                 max_lr=0.01,\n",
    "                 weight_decay=0.,\n",
    "                 epochs=100,\n",
    "                 one_cycle_scheduler=True,\n",
    "                 optimizer_type='Adam',\n",
    "                 scheduler_type='OneCycle',\n",
    "                 cross_attention=False, # not implemented\n",
    "                 use_mask=False,\n",
    "                 patch_continuity_loss=0, # indicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities\n",
    "                 huber_delta=None, # huber loss delta, not used otherwise\n",
    "                 **patchmeup_kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert loss_func.lower() in ['mae','mse','cosine','huber']\n",
    "        self.scheduler_type = scheduler_type\n",
    "        if self.scheduler_type is not None:\n",
    "            assert self.scheduler_type.lower() in ['onecycle', 'cosineannealingwarmrestarts'], \"scheduler must be either OneCycle, CosineAnnealingWarmRestarts, or None\"\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_lr = max_lr\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.metrics = metrics\n",
    "        self.one_cycle_scheduler = one_cycle_scheduler\n",
    "        self.channels = channels\n",
    "        self.use_mask = use_mask\n",
    "        self.loss_func = loss_func\n",
    "        self.huber_delta = huber_delta\n",
    "        self.patch_continuity_loss = patch_continuity_loss\n",
    "        self.precalculate_onebatch_stft_stats = precalculate_onebatch_stft_stats\n",
    "        self.use_sequence_padding_mask = use_sequence_padding_mask\n",
    "        self.cross_attention = cross_attention\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = PatchTFTSimple(**patchmeup_kwargs)\n",
    "    \n",
    "    def forward(self, x, sequence_padding_mask=None):\n",
    "        if self.training:\n",
    "            x_hat, y, training_mask, key_padding_mask = self.model(x, sequence_padding_mask=sequence_padding_mask)\n",
    "            return x_hat, y, training_mask, key_padding_mask\n",
    "        else:\n",
    "            z, x_hat, y, training_mask, key_padding_mask = self.model(x, sequence_padding_mask=sequence_padding_mask)\n",
    "            return z, x_hat, y, training_mask, key_padding_mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        if self.use_sequence_padding_mask:\n",
    "            x, _, sequence_padding_mask = batch\n",
    "        else:\n",
    "            x, _ = batch\n",
    "            sequence_padding_mask = None\n",
    "        x_hat, y, training_mask, key_padding_mask = self.model(x, sequence_padding_mask=sequence_padding_mask)\n",
    "        if self.patch_continuity_loss > 0:\n",
    "            patch_cont_loss = patch_continuity_loss(x_hat) * self.patch_continuity_loss\n",
    "        else:\n",
    "            patch_cont_loss = 0\n",
    "        if self.loss_func == 'cosine':\n",
    "            loss = cosine_similarity_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask)\n",
    "        elif self.loss_func in ['MAE','mae']:\n",
    "            loss = masked_mae_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask)\n",
    "        elif self.loss_func in ['Huber','huber']:\n",
    "            loss = huber_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask, delta=self.huber_delta if self.huber_delta is not None else 1)\n",
    "        else:\n",
    "            loss = masked_mse_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask)\n",
    "        loss = loss + patch_cont_loss\n",
    "        loss = loss.to(self.device)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        if self.patch_continuity_loss > 0:\n",
    "            self.log('patch_continuity_loss', patch_cont_loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        self.log_dict({f'train_{metric.__name__}':metric(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask) for metric in self.metrics}, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.use_sequence_padding_mask:\n",
    "            x, _, sequence_padding_mask = batch\n",
    "        else:\n",
    "            x, _ = batch\n",
    "            sequence_padding_mask = None\n",
    "        _, x_hat, y, training_mask, key_padding_mask = self.model(x, sequence_padding_mask=sequence_padding_mask)\n",
    "        if self.patch_continuity_loss > 0:\n",
    "            patch_cont_loss = patch_continuity_loss(x_hat) * self.patch_continuity_loss\n",
    "        else:\n",
    "            patch_cont_loss = 0\n",
    "        if self.loss_func == 'cosine':\n",
    "            loss = cosine_similarity_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask)\n",
    "        elif self.loss_func in ['MAE','mae']:\n",
    "            loss = masked_mae_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask)\n",
    "        elif self.loss_func in ['Huber','huber']:\n",
    "            loss = huber_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask, delta=self.huber_delta if self.huber_delta is not None else 1)\n",
    "        else:\n",
    "            loss = masked_mse_loss(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask)\n",
    "        loss = loss + patch_cont_loss\n",
    "        loss = loss.to(self.device)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        if self.patch_continuity_loss > 0:\n",
    "            self.log('patch_continuity_loss', patch_cont_loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        self.log_dict({f'valid_{metric.__name__}':metric(x_hat, y, training_mask, use_mask=self.use_mask, padding_mask=key_padding_mask) for metric in self.metrics}, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay) if self.optimizer_type.lower() == 'adamw' else\\\n",
    "                     torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        if self.scheduler_type.lower() == 'onecycle':\n",
    "            scheduler = OneCycleLR(optimizer, max_lr=self.max_lr, epochs=self.epochs, steps_per_epoch=(self.train_size//self.batch_size))\n",
    "            lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        elif self.scheduler_type.lower() == 'cosineannealingwarmrestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=self.epochs//10, T_mult=2, eta_min=1e-8) # lr max is initial LR\n",
    "            lr_scheduler = {'scheduler': scheduler, 'interval': 'epoch'}\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "class EncoderTestFreezingWeights(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(750,512)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class DecoderTest(pl.LightningModule):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.freeze()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(list(self.encoder.parameters()))\n",
    "        print(self.encoder.training)\n",
    "        # x = self.encoder(x)\n",
    "        # return x\n",
    "\n",
    "encoder = EncoderTestFreezingWeights()\n",
    "decoder = DecoderTest(encoder)\n",
    "\n",
    "decoder.training_step(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Training with Linear Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchTFT Masked Autoregression SS Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchTFTSleepStage(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 learning_rate, # desired learning rate, initial learning rate in if one_cycle_scheduler\n",
    "                 train_size, # the training data size (for one_cycle_scheduler=True)\n",
    "                 batch_size, # the batch size (for one_cycle_scheduler=True)\n",
    "                 linear_probing_head, # model head to train\n",
    "                 metrics=[], # metrics to calculate\n",
    "                 fine_tune=False, # indicator to finetune encoder model or perform linear probing and freeze encoder weights\n",
    "                 loss_fxn='CrossEntropy', # loss function to use, can be CrossEntropy or FocalLoss\n",
    "                 class_weights=None, # weights of classes to use in CE loss fxn\n",
    "                 gamma=2., # for focal loss\n",
    "                 label_smoothing=0, # label smoothing for cross entropy loss\n",
    "                 use_sequence_padding_mask=False, #indicator to use the sequence padding mask when training/in the loss fxn\n",
    "                 y_padding_mask=-100, # padded value that was added to target and indice to ignore when computing loss\n",
    "                 max_lr=0.01, # maximum learning rate for one_cycle_scheduler\n",
    "                 epochs=100, # number of epochs for one_cycle_scheduler\n",
    "                 one_cycle_scheduler=True, # indicator to use a one cycle scheduler to vary the learning rate \n",
    "                 weight_decay=0., # weight decay for Adam optimizer\n",
    "                 pretrained_encoder_path=None, # path of the pretrained model to use for linear probing\n",
    "                 optimizer_type='Adam', # optimizer to use, 'Adam' or 'AdamW'\n",
    "                 scheduler_type='OneCycle', # scheduler to use, 'OneCycle' or 'CosineAnnealingWarmRestarts'\n",
    "                 preloaded_model=None, # loaded pretrained model to use for linear probing\n",
    "                 torch_model_name='model', # name of the pytorch model within the lightning model module, this is to remove layers (for example lightning_model.pytorch_model.head = nn.Identity())\n",
    "                 remove_pretrain_layers=['head', 'mask'], # layers within the lightning model or lightning model.pytorch_model to remove\n",
    "                 return_softmax=True, # indicator to return softmax probabilities in forward and predict_step\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.encoder = preloaded_model\n",
    "        assert loss_fxn.lower() in ['crossentropy', 'focalloss', 'kldivloss'], \"loss_fxn must be either CrossEntropy or FocalLoss or KLDivLoss\"\n",
    "        if remove_pretrain_layers is not None and len(remove_pretrain_layers) > 0:\n",
    "            for l in remove_pretrain_layers:\n",
    "                if torch_model_name is not None:\n",
    "                    setattr(getattr(self.encoder, torch_model_name), l, nn.Identity())\n",
    "                else:\n",
    "                    setattr(self.encoder, l, nn.Identity())\n",
    "        self.scheduler_type = scheduler_type\n",
    "        if self.scheduler_type is not None:\n",
    "            assert self.scheduler_type.lower() in ['onecycle', 'cosineannealingwarmrestarts'], \"scheduler must be either OneCycle, CosineAnnealingWarmRestarts, or None\"\n",
    "        self.one_cycle_scheduler = one_cycle_scheduler\n",
    "        self.weight_decay = weight_decay\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_lr = max_lr\n",
    "        self.epochs = epochs\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.metrics = metrics\n",
    "        self.y_padding_mask = y_padding_mask\n",
    "        self.class_weights = class_weights\n",
    "        self.use_sequence_padding_mask = use_sequence_padding_mask\n",
    "        self.loss_fxn = loss_fxn\n",
    "        self.gamma = gamma\n",
    "        self.fine_tune = fine_tune\n",
    "        self.return_softmax = return_softmax\n",
    "        if not self.fine_tune:\n",
    "            self.encoder.freeze() # freeze the encoder weights\n",
    "            #self.encoder.eval() # should this be on here?\n",
    "        self.feedforward = linear_probing_head #RNNProbingHead #TimeDistributedConvolutionalFeedForward(**linear_probing_kwargs)#LinearProbingHead(**linear_probing_kwargs)\n",
    "        #self.feedforward = self.feedforward.to(self.device)\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.save_hyperparameters(ignore=['linear_probing_head', 'preloaded_model'])\n",
    "\n",
    "    def forward(self, x, sequence_padding_mask=None):\n",
    "        \"\"\"\n",
    "        In: [bs, n_channels, seq_len]\n",
    "        \"\"\"\n",
    "        x = self.encoder(x, sequence_padding_mask=sequence_padding_mask) # [bs, n_channels, d_model, n_ffts/n_patches]\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x = self.feedforward(x, return_softmax=self.return_softmax, sequence_padding_mask=sequence_padding_mask) # [bs, n_classes, pred_len_seconds]\n",
    "        return x\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if self.use_sequence_padding_mask:\n",
    "            x, _, sequence_padding_mask = batch\n",
    "        else:\n",
    "            x,_ = batch\n",
    "            sequence_padding_mask = None\n",
    "        x = self.encoder(x, sequence_padding_mask = sequence_padding_mask)\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        preds = self.feedforward(x, return_softmax=self.return_softmax, sequence_padding_mask=sequence_padding_mask)\n",
    "        return preds\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        if self.use_sequence_padding_mask:\n",
    "            x, y, sequence_padding_mask = batch\n",
    "        else:\n",
    "            x, y = batch\n",
    "            sequence_padding_mask = None\n",
    "        if not self.fine_tune:\n",
    "            self.encoder.eval()\n",
    "            with torch.no_grad():\n",
    "                x = self.encoder(x, sequence_padding_mask=sequence_padding_mask)\n",
    "        else:\n",
    "            x = self.encoder(x, sequence_padding_mask=sequence_padding_mask)\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x = self.feedforward(x, sequence_padding_mask=sequence_padding_mask)\n",
    "        if self.loss_fxn.lower() == 'kldivloss':\n",
    "            loss = KLDivLoss(weight=self.class_weights.to(x.device) if self.class_weights is not None else None, ignore_index=self.y_padding_mask)(x,y)\n",
    "        else:\n",
    "            ce_loss = nn.CrossEntropyLoss(weight=self.class_weights.to(x.device) if self.class_weights is not None else None, label_smoothing=self.label_smoothing, ignore_index=self.y_padding_mask)\n",
    "            focal_loss = FocalLoss(weight=self.class_weights.to(x.device) if self.class_weights is not None else None, gamma=self.gamma, ignore_index=self.y_padding_mask)\n",
    "            loss = ce_loss(x,y) if self.loss_fxn == 'CrossEntropy' else focal_loss(x,y)\n",
    "            self.log('train_ce_loss' if self.loss_fxn != 'CrossEntropy' else 'train_focal_loss', ce_loss(x,y) if self.loss_fxn != 'CrossEntropy' else focal_loss(x,y), prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "            self.log_dict({f'train_{metric.__name__}':metric(x, y) for metric in self.metrics}, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        #loss = loss.to(x.device)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.use_sequence_padding_mask:\n",
    "            x, y, sequence_padding_mask = batch\n",
    "        else:\n",
    "            x, y = batch\n",
    "            sequence_padding_mask = None\n",
    "        x = self.encoder(x, sequence_padding_mask=sequence_padding_mask) # val should be in eval mode\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x = self.feedforward(x, sequence_padding_mask=sequence_padding_mask)\n",
    "        if self.loss_fxn.lower() == 'kldivloss':\n",
    "            loss = KLDivLoss(weight=self.class_weights.to(x.device) if self.class_weights is not None else None, ignore_index=self.y_padding_mask)(x,y)\n",
    "        else:\n",
    "            ce_loss = nn.CrossEntropyLoss(weight=self.class_weights.to(x.device) if self.class_weights is not None else None, label_smoothing=self.label_smoothing, ignore_index=self.y_padding_mask)\n",
    "            focal_loss = FocalLoss(weight=self.class_weights.to(x.device) if self.class_weights is not None else None, gamma=self.gamma, ignore_index=self.y_padding_mask)\n",
    "            loss = ce_loss(x,y) if self.loss_fxn == 'CrossEntropy' else focal_loss(x,y)\n",
    "            self.log('val_ce_loss' if self.loss_fxn != 'CrossEntropy' else 'val_focal_loss', ce_loss(x,y) if self.loss_fxn != 'CrossEntropy' else focal_loss(x,y), prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "            self.log_dict({f'val_{metric.__name__}':metric(x, y) for metric in self.metrics}, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        #loss = loss.to(x.device)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay) if self.optimizer_type.lower() == 'adamw' else\\\n",
    "                     torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        if self.scheduler_type.lower() == 'onecycle':\n",
    "            scheduler = OneCycleLR(optimizer, max_lr=self.max_lr, epochs=self.epochs, steps_per_epoch=(self.train_size//self.batch_size))\n",
    "            lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        elif self.scheduler_type.lower() == 'cosineannealingwarmrestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=self.epochs//10, T_mult=2, eta_min=1e-8)\n",
    "            lr_scheduler = {'scheduler': scheduler, 'interval': 'epoch'}\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
