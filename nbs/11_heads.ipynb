{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31197e9b",
   "metadata": {},
   "source": [
    "# SSL, Fine Tuning, and Linear Probing Heads\n",
    "\n",
    "> none of these work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ec6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67538189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89003ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from pftsleep.transformers import TSTEncoderLayer\n",
    "from pftsleep.layers import Transpose, get_activation_fn, EmbeddingAugmentations\n",
    "from pftsleep.augmentations import create_patch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37998f1e",
   "metadata": {},
   "source": [
    "## Linear Probing and Fine Tuning Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RNNProbingHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 n_classes, \n",
    "                 contrastive=False, \n",
    "                 module='GRU', \n",
    "                 linear_dropout=0., \n",
    "                 rnn_dropout=0., \n",
    "                 num_rnn_layers=1, \n",
    "                 act='gelu', \n",
    "                 pool='average', # 'average' or 'max' or 'majority'\n",
    "                 temperature=2., # only used if pool='majority'\n",
    "                 n_linear_layers=1, \n",
    "                 predict_every_n_patches=1, \n",
    "                 bidirectional=True, \n",
    "                 affine=False, \n",
    "                 shared_embedding=True,\n",
    "                 augmentations=None,\n",
    "                 augmentation_mask_ratio=0.,\n",
    "                 augmentation_dims_to_shuffle=[1,2,3],\n",
    "                 norm=None # one of [None, 'pre', 'post']\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.c_in = c_in\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "        self.linear_dropout = linear_dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_classes = n_classes\n",
    "        self.n_linear_layers = n_linear_layers\n",
    "        self.act = act\n",
    "        self.pool = pool\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.predict_every_n_patches = predict_every_n_patches\n",
    "        self.affine = affine\n",
    "        self.shared_embedding = shared_embedding\n",
    "        self.contrastive = contrastive\n",
    "        self.augmentations = augmentations\n",
    "        self.temperature = temperature\n",
    "\n",
    "        if augmentations is not None:\n",
    "            self.augmentations = augmentations\n",
    "            self.augmentation_layer = EmbeddingAugmentations(augmentations=augmentations, dims_to_shuffle=augmentation_dims_to_shuffle, patch_mask_ratio=augmentation_mask_ratio, jitter_zero_mask_ratio=augmentation_mask_ratio)\n",
    "        if norm in ['pre', 'post']:\n",
    "            self.do_norm = norm\n",
    "            self.norm_layer = nn.BatchNorm2d(self.c_in)# nn.LayerNorm(self.hidden_size * (2 if self.bidirectional else 1))\n",
    "        else:\n",
    "            self.do_norm = False\n",
    "            self.norm_layer = nn.Identity()\n",
    "        self.flatten = nn.Flatten(start_dim=-3, end_dim=-2)\n",
    "        #self.attn = MultiheadAttentionCustom(d_model=input_size, n_heads=n_heads)\n",
    "\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,c_in,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,c_in,1))\n",
    "\n",
    "        if module.lower() == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_rnn_layers, bias=True, batch_first=True, dropout=rnn_dropout, bidirectional=self.bidirectional)\n",
    "            if self.contrastive:\n",
    "                self.rnn2 = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_rnn_layers, bias=True, batch_first=True, dropout=rnn_dropout, bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_rnn_layers, bias=True, batch_first=True, dropout=rnn_dropout, bidirectional=self.bidirectional)\n",
    "            if self.contrastive:\n",
    "                self.rnn2 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_rnn_layers, bias=True, batch_first=True, dropout=rnn_dropout, bidirectional=self.bidirectional)\n",
    "        self.layers = []\n",
    "        #n_nuerons = int(((hidden_size*2 if self.bidirectional else hidden_size) - n_classes)/self.n_linear_layers) if self.n_linear_layers >= 1 else 0\n",
    "        # if self.n_linear_layers == 1:\n",
    "        #     self.layers.append(nn.Sequential(nn.Linear(hidden_size*2 if self.bidirectional else hidden_size, n_classes)))\n",
    "        # else:\n",
    "        #     for i in range(self.n_linear_layers - 1):\n",
    "        #         self.layers.append(nn.Sequential(nn.Linear(n_nuerons*i+(hidden_size*2 if self.bidirectional else hidden_size), (hidden_size*2 if self.bidirectional else hidden_size)+n_nuerons*(i+1)), nn.BatchNorm2d(c_in), get_activation_fn(act), nn.Dropout(linear_dropout)))\n",
    "        #     self.layers.append(nn.Sequential(nn.Linear((hidden_size*2 if self.bidirectional else hidden_size)+n_nuerons*(i+1), n_classes)))\n",
    "        # self.layers = nn.Sequential(*self.layers)\n",
    "        f = 2 if self.bidirectional else 1\n",
    "        f = f * 2 if self.contrastive else f\n",
    "        if not self.shared_embedding:\n",
    "            for _ in range(self.c_in): self.layers.append(nn.Linear(hidden_size*f, n_classes))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size*f, n_classes)) # [bs x c_in x n_patches x n_classes]\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        #self.fc = nn.Linear(hidden_size*2 if self.bidirectional else hidden_size, n_classes)\n",
    "        self.pool_kernel_size = int(self.c_in*self.predict_every_n_patches) if self.predict_every_n_patches > 1 else self.c_in\n",
    "\n",
    "        if pool in ['average', 'max']:\n",
    "            self.pool_layer = nn.AvgPool1d(kernel_size=self.pool_kernel_size, stride=self.pool_kernel_size) if self.pool == 'average' else nn.MaxPool1d(kernel_size=self.pool_kernel_size, stride=self.pool_kernel_size)\n",
    "        elif pool == 'majority':\n",
    "            self.pool_layer = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Pooling type {pool} not supported. Choose one of ['average', 'max', 'majority']\")\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, return_softmax=False, sequence_padding_mask=None):\n",
    "        \"\"\"\n",
    "        GRU input expects [bs x seq_len x H_in]\n",
    "        x: [bs x n channels x d model x n patches]\n",
    "        \"\"\"\n",
    "        # pass each channel through GRU and concat\n",
    "        bs, n_channels, d_model, n_patches = x.shape\n",
    "        #x = x.permute(0,1,3,2) #[bs x n channels x n patches x d model]\n",
    "        #x = x.reshape(-1, n_patches, d_model)\n",
    "        #x, _ = self.attn(x)\n",
    "        #x = x.reshape(bs, n_channels, n_patches, d_model).permute(0,2,1,3) # with just rnn only need permute(0, 3, 1, 2) [bs x n patches x n channels x d model]\n",
    "        if self.augmentations is not None:\n",
    "            x = self.augmentation_layer(x)\n",
    "        if self.contrastive:\n",
    "            x1 = x[:,:,:(d_model//2),:].clone()\n",
    "            x2 = x[:,:,(d_model//2):,:].clone()\n",
    "\n",
    "            x1, x2 = x1.permute(0, 3, 1, 2), x2.permute(0, 3, 1, 2)\n",
    "            x1, x2 = self.flatten(x1), self.flatten(x2)\n",
    "            x1,_ = self.rnn(x1)\n",
    "            x2,_ = self.rnn2(x2)\n",
    "\n",
    "            x1, x2 = torch.reshape(x1, (bs, n_patches, n_channels, -1)), torch.reshape(x2, (bs, n_patches, n_channels, -1))\n",
    "            x_cat = torch.cat([x1,x2], dim=-1)\n",
    "        else:\n",
    "            if self.do_norm == 'pre':\n",
    "                x = self.norm_layer(x) # [bs x n channels x d model x n patches]\n",
    "            x_cat = x.permute(0, 3, 1, 2) # [bs x n patches x n channels x d model] \n",
    "            x_cat = self.flatten(x_cat) # [bs x n patches * n channels x d model] \n",
    "            x_cat,_ = self.rnn(x_cat) # [bs x n patches * n channels x hidden_size]\n",
    "            x_cat = torch.reshape(x_cat, (bs, n_patches, n_channels, -1)) # [bs x n patches x n channels x hidden_size]\n",
    "            if self.do_norm == 'post':\n",
    "                x_cat = x_cat.permute(0, 2, 3, 1)  # [bs, n_channels, hidden_size, n_patches]\n",
    "                x_cat = self.norm_layer(x_cat)\n",
    "                x_cat = x_cat.permute(0, 3, 1, 2)  # [bs, n_patches, n_channels, hidden_size]\n",
    "            \n",
    "        if not self.shared_embedding:\n",
    "            x_out = []\n",
    "            for i in range(self.c_in):\n",
    "                z_ = self.layers[i](x_cat[:,:,i,:])\n",
    "                x_out.append(z_)\n",
    "            x_cat = torch.stack(x_out, dim=2) # [bs x n patches x n channels x n classes]\n",
    "        else:\n",
    "            x_cat = self.layers(x_cat) # [bs x n patches x n channels x n classes]\n",
    "        \n",
    "        if self.affine:\n",
    "            x_cat = x_cat * self.affine_weight\n",
    "            x_cat = x_cat + self.affine_bias\n",
    "        x_cat = self.flatten(x_cat) # [bs x n patches * n channels x n classes]\n",
    "        if self.pool == 'majority':\n",
    "            # Group patches that belong to same segment (30 seconds)\n",
    "            n_segments = (n_patches * n_channels) // self.pool_kernel_size\n",
    "            x_segments = x_cat.view(bs, n_segments, self.pool_kernel_size, self.n_classes) # [bs x n_segments x patches_per_segment x n_classes]\n",
    "            # vote with a temperature scaling\n",
    "            soft_votes = self.pool_layer(x_segments / self.temperature) # [bs x n_segments x patches_per_segment x n_classes]\n",
    "            # majority vote!\n",
    "            segment_decisions = soft_votes.sum(dim=2) # [bs x n_segments x n_classes]\n",
    "            segment_decisions = segment_decisions.transpose(1,2) # [bs x n_classes x n_segments]\n",
    "        else:\n",
    "            x_cat = x_cat.transpose(1,2)\n",
    "            segment_decisions = self.pool_layer(x_cat)\n",
    "        #x = x.unfold(dimension=-1, size=self.avg_pool_kernel_stride, step=self.avg_pool_kernel_stride).median(dim=-1)[0]\n",
    "        #x = x.transpose(1,2)\n",
    "        #x = x.reshape(x.shape[0], self.c_in, -1, self.n_classes)\n",
    "        if return_softmax:\n",
    "            segment_decisions = self.softmax(segment_decisions)\n",
    "        return segment_decisions\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RNNProbingHeadExperimental(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 n_classes, \n",
    "                 contrastive=False, # deprecated\n",
    "                 module='GRU', \n",
    "                 linear_dropout=0., \n",
    "                 rnn_dropout=0., \n",
    "                 num_rnn_layers=1, \n",
    "                 act='gelu', \n",
    "                 pool='average', # 'average' or 'max' or 'majority'\n",
    "                 temperature=2., # only used if pool='majority'\n",
    "                 predict_every_n_patches=1, \n",
    "                 bidirectional=True, \n",
    "                 affine=False, \n",
    "                 augmentations=None,\n",
    "                 augmentation_mask_ratio=0.,\n",
    "                 augmentation_dims_to_shuffle=[1,2,3],\n",
    "                 pre_norm=True, # one of [None, 'pre', 'post']\n",
    "                 mlp_final_head=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.c_in = c_in\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "        self.linear_dropout = linear_dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.pool = pool\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.predict_every_n_patches = predict_every_n_patches\n",
    "        self.affine = affine\n",
    "        self.augmentations = augmentations\n",
    "        self.temperature = temperature\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "        if augmentations is not None:\n",
    "            self.augmentations = augmentations\n",
    "            self.augmentation_layer = EmbeddingAugmentations(augmentations=augmentations, dims_to_shuffle=augmentation_dims_to_shuffle, patch_mask_ratio=augmentation_mask_ratio, jitter_zero_mask_ratio=augmentation_mask_ratio)\n",
    "        if pre_norm:\n",
    "            self.norm_layer = nn.BatchNorm2d(self.c_in)# nn.LayerNorm(self.hidden_size * (2 if self.bidirectional else 1))\n",
    "        else:\n",
    "            self.norm_layer = nn.Identity()\n",
    "        self.flatten = nn.Flatten(start_dim=-2, end_dim=-1) # flatten d_model and n channels\n",
    "\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,c_in,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,c_in,1))\n",
    "\n",
    "        if module.lower() == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=self.input_size*self.c_in, hidden_size=self.hidden_size, num_layers=num_rnn_layers, bias=True, batch_first=True, dropout=rnn_dropout, bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size=self.input_size*self.c_in, hidden_size=self.hidden_size, num_layers=num_rnn_layers, bias=True, batch_first=True, dropout=rnn_dropout, bidirectional=self.bidirectional)\n",
    "        self.layers = []\n",
    "        \n",
    "        f = 2 if self.bidirectional else 1\n",
    "        \n",
    "        if mlp_final_head:\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(hidden_size*f, hidden_size*f//2),\n",
    "                nn.LayerNorm([hidden_size*f//2]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(linear_dropout),\n",
    "                nn.Linear(hidden_size*f//2, n_classes)\n",
    "            ))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size*f, n_classes)) # [bs x c_in x n_patches x n_classes]\n",
    "        \n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        #self.fc = nn.Linear(hidden_size*2 if self.bidirectional else hidden_size, n_classes)\n",
    "        self.pool_kernel_size = int(self.predict_every_n_patches)\n",
    "\n",
    "        if pool in ['average', 'max']:\n",
    "            self.pool_layer = nn.AvgPool1d(kernel_size=self.pool_kernel_size, stride=self.pool_kernel_size) if self.pool == 'average' else nn.MaxPool1d(kernel_size=self.pool_kernel_size, stride=self.pool_kernel_size)\n",
    "        elif pool == 'majority':\n",
    "            self.pool_layer = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Pooling type {pool} not supported. Choose one of ['average', 'max', 'majority']\")\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, return_softmax=False, sequence_padding_mask=None):\n",
    "        \"\"\"\n",
    "        GRU input expects [bs x seq_len x H_in]\n",
    "        x: [bs x n channels x d model x n patches]\n",
    "        \"\"\"\n",
    "        # pass each channel through GRU and concat\n",
    "        bs, n_channels, d_model, n_patches = x.shape\n",
    "        if self.augmentations is not None:\n",
    "            x = self.augmentation_layer(x)\n",
    "        \n",
    "        if self.pre_norm:\n",
    "            x = self.norm_layer(x) # [bs x n channels x d model x n patches]\n",
    "        x_cat = x.permute(0, 3, 1, 2) # [bs x n patches x n channels x d model] \n",
    "        \n",
    "        if self.affine:\n",
    "            x_cat = x_cat * self.affine_weight\n",
    "            x_cat = x_cat + self.affine_bias\n",
    "        \n",
    "        x_cat = self.flatten(x_cat) # [bs x n patches x n channels * d model] \n",
    "        if sequence_padding_mask is not None:\n",
    "\n",
    "            patch_len = sequence_padding_mask.shape[-1] // n_patches\n",
    "            key_padding_mask = create_patch(sequence_padding_mask, patch_len=patch_len, stride=patch_len, constant_pad=True, constant_pad_value=1, max_seq_len=sequence_padding_mask.shape[-1])\n",
    "            key_padding_mask = torch.all(key_padding_mask, -1).squeeze(-1).int() # [bs x n patches]\n",
    "            # calc lengths of each sequence, 0s are valid inputs, 1s are padding\n",
    "            lengths = n_patches - torch.flip(key_padding_mask, dims=[1]).argmin(dim=1) # find padding index at the end of each batch sample (only ignoring paddingh at the end of the sequence)\n",
    "            #lengths = (key_padding_mask == 0).sum(dim=1).cpu()\n",
    "            lengths = lengths.cpu()\n",
    "            x_cat = pack_padded_sequence(x_cat, lengths, batch_first=True, enforce_sorted=False)\n",
    "        x_cat,_ = self.rnn(x_cat) # [bs x n patches x hidden_size*f]\n",
    "\n",
    "        if sequence_padding_mask is not None:\n",
    "            x_cat, _ = pad_packed_sequence(x_cat, batch_first=True, total_length=n_patches)\n",
    "        x_cat = self.layers(x_cat) # [bs x n patches x n classes]\n",
    "        \n",
    "        if self.pool == 'majority':\n",
    "            # Group patches that belong to same segment (30 seconds)\n",
    "            n_segments = n_patches // self.pool_kernel_size\n",
    "            x_segments = x_cat.view(bs, n_segments, self.pool_kernel_size, self.n_classes) # [bs x n_segments x patches_per_segment x n_classes]\n",
    "            # vote with a temperature scaling\n",
    "            soft_votes = self.pool_layer(x_segments / self.temperature) # [bs x n_segments x patches_per_segment x n_classes]\n",
    "            # majority vote!\n",
    "            segment_decisions = soft_votes.sum(dim=2) # [bs x n_segments x n_classes]\n",
    "            segment_decisions = segment_decisions.transpose(1,2) # [bs x n_classes x n_segments]\n",
    "        else:\n",
    "            x_cat = x_cat.transpose(1,2)\n",
    "            segment_decisions = self.pool_layer(x_cat)\n",
    "        #x = x.unfold(dimension=-1, size=self.avg_pool_kernel_stride, step=self.avg_pool_kernel_stride).median(dim=-1)[0]\n",
    "        #x = x.transpose(1,2)\n",
    "        #x = x.reshape(x.shape[0], self.c_in, -1, self.n_classes)\n",
    "        if return_softmax:\n",
    "            segment_decisions = self.softmax(segment_decisions)\n",
    "        return segment_decisions\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 30])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "m = RNNProbingHeadExperimental(c_in=7, \n",
    "                                pool='average', \n",
    "                                input_size = 384, \n",
    "                                bidirectional=True,\n",
    "                                affine=False, \n",
    "                                hidden_size=1200,\n",
    "                                module='GRU',\n",
    "                                n_classes=4,\n",
    "                                predict_every_n_patches=32,\n",
    "                                rnn_dropout=0.,\n",
    "                                num_rnn_layers=1,\n",
    "                                linear_dropout=0.,\n",
    "                                mlp_final_head=False,\n",
    "                                pre_norm=True)\n",
    "x = torch.randn((4,7,384,960))\n",
    "sequence_padding_mask = torch.zeros(4,960)\n",
    "sequence_padding_mask[:,-32:] = 1\n",
    "m(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f7774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 30])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "m = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\n",
    "x = torch.randn((4,7,384,960))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90572d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "m = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\n",
    "x = torch.randn((4,7,512*2,3600))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformerDecoderProbingHead(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c_in, \n",
    "                 d_model, \n",
    "                 n_classes, \n",
    "                 norm='BatchNorm', \n",
    "                 dropout=0., \n",
    "                 act='gelu', \n",
    "                 d_ff=2048, \n",
    "                 num_layers=1, \n",
    "                 n_heads=2, \n",
    "                 predict_every_n_patches=1, \n",
    "                 affine=False, \n",
    "                 shared_embedding=True\n",
    "                 ):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.act = act\n",
    "        self.c_in = c_in\n",
    "        self.dropout = dropout\n",
    "        self.n_classes = n_classes\n",
    "        self.predict_every_n_patches = predict_every_n_patches\n",
    "        self.affine = affine\n",
    "        self.shared_embedding = shared_embedding\n",
    "\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,c_in,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,c_in,1))\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=-3, end_dim=-2)\n",
    "        norm_m = nn.BatchNorm1d(d_model) if \"batch\" in norm.lower() else nn.LayerNorm([c_in, n_patches, d_model])\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout, dim_feedforward=d_ff, batch_first = True, activation=act)\n",
    "        self.decoder =  nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.layers = []\n",
    "        if not self.shared_embedding:\n",
    "            for _ in range(self.c_in): self.layers.append(nn.Linear(d_model, n_classes))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(d_model, n_classes)) # [bs x c_in x n_patches x n_classes]\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "        avg_pool_kernel_stride = int(self.c_in*self.predict_every_n_patches) if self.predict_every_n_patches > 1 else self.c_in\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=avg_pool_kernel_stride, stride=avg_pool_kernel_stride)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, return_softmax=False):\n",
    "        \"\"\"\n",
    "        x: [bs x n channels x d model x n patches]\n",
    "        \"\"\"\n",
    "        # pass each channel through GRU and concat\n",
    "        bs, n_channels, d_model, n_patches = x.shape\n",
    "        x = x.permute(0,1,3,2) #[bs x n channels x n patches x d model]\n",
    "        x = x.reshape(-1, n_patches, d_model)\n",
    "        x = self.decoder(x,x) # x is both target and memory\n",
    "        x = x.reshape(bs, n_channels, n_patches, d_model).permute(0,2,1,3) # with just rnn only need permute(0, 3, 1, 2) [bs x n patches x n channels x d model]\n",
    "\n",
    "        if not self.shared_embedding:\n",
    "            x_out = []\n",
    "            for i in range(self.c_in):\n",
    "                z_ = self.layers[i](x[:,:,i,:])\n",
    "                x_out.append(z_)\n",
    "            x = torch.stack(x_out, dim=2) # [bs x n patches x n channels x n classes]\n",
    "        else:\n",
    "            x = self.layers(x) # [bs x n patches x n channels x n classes]\n",
    "        \n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.avg_pool(x)\n",
    "        #x = x.transpose(1,2)\n",
    "        #x = x.reshape(x.shape[0], self.c_in, -1, self.n_classes)\n",
    "        if return_softmax:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d7c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "layer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\n",
    "x = torch.randn((4, 7, 512, 3600))\n",
    "\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c066ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DecoderFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by\n",
    "    a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 c_in, # the number of input channels\n",
    "                 predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n",
    "                 num_layers,\n",
    "                 d_ff,\n",
    "                 attn_dropout,\n",
    "                 res_attention, \n",
    "                 pre_norm, \n",
    "                 store_attn,\n",
    "                 n_heads,\n",
    "                 shared_embedding,\n",
    "                 affine,\n",
    "                 n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
    "                 d_model, # the dimension of the transformer model\n",
    "                 norm='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n",
    "                 act='gelu', # activation function to use between layers, 'gelu' or 'relu'\n",
    "                 dropout=0. # dropout in between linear layers\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert n_classes >= 2, \"The number of classes must be >= 2. If you want to perform a binary task, pass in the value 2.\"\n",
    "        self.c_in = c_in\n",
    "        self.n_classes = n_classes\n",
    "        self.predict_every_n_patches = predict_every_n_patches\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.res_attention = res_attention\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "        self.n_heads = n_heads\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.shared_embedding = shared_embedding\n",
    "        self.affine = affine\n",
    "\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,c_in,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,c_in,1))\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([TSTEncoderLayer(d_model=d_model, n_heads=n_heads, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      activation=act, res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(num_layers)])\n",
    "        self.layers = []\n",
    "        if not self.shared_embedding:\n",
    "            for _ in range(self.c_in): self.layers.append(nn.Linear(d_model, n_classes))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(d_model, n_classes)) # [bs x c_in x n_patches x n_classes]\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        \n",
    "        #self.linear_layer = nn.Sequential(nn.Linear(d_model, self.win_length), nn.BatchNorm2d(c_in) if \"batch\" in norm.lower() else nn.LayerNorm([c_in, n_patches, self.win_length]), get_activation_fn(act), nn.Dropout(dropout))\n",
    "        #self.flatten = nn.Flatten(start_dim=-2) # now flatten everything to get full length sequences\n",
    "        # now convolve with stride/kernel = frequency * n second predicitions\n",
    "        #final_kernel_stride = int(frequency*predict_every_seconds) # this will make one predictions for every predict_every_seconds seq length\n",
    "        #self.conv1d = nn.Conv1d(in_channels=c_in, out_channels=n_classes, kernel_size=final_kernel_stride, stride=final_kernel_stride)\n",
    "        #self.unfold_size = int(self.predict_every_seconds/(self.win_length/self.frequency))\n",
    "        self.flatten = nn.Flatten(start_dim=-3, end_dim=-2)\n",
    "        self.avg_pool_kernel_stride = int(self.c_in*self.predict_every_n_patches) if self.predict_every_n_patches > 1 else self.c_in\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=self.avg_pool_kernel_stride, stride=self.avg_pool_kernel_stride)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, key_padding_mask=None, return_softmax=False):\n",
    "        \"\"\"\n",
    "        Make sure to freeze the encoder and remove its pretrain head! \n",
    "\n",
    "        in: [bs x n channels x d model x n patches]\n",
    "        out: [bs x n classes x pred len]\n",
    "        \"\"\"\n",
    "        bs, n_channels, d_model, n_patches = x.shape\n",
    "        z = x.permute(0,1,3,2) #[bs x n channels x n patches x d model]\n",
    "        z = z.reshape(-1, n_patches, d_model) # z: [bs * nvars x num_patch x d_model]\n",
    "        if self.res_attention:\n",
    "            scores = None\n",
    "            for mod in self.decoder_layers:\n",
    "                z, scores = mod(z, prev=scores, key_padding_mask=key_padding_mask) # z: [bs * n_vars x num_patch x d_model], scores: [bs * n_vars x n_heads x num_patch x num_patch]\n",
    "        else:\n",
    "            for mod in self.decoder_layers: \n",
    "                z = mod(z, key_padding_mask=key_padding_mask) # z: [bs * n_vars x num_patch x d_model]\n",
    "        z = z.reshape(bs, n_channels, n_patches, d_model).permute(0,2,1,3) # z: [bs x n patches x n channels x d model]\n",
    "        if not self.shared_embedding:\n",
    "            x_out = []\n",
    "            for i in range(self.c_in):\n",
    "                z_ = self.layers[i](z[:,:,i,:])\n",
    "                x_out.append(z_)\n",
    "            z = torch.stack(x_out, dim=2) # [bs x n patches x n channels x n classes]\n",
    "        else:\n",
    "            z = self.layers(z) # [bs x n patches x n channels x n classes]\n",
    "\n",
    "        if self.affine:\n",
    "            z = z * self.affine_weight\n",
    "            z = z + self.affine_bias\n",
    "        z = self.flatten(z) # # [bs x n channels * n patches x n classes]\n",
    "        z = z.transpose(1,2) # [bs x n classes x n channels * n patches]\n",
    "        z  = self.avg_pool(z)\n",
    "        #z = z.unfold(-2, size=self.unfold_size, step=self.unfold_size).sum(dim=-1) # take 30 seconds and sum up logits [bs x n channels x 30sec patches x n_classes]\n",
    "        \n",
    "        #z = self.flatten(z)\n",
    "        #z=z.sum(dim=1) \n",
    "        #z = z.transpose(1,2)\n",
    "        #z = self.conv1d(z)\n",
    "        if return_softmax:\n",
    "            z = self.softmax(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265e3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "c_in = 7\n",
    "frequency = 125\n",
    "win_length=750 \n",
    "overlap = 0.\n",
    "hop_length=win_length - int(overlap*win_length)\n",
    "max_seq_len_sec = (6*3600) # for dataloader\n",
    "#seq_len_sec = sample_stride = 3*3600 # for dataloader\n",
    "max_seq_len = max_seq_len_sec*frequency # for model\n",
    "#n_patches = n_fft // 2 + 1\n",
    "n_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n",
    "\n",
    "#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\n",
    "x = torch.randn(2,c_in,512,n_patches)\n",
    "\n",
    "model = DecoderFeedForward(c_in=c_in,\n",
    "                           predict_every_n_patches=5,\n",
    "                           num_layers=1,\n",
    "                           d_ff = 2048,\n",
    "                           attn_dropout=0.,\n",
    "                           res_attention = False,\n",
    "                           pre_norm = False,\n",
    "                           store_attn = False,\n",
    "                           n_heads=2,\n",
    "                           affine=False,\n",
    "                           shared_embedding=False,\n",
    "                           n_classes=5,\n",
    "                           d_model=512,\n",
    "                           norm='BatchNorm',\n",
    "                           act='gelu',\n",
    "                           dropout=0.\n",
    "                           )\n",
    "\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeDistributedConvolutionalFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension.\n",
    "    Then, a convolutional transpose is used to extrapolate the data to its original form.\n",
    "    Finally, a final convolution is used to predict the classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 c_in, # the number of input channels\n",
    "                 frequency, # the frequency of the original channels\n",
    "                 predict_every_seconds, # for a given sequence of length m with frequency f, number of predictions\n",
    "                 n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
    "                 win_length, # the convolved patch length, the first step in this is to do a linear layer to this dimension\n",
    "                 d_model, # the dimension of the transformer model\n",
    "                 affine = False,\n",
    "                 shared_embedding=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_in = c_in\n",
    "        self.n_classes = n_classes\n",
    "        self.predict_every_seconds = predict_every_seconds\n",
    "        self.d_model = d_model\n",
    "        self.win_length = win_length\n",
    "        self.frequency = frequency\n",
    "        self.layers = []\n",
    "        self.affine = affine\n",
    "        self.shared_embedding = shared_embedding\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,c_in,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,c_in,1))\n",
    "\n",
    "        if not self.shared_embedding:\n",
    "            for _ in range(self.c_in): self.layers.append(nn.Linear(d_model, win_length))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(d_model, win_length)) # [bs x c_in x n_patches x n_classes]\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        # now we should probably to a 2d convtranspose to get to the original number of channels, now the channels and patch len are back to normal size\n",
    "        self.flatten = nn.Flatten(start_dim=-2) # now flatten everything to get full length sequences\n",
    "        # now convolve with stride/kernel = frequency * n second predicitions\n",
    "        final_kernel_stride = int(frequency*predict_every_seconds) # this will make one predictions for every predict_every_seconds seq length\n",
    "        self.conv1d = nn.Conv1d(in_channels=c_in, out_channels=n_classes, kernel_size=final_kernel_stride, stride=final_kernel_stride)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, return_softmax=False):\n",
    "        \"\"\"\n",
    "        Make sure to freeze the encoder and remove its pretrain head! \n",
    "\n",
    "        in: [bs x n channels x d model x n patches]\n",
    "        out: [bs x n classes x pred len]\n",
    "        \"\"\"\n",
    "        x = x.permute(0,3,1,2) # [bs x n patches x n channels x d model]\n",
    "        if not self.shared_embedding:\n",
    "            x_out = []\n",
    "            for i in range(self.c_in):\n",
    "                z_ = self.layers[i](x[:,:,i,:])\n",
    "                x_out.append(z_)\n",
    "            x = torch.stack(x_out, dim=2) # [bs x n patches x n channels x patch len]\n",
    "        else:\n",
    "            x = self.layers(x) # [bs x n patches x n channels x patch len]\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        x = x.transpose(1,2) # [bs x n channels x n patches x patch len]\n",
    "        x = self.flatten(x)\n",
    "        x = self.conv1d(x)\n",
    "        if return_softmax:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearProbingHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 c_in, # the number of input channels in the original input\n",
    "                 predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n",
    "                 n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
    "                 input_size, # the dimension of the transformer model\n",
    "                 n_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n",
    "                 num_patch,\n",
    "                 shared_embedding=True, # whether or not to have a dense layer per channel or one layer per channel\n",
    "                 affine=True, # include learnable parameters to weight predictions\n",
    "                 norm='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n",
    "                 act='gelu', # activation function to use between layers, 'gelu' or 'relu'\n",
    "                 dropout=0. # dropout in between linear layers\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.n_classes = n_classes\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.affine = affine\n",
    "        self.shared_embedding = shared_embedding\n",
    "        self.predict_every_n_patches = predict_every_n_patches\n",
    "        self.num_patch = num_patch\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1,c_in,1))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1,c_in,1))\n",
    "        \n",
    "        n_nuerons = int((self.n_classes - self.input_size)/self.n_layers) if self.n_layers >= 1 else 0\n",
    "\n",
    "        self.layers = []\n",
    "        if self.n_layers == 1:\n",
    "            if not shared_embedding:\n",
    "                for _ in range(self.c_in): self.layers.append(nn.Linear(input_size, n_classes))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(input_size, n_classes)) # [bs x c_in x n_patches x n_classes]\n",
    "        else:\n",
    "            if not shared_embedding:\n",
    "                for _ in range(self.c_in):\n",
    "                    channel_layers = []\n",
    "                    for i in range(self.n_layers-1):\n",
    "                        channel_layers += [nn.Linear(n_nuerons*i+input_size, input_size+n_nuerons*(i+1)), nn.BatchNorm1d(self.num_patch) if \"batch\" in norm.lower() else nn.LayerNorm(1), get_activation_fn(act), nn.Dropout(dropout)]\n",
    "                    channel_layers += [nn.Linear(input_size+n_nuerons*(i+1), self.n_classes)]\n",
    "                    self.layers.append(nn.Sequential(*channel_layers))\n",
    "            else:\n",
    "                for i in range(self.n_layers-1):\n",
    "                    self.layers.append(nn.Sequential(nn.Linear(n_nuerons*i+input_size, input_size+n_nuerons*(i+1)), Transpose(1,2), nn.BatchNorm2d(c_in) if \"batch\" in norm.lower() else nn.LayerNorm(c_in), Transpose(1,2), get_activation_fn(act), nn.Dropout(dropout)))\n",
    "                self.layers.append(nn.Linear(input_size+n_nuerons*(i+1), self.n_classes))\n",
    "        self.layers = nn.Sequential(*self.layers) # [bs x c_in x n_patches x n_classes], need to average or max over channels\n",
    "        self.flatten = nn.Flatten(start_dim=-3, end_dim=-2)\n",
    "        self.avg_pool_kernel_stride = int(self.c_in*self.predict_every_n_patches) if self.predict_every_n_patches > 1 else self.c_in\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=self.avg_pool_kernel_stride, stride=self.avg_pool_kernel_stride)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, return_softmax=False):\n",
    "        \"\"\"\n",
    "        Make sure to freeze the encoder and remove its pretrain head! \n",
    "\n",
    "        in: [bs x n channels x d model x n patches]\n",
    "        out: [bs x n classes x pred len]\n",
    "        \"\"\"\n",
    "        x = x.permute(0,3,1,2) # [bs x n patches x n channels x d_model]\n",
    "        if not self.shared_embedding:\n",
    "            x_out = []\n",
    "            for i in range(self.c_in): \n",
    "                z = self.layers[i](x[:,:,i,:])\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=2) # [bs x n patches x n channels x n_classes]\n",
    "        else:\n",
    "            x = self.layers(x) # [bs x n patches x n channels x n_classes]\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        x = self.flatten(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x  = self.avg_pool(x)\n",
    "        if return_softmax:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b22b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "m = LinearProbingHead(c_in=7, \n",
    "                      input_size = 512, \n",
    "                      predict_every_n_patches=5,\n",
    "                      n_classes=5,\n",
    "                      n_layers=3,\n",
    "                      shared_embedding=True,\n",
    "                      affine=True,\n",
    "                      num_patch=3600,\n",
    "                      dropout=0.1)\n",
    "\n",
    "x = torch.randn((4,7,512,3600))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09286c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TimeDistributedFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed forward head that uses a convolutional layer to reduce channel dimensionality\n",
    "    Followed by a feedforward network to make \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 c_in, # the number of input channels\n",
    "                 n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
    "                 n_patches, # the number of stft or time patches\n",
    "                 d_model, # the dimension of the transformer model\n",
    "                 pred_len_seconds, # the sequence multiclass prediction length in seconds\n",
    "                 n_linear_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n",
    "                 conv_kernel_stride_size, # the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n",
    "                 dropout=0. # dropout in between linear layers\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.n_classes = n_classes\n",
    "        self.pred_len_seconds = pred_len_seconds\n",
    "        self.n_linear_layers = n_linear_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_patches = n_patches\n",
    "        self.pool_stride = (d_model*n_patches)//pred_len_seconds\n",
    "        self.pool_kernel = (d_model*n_patches) - (pred_len_seconds - 1) * self.pool_stride\n",
    "        #self.pred_len = int(self.pred_len_seconds / (self.hop_length / self.frequency))  # lets predict on a second basis, not on a timepoint basis\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.layers = []\n",
    "        n_nuerons = int((self.pred_len_seconds - d_model*n_patches)/self.n_linear_layers) if self.n_linear_layers >= 1 else 0\n",
    "        if self.n_linear_layers == 1:\n",
    "            self.layers.append(nn.Linear(d_model*n_patches, self.pred_len_seconds))\n",
    "        elif self.n_linear_layers == 0:\n",
    "            self.layers.append(nn.AvgPool1d(kernel_size=self.pool_kernel, stride=self.pool_stride, padding=0))\n",
    "        else:\n",
    "            for i in range(self.n_linear_layers-1):\n",
    "                self.layers.append(nn.Sequential(nn.Linear(n_nuerons*i+d_model*n_patches, d_model*n_patches+n_nuerons*(i+1)), nn.ReLU(), nn.Dropout(dropout)))\n",
    "            self.layers.append(nn.Linear(d_model*n_patches + n_nuerons*(i+1), self.pred_len_seconds))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.conv1d = nn.Conv1d(c_in, n_classes, kernel_size=conv_kernel_stride_size, stride=conv_kernel_stride_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, return_softmax=False):\n",
    "        \"\"\"\n",
    "        Make sure to freeze the encoder and remove its pretrain head! \n",
    "\n",
    "        in: [bs x n channels x d model x n patches]\n",
    "        out: [bs x n classes x pred len]\n",
    "        \"\"\"\n",
    "        x = x.transpose(2,3)\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.conv1d(x)\n",
    "        if return_softmax:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f65fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvGRU1DCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate a convolutional GRU cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, kernel_size):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reset_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.update_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.out_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "\n",
    "        init.orthogonal_(self.reset_gate.weight)\n",
    "        init.orthogonal_(self.update_gate.weight)\n",
    "        init.orthogonal_(self.out_gate.weight)\n",
    "        init.zeros_(self.reset_gate.bias)\n",
    "        init.zeros_(self.update_gate.bias)\n",
    "        init.zeros_(self.out_gate.bias)\n",
    "\n",
    "\n",
    "    def forward(self, input_, prev_state):\n",
    "\n",
    "        # get batch and spatial sizes\n",
    "        batch_size = input_.data.size()[0]\n",
    "        spatial_size = input_.data.size()[2:]\n",
    "\n",
    "        # generate empty prev_state, if None is provided\n",
    "        if prev_state is None:\n",
    "            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n",
    "            prev_state = torch.zeros(state_size, device=input_.device)\n",
    "            \n",
    "        # data size is [batch, channel, height, width]\n",
    "        stacked_inputs = torch.cat([input_, prev_state], dim=1)\n",
    "        update = F.sigmoid(self.update_gate(stacked_inputs))\n",
    "        reset = F.sigmoid(self.reset_gate(stacked_inputs))\n",
    "        out_inputs = F.tanh(self.out_gate(torch.cat([input_, prev_state * reset], dim=1)))\n",
    "        new_state = prev_state * (1 - update) + out_inputs * update\n",
    "\n",
    "        return new_state\n",
    "\n",
    "\n",
    "class ConvGRU1D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, kernel_sizes, n_layers):\n",
    "        '''\n",
    "        Generates a multi-layer convolutional GRU.\n",
    "        Preserves spatial dimensions across cells, only altering depth.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : integer. depth dimension of input tensors.\n",
    "        hidden_sizes : integer or list. depth dimensions of hidden state.\n",
    "            if integer, the same hidden size is used for all cells.\n",
    "        kernel_sizes : integer or list. sizes of Conv2d gate kernels.\n",
    "            if integer, the same kernel size is used for all cells.\n",
    "        n_layers : integer. number of chained `ConvGRUCell`.\n",
    "        '''\n",
    "\n",
    "        super(ConvGRU1D, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        if type(hidden_sizes) != list:\n",
    "            self.hidden_sizes = [hidden_sizes]*n_layers\n",
    "        else:\n",
    "            assert len(hidden_sizes) == n_layers, '`hidden_sizes` must have the same length as n_layers'\n",
    "            self.hidden_sizes = hidden_sizes\n",
    "        if type(kernel_sizes) != list:\n",
    "            self.kernel_sizes = [kernel_sizes]*n_layers\n",
    "        else:\n",
    "            assert len(kernel_sizes) == n_layers, '`kernel_sizes` must have the same length as n_layers'\n",
    "            self.kernel_sizes = kernel_sizes\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        cells = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                input_dim = self.input_size\n",
    "            else:\n",
    "                input_dim = self.hidden_sizes[i-1]\n",
    "\n",
    "            cell = ConvGRU1DCell(input_dim, self.hidden_sizes[i], self.kernel_sizes[i])\n",
    "            name = 'ConvGRU1DCell_' + str(i).zfill(2)\n",
    "\n",
    "            setattr(self, name, cell)\n",
    "            cells.append(getattr(self, name))\n",
    "        self.cells = cells\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        '''\n",
    "        x: [batch, n_channels, dim_model, n_patches]\n",
    "        hidden: [batch, n_channels, dim_model, n_patches]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        upd_cell_hidden: [batch, n_channels, dim_model, n_patches]\n",
    "        '''\n",
    "        if not hidden:\n",
    "            hidden = [None]*self.n_layers\n",
    "\n",
    "        input_ = x\n",
    "\n",
    "        upd_hidden = []\n",
    "\n",
    "        for layer_idx in range(self.n_layers):\n",
    "            cell = self.cells[layer_idx]\n",
    "            cell_hidden = hidden[layer_idx]\n",
    "\n",
    "            # pass through layer\n",
    "            upd_cell_hidden = cell(input_, cell_hidden)\n",
    "            #upd_hidden.append(upd_cell_hidden)\n",
    "            # update input_ to the last updated hidden layer for next pass\n",
    "            input_ = upd_cell_hidden\n",
    "\n",
    "        # retain tensors in list to allow different hidden sizes\n",
    "        return upd_cell_hidden\n",
    "\n",
    "class ConvBiGRU(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, input_size, hidden_sizes, kernel_sizes, n_layers, d_model, predict_every_n_patches, n_classes):\n",
    "        super(ConvBiGRU, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-2, end_dim=-1)\n",
    "\n",
    "        self.forward_net = ConvGRU1D(input_size, hidden_sizes, kernel_sizes, n_layers)\n",
    "        self.reverse_net = ConvGRU1D(input_size, hidden_sizes, kernel_sizes, n_layers)\n",
    "\n",
    "        final_kernel_stride = int(d_model*predict_every_n_patches) # this will make one predictions for every predict_every_seconds seq length\n",
    "        self.conv1d = nn.Conv1d(in_channels=hidden_sizes*2, out_channels=n_classes, kernel_size=final_kernel_stride, stride=final_kernel_stride)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, return_softmax=False):\n",
    "        \"\"\"\n",
    "        x: [batch, n_channels, dim_model, n_patches]\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        #x = self.flatten(x)\n",
    "        x_rev = torch.flip(x, dims=[-1])\n",
    "        y_out_fwd = self.forward_net(x)\n",
    "        y_out_rev = self.reverse_net(x_rev)\n",
    "        \n",
    "        y_out_rev = torch.flip(y_out_rev, dims=[-1])\n",
    "        #reversed_idx = list(reversed(range(y_out_rev.shape[1])))\n",
    "        #y_out_rev = y_out_rev[:, reversed_idx, ...] # reverse temporal outputs.\n",
    "        ycat = torch.cat((y_out_fwd, y_out_rev), dim=1)\n",
    "        ycat = self.flatten(ycat)\n",
    "        ycat = self.conv1d(ycat)\n",
    "        if return_softmax:\n",
    "            ycat = self.softmax(ycat)\n",
    "        return ycat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f289111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "x = torch.randn((4,7,512,3600))\n",
    "\n",
    "convgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n",
    "\n",
    "out = convgru(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
