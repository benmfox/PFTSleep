[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PFTSleep",
    "section": "",
    "text": "PFTSleep is a Python package for sleep stage classification using a pre-trained foundational transformer. The repository is built using nbdev, which means the package is developed in Jupyter notebooks.\nSee the publication in SLEEP and original preprint for more details.",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "PFTSleep",
    "section": "",
    "text": "PFTSleep is a Python package for sleep stage classification using a pre-trained foundational transformer. The repository is built using nbdev, which means the package is developed in Jupyter notebooks.\nSee the publication in SLEEP and original preprint for more details.",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "PFTSleep",
    "section": "Install",
    "text": "Install\npip install PFTSleep",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#repository-structure-and-usage",
    "href": "index.html#repository-structure-and-usage",
    "title": "PFTSleep",
    "section": "Repository Structure and Usage",
    "text": "Repository Structure and Usage\nThis is an nbdev repository, which means the package is developed in Jupyter notebooks located in the nbs/ directory. Any modifications or additions to the PFTSleep package should be made by editing these notebooks.\nTo build the package, run nbdev_prepare in the terminal. This will generate the PFTSleep package in the PFTSleep/ directory and all python modules, which can be imported and used in other Python projects.\nTo add new functionality, create a new notebook or add to exisitng in the nbs/ directory and follow the instructions in the nbdev documentation to add the new functionality. Then, run nbdev_prepare to generate the PFTSleep package with the new functionality.\n\nDirectory Structure:\n\nnbs/: Contains the source notebooks that generate the Python package\njobs/: Contains processing and training scripts\n\napples/: Processing scripts for the apples dataset\nmesa/: Processing scripts for the mesa dataset\nmros/: Processing scripts for the mros dataset\nshhs/: Processing scripts for the shhs dataset\nwsc/: Processing scripts for the wsc dataset\nmodel_training/:\n\ntrain_transformer.py: Trains the initial foundational transformer model\ntrain_classifier.py: Trains the probing head for sleep stage classification\n\n\n\nEach dataset directory contains scripts to: - Create hypnogram CSVs from annotations - Build zarr files from EDF files - Process and standardize the data for model training\n\n\nModel Training Pipeline\n\nFoundation Model Training (jobs/model_training/train_transformer.py)\n\nTrains the base transformer model on sleep data zarr files\nCreates general purpose representations of sleep signals\n\nProbe Training (jobs/model_training/train_classifier.py)\n\nTrains a classification head on top of the foundation model",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "PFTSleep",
    "section": "Technical Details",
    "text": "Technical Details\n\nWe trained the foundational model on 2x H100 80gb GPUs using PyTorch Lightning.\nWe monitored training using the Weights and Biases platform.\nWe performed hyperparameter optimization using Optuna.",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "PFTSleep",
    "section": "Citation",
    "text": "Citation\nIf you use PFTSleep in your research, please cite:\n@ARTICLE{Fox2025-zc,\n  title     = \"A foundational transformer leveraging full night, multichannel\n               sleep study data accurately classifies sleep stages\",\n  author    = \"Fox, Benjamin and Jiang, Joy and Wickramaratne, Sajila and\n               Kovatch, Patricia and Suarez-Farinas, Mayte and Shah, Neomi A\n               and Parekh, Ankit and Nadkarni, Girish N\",\n  journal   = \"Sleep\",\n  publisher = \"Oxford University Press (OUP)\",\n  month     =  mar,\n  year      =  2025,\n  language  = \"en\"\n}",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "source\n\n\n\n PatchEncoder (c_in, patch_len, d_model, shared_embedding)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\nDetails\n\n\n\n\nc_in\nthe number of input channels\n\n\npatch_len\nthe length of the patches (either stft or interval length)\n\n\nd_model\nthe dimension of the initial linear layers for inputting patches into transformer\n\n\nshared_embedding\nindicator of whether to project each channel individually or together",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#linear-layers-for-patches",
    "href": "layers.html#linear-layers-for-patches",
    "title": "Layers",
    "section": "",
    "text": "source\n\n\n\n PatchEncoder (c_in, patch_len, d_model, shared_embedding)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\nDetails\n\n\n\n\nc_in\nthe number of input channels\n\n\npatch_len\nthe length of the patches (either stft or interval length)\n\n\nd_model\nthe dimension of the initial linear layers for inputting patches into transformer\n\n\nshared_embedding\nindicator of whether to project each channel individually or together",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#positional-encoding-layers",
    "href": "layers.html#positional-encoding-layers",
    "title": "Layers",
    "section": "Positional Encoding Layers",
    "text": "Positional Encoding Layers\n\nsource\n\nPositionalEncoding\n\n PositionalEncoding (num_patch, d_model)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nDetails\n\n\n\n\nnum_patch\nnumber of patches of time series or stft in input\n\n\nd_model\ndimension of patch embeddings\n\n\n\n\nsource\n\n\ntAPE\n\n tAPE (d_model:int, seq_len:int, scale_factor=1.0)\n\ntime Absolute Position Encoding Adapted from tsai\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd_model\nint\n\nthe embedding dimension\n\n\nseq_len\nint\n\nthe max. length of the incoming sequence or num patches\n\n\nscale_factor\nfloat\n1.0\ndropout:float=0., # dropout value",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#mask-and-augmentation-layers",
    "href": "layers.html#mask-and-augmentation-layers",
    "title": "Layers",
    "section": "Mask and Augmentation Layers",
    "text": "Mask and Augmentation Layers\n\nsource\n\nMask\n\n Mask (mask_type, mask_ratio, return_mask=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\ntorch.manual_seed(125)\nm = Mask(mask_type='jitter_zero', mask_ratio=0.5)\nx = torch.randn((9))\n\nm(x), m(x)\n\n((tensor([ 0.0000,  0.0000,  0.0000,  1.0975,  0.0000,  1.4248,  0.0000, -0.1104,\n           1.1865]),\n  tensor(8)),\n (tensor([ 1.1462, -1.8379, -0.2368,  2.0488,  0.0000,  0.0000, -0.3927, -1.0523,\n           0.1294]),\n  tensor(4)))\n\n\n\nx = torch.randn((4))\nx_aug = x.clone()\nmask = torch.tensor([True,True,False,False])\n\nx_aug[mask] = 1\nx_aug, x\n\n(tensor([ 1.0000,  1.0000,  1.9390, -0.0338]),\n tensor([-1.7385, -0.5780,  1.9390, -0.0338]))\n\n\n\nsource\n\n\nPatchAugmentations\n\n PatchAugmentations (augmentations=['patch_mask', 'jitter_zero_mask',\n                     'reverse_sequence', 'shuffle_channels'],\n                     patch_mask_ratio=0.0, jitter_zero_mask_ratio=0.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nx = torch.randn(4,3600,7,750)\n\ns=PatchAugmentations(patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\ns(x).shape\n\ntorch.Size([4, 3600, 7, 750])\n\n\n\nsource\n\n\nEmbeddingAugmentations\n\n EmbeddingAugmentations (augmentations=['shuffle_dims',\n                         'jitter_zero_mask', 'patch_mask'],\n                         dims_to_shuffle=[1, 2, 3], patch_mask_ratio=0.0,\n                         jitter_zero_mask_ratio=0.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nx = torch.randn(4,7,512,3600)\n\ns = EmbeddingAugmentations(augmentations=['jitter_zero_mask'], dims_to_shuffle=[1], patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\ns(x).shape\n\ntorch.Size([4, 7, 512, 3600])",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#patch-and-fourier-layers",
    "href": "layers.html#patch-and-fourier-layers",
    "title": "Layers",
    "section": "Patch and Fourier Layers",
    "text": "Patch and Fourier Layers\n\nsource\n\nPatch\n\n Patch (patch_len, stride, max_seq_len=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nSTFT\n\n STFT (n_fft, win_length, hop_length, stft_norm, decibel_scale,\n       channel_stft_means=None, channel_stft_stds=None,\n       pad_win_length_to_nfft=True, pad_mode='reflect', center=False,\n       return_complex=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nFFT\n\n FFT (dim=-1, norm='backward')\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndim\nint\n-1\ndimension to calculate fft over\n\n\nnorm\nstr\nbackward\n“forward” - normalize by 1/n, “backward” - no normalization, “ortho” - normalize by 1/sqrt(n) (making the FFT orthonormal)",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#reversible-instance-normalization",
    "href": "layers.html#reversible-instance-normalization",
    "title": "Layers",
    "section": "Reversible Instance Normalization",
    "text": "Reversible Instance Normalization\n\nsource\n\nRevIN\n\n RevIN (num_features:int, eps=1e-05, dim_to_reduce=-1, affine=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_features\nint\n\nthe number of channels or features in the input\n\n\neps\nfloat\n1e-05\nadded to avoid division by zero errors\n\n\ndim_to_reduce\nint\n-1\nthe dimension to reduce,\n\n\naffine\nbool\nTrue\nlearning affine parameters bias and weight per channel\n\n\n\n\nx = torch.randn(4,7,1000)\n\nrevin = RevIN(x.shape[1], dim_to_reduce=-1, affine=True)\nx_norm = revin(x, mode=True)\nx_denorm = revin(x_norm, mode=False)\n\nx.shape, x_norm.shape, x_denorm.shape, revin.mean.shape, revin.stdev.shape\n\n(torch.Size([4, 7, 1000]),\n torch.Size([4, 7, 1000]),\n torch.Size([4, 7, 1000]),\n torch.Size([4, 7, 1]),\n torch.Size([4, 7, 1]))",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#attention",
    "href": "layers.html#attention",
    "title": "Layers",
    "section": "Attention",
    "text": "Attention\n\nsource\n\nMultiheadFlashAttention\n\n MultiheadFlashAttention (d_model:int, n_heads:int, qkv_bias:bool=True,\n                          is_causal:bool=False, attn_dropout:float=0.0,\n                          proj_dropout:float=0.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmha = MultiheadFlashAttention(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0.)\nx = torch.randn(2*7,100,512) # [bs * n_vars x n_patches x d_model]\nkey_padding_mask = torch.zeros(2*7, 100, dtype=torch.bool)\nkey_padding_mask[:, -2:] = True  # mask last 2 positions\noutput = mha(x, key_padding_mask=key_padding_mask)\noutput.shape\n\ntorch.Size([14, 100, 512])\n\n\n\nsource\n\n\nScaledDotProductAttention\n\n ScaledDotProductAttention (d_model, n_heads, attn_dropout=0.0,\n                            res_attention=False, lsa=False)\n\nScaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets by Lee et al, 2021)\n\nsource\n\n\nMultiheadAttentionCustom\n\n MultiheadAttentionCustom (d_model, n_heads, d_k=None, d_v=None,\n                           res_attention=False, attn_dropout=0.0,\n                           proj_dropout=0.0, qkv_bias=True, lsa=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmha_attn = MultiheadAttentionCustom(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0., res_attention=False)\nmha_attn\n\nMultiheadAttentionCustom(\n  (W_Q): Linear(in_features=512, out_features=512, bias=True)\n  (W_K): Linear(in_features=512, out_features=512, bias=True)\n  (W_V): Linear(in_features=512, out_features=512, bias=True)\n  (sdp_attn): ScaledDotProductAttention(\n    (attn_dropout): Dropout(p=0.0, inplace=False)\n  )\n  (to_out): Sequential(\n    (0): Linear(in_features=512, out_features=512, bias=True)\n    (1): Dropout(p=0.0, inplace=False)\n  )\n)\n\n\n\ndef test_attention_equivalence():\n    # Set random seed for reproducibility\n    torch.manual_seed(42)\n    \n    # Test parameters\n    batch_size = 2\n    seq_len = 10\n    d_model = 64\n    n_heads = 4\n    \n    # Create input tensor (only need one since we're using self-attention)\n    x = torch.randn(batch_size, seq_len, d_model)\n    \n    # Create key padding mask\n    key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)\n    key_padding_mask[:, -2:] = True  # mask last 2 positions\n\n    # Initialize both implementations\n    custom_mha = MultiheadAttentionCustom(d_model=d_model, n_heads=n_heads)\n    flash_mha = MultiheadFlashAttention(d_model=d_model, n_heads=n_heads)\n    \n    # Set both models to eval mode to disable dropout\n    custom_mha.eval()\n    flash_mha.eval()\n    \n    # Copy weights to ensure identical parameters\n    # Combine QKV weights from custom implementation into single matrix for flash attention\n    combined_weight = torch.cat([\n        custom_mha.W_Q.weight,\n        custom_mha.W_K.weight,\n        custom_mha.W_V.weight\n    ], dim=0)\n    combined_bias = torch.cat([\n        custom_mha.W_Q.bias,\n        custom_mha.W_K.bias,\n        custom_mha.W_V.bias\n    ], dim=0)\n    \n    # Copy combined weights to flash attention\n    flash_mha.c_attn.weight.data = combined_weight\n    flash_mha.c_attn.bias.data = combined_bias\n    \n    # Output projection weights\n    flash_mha.c_proj.weight.data = custom_mha.to_out[0].weight.data.clone()\n    flash_mha.c_proj.bias.data = custom_mha.to_out[0].bias.data.clone()\n    \n    # Forward pass\n    with torch.no_grad():\n        custom_output, custom_attn = custom_mha(x, key_padding_mask=key_padding_mask)\n        \n        flash_output = flash_mha(x, attn_mask=key_padding_mask)\n    \n    # Compare outputs\n    print(f\"Custom output shape: {custom_output.shape}\")\n    print(f\"Flash output shape: {flash_output.shape}\")\n    \n    output_close = torch.allclose(custom_output, flash_output, rtol=0, atol=0)\n    print(f\"Outputs match: {output_close}\")\n    \n    if not output_close:\n        print(\"\\nOutput differences:\")\n        print(f\"Max difference: {(custom_output - flash_output).abs().max().item()}\")\n        print(f\"Mean difference: {(custom_output - flash_output).abs().mean().item()}\")\n    \n    return custom_output, flash_output\n\ncustom_output, flash_output = test_attention_equivalence()\n#: 8.940696716308594e-08\n#Mean difference: 1.0550138540565968e-08\n\nCustom output shape: torch.Size([2, 10, 64])\nFlash output shape: torch.Size([2, 10, 64])\nOutputs match: True\n\n\n\nd_model=512\nn_heads=8\nd_k = d_v = d_model // n_heads\nattn = ScaledDotProductAttention(d_model=d_model, n_heads=n_heads)\nmha_attn = MultiheadAttentionCustom(d_model, n_heads)\n\nW_Q = nn.Linear(d_model, d_k * n_heads)\nW_K = nn.Linear(d_model, d_k * n_heads)\nW_V = nn.Linear(d_model, d_v * n_heads)\nX,_,_ = ds[0]\n\nX = create_patch(X, patch_len=(10*50), stride=(5*50), constant_pad=True)\n\npatch_len = X.shape[-1]\n\nX = X[None, ...].permute(0,2,1,3)  # simulate batch size of 1 [bs x n_vars x num_patch x patch_len]\n\nprint(f'X input shape: {X.shape}')\nW_P = nn.Linear(patch_len, d_model)\n\nX = W_P(X) # project to d_model\nprint(f\"Projected X shape to d_model: {X.shape}\")\n\nX = torch.reshape(X, (X.shape[0]*X.shape[1],X.shape[2],X.shape[3]))\nprint(f\"Reshape for attention: {X.shape}\")\n\n# test multihead attention\nprint(\"\\nTesting MHA and SDA attention, with just 50 elements.\")\nmha_output, mha_attn_weights = mha_attn(Q=X[:,:50,:])\nprint(f\"MHA attention output shape: {mha_output.shape}, mha attn weight shape: {mha_attn_weights.shape}\")\n\n# test scaled dot product attn\nK = Q = V = X\n\n# # Linear (+ split in multiple heads)\nbs = 1 # 1 * 16\nq_s = W_Q(Q).reshape(bs, -1, n_heads, d_k).transpose(1, 2)\nk_s = W_K(K).reshape(bs, -1, n_heads, d_k).permute(0, 2, 3, 1)\nv_s = W_V(V).reshape(bs, -1, n_heads, d_v).transpose(1, 2)\nprint(f\"Q shape: {q_s.shape}, K shape: {k_s.shape}, V shape: {v_s.shape}\")\n\nto_out = nn.Linear(n_heads * d_v, d_model)\noutput, attn_weights = attn(q_s[:,:,:50,:],k_s[:,:,:,:50], v_s[:,:,:50,:])\noutput = output.transpose(1, 2).contiguous().view(bs, -1, n_heads * d_v)\nprint(f\"Attn output shape {output.shape}, attn weight shape: {attn_weights.shape}\")\n\nX input shape: torch.Size([1, 7, 10799, 500])\nProjected X shape to d_model: torch.Size([1, 7, 10799, 512])\nReshape for attention: torch.Size([7, 10799, 512])\n\nTesting MHA and SDA attention, with just 50 elements.\nMHA attention output shape: torch.Size([7, 50, 512]), mha attn weight shape: torch.Size([7, 8, 50, 50])\nQ shape: torch.Size([1, 8, 75593, 64]), K shape: torch.Size([1, 8, 64, 75593]), V shape: torch.Size([1, 8, 75593, 64])\nAttn output shape torch.Size([1, 50, 512]), attn weight shape: torch.Size([1, 8, 50, 50])\n\n\n\nsource\n\n\nAttention_Rel_Scl\n\n Attention_Rel_Scl (d_model:int, n_heads:int, seq_len:int, d_k:int=None,\n                    d_v:int=None, res_attention:bool=False,\n                    attn_dropout:float=0.0, lsa:bool=False,\n                    proj_dropout:float=0.0, qkv_bias:bool=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd_model\nint\n\nEmbedding dimension\n\n\nn_heads\nint\n\nnumber of attention heads\n\n\nseq_len\nint\n\nsequence length or num patches\n\n\nd_k\nint\nNone\nkey dimension\n\n\nd_v\nint\nNone\nvalue dimension\n\n\nres_attention\nbool\nFalse\nwhether to use residual attention\n\n\nattn_dropout\nfloat\n0.0\ndropout for attention\n\n\nlsa\nbool\nFalse\nwhether to use LSA, trainable paramater for scaling\n\n\nproj_dropout\nfloat\n0.0\ndropout for projection\n\n\nqkv_bias\nbool\nTrue\nbias for q, k, v\n\n\n\n\nd_model=16\nc_in = 2\nseq_len = 1*360*10\nx = torch.randn(4,c_in,seq_len)\nembed_layer = nn.Sequential(nn.Conv2d(1, d_model*4, kernel_size=[1, 7], padding='same'), nn.BatchNorm2d(d_model*4), nn.GELU())\nembed_layer2 = nn.Sequential(nn.Conv2d(d_model*4, d_model, kernel_size=[c_in, 1], padding='valid'), nn.BatchNorm2d(d_model), nn.GELU())\nabs_position = tAPE(d_model, seq_len=seq_len)\nx_emb = embed_layer2(embed_layer(x.unsqueeze(1))).squeeze(2)\nx_emb = x_emb.permute(0,2,1)\nx_emb_pos = abs_position(x_emb)\n\nmodel = Attention_Rel_Scl(d_model=d_model,\n        n_heads=2, # number of attention heads\n        seq_len=seq_len, # sequence length or num patches\n        )\n\nout, attn_weights = model(x_emb)\n\n\n## test w patches [bs *c_in x num_patches x d_model]\nd_model=512\nc_in = 2\nnum_patches = 10\nx_emb = torch.randn(4*c_in,num_patches, d_model)\nabs_position = tAPE(d_model, seq_len=num_patches)\nx_emb_pos = abs_position(x_emb)\n\nmodel = Attention_Rel_Scl(d_model=d_model,\n        n_heads=2, # number of attention heads\n        seq_len=num_patches, # sequence length or num patches\n        )\n\nout, attn_weights = model(x_emb)",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#pretraining-heads",
    "href": "layers.html#pretraining-heads",
    "title": "Layers",
    "section": "Pretraining Heads",
    "text": "Pretraining Heads\n\nsource\n\nMaskedAutogressionFeedForward2\n\n MaskedAutogressionFeedForward2 (d_model, patch_len, n_layers, dropout)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nMaskedAutogressionFeedForward\n\n MaskedAutogressionFeedForward (c_in, patch_len, d_model,\n                                shared_recreation=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\npatch_len\n\n\nthe length of the patches (either stft or interval length)\n\n\nd_model\n\n\nthe dimension of the initial linear layers for inputting patches into transformer\n\n\nshared_recreation\nbool\nTrue\nindicator of whether to project each channel individually or together",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#miscellaneous",
    "href": "layers.html#miscellaneous",
    "title": "Layers",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nsource\n\nIdentity\n\n Identity ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nget_activation_fn\n\n get_activation_fn (activation)\n\n\nsource\n\n\nTranspose\n\n Transpose (*dims, contiguous=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "loss.html",
    "href": "loss.html",
    "title": "Loss functions",
    "section": "",
    "text": "source\n\nhuber_loss\n\n huber_loss (preds, target, mask, use_mask=False, padding_mask=None,\n             delta=1)\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars] padding_mask: [bs x num_patch]\n\nsource\n\n\ncosine_similarity_loss\n\n cosine_similarity_loss (preds, target, mask, use_mask=False,\n                         padding_mask=None)\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars]\n\nsource\n\n\ncosine_similarity\n\n cosine_similarity (preds, target, mask, use_mask=False,\n                    padding_mask=None)\n\n\nsource\n\n\nmape\n\n mape (preds, target, mask, use_mask=False)\n\n\nsource\n\n\nmae\n\n mae (preds, target, mask, use_mask=False, padding_mask=None)\n\n\nsource\n\n\nrmse\n\n rmse (preds, target, mask, use_mask=False, padding_mask=None)\n\n\nsource\n\n\nmse\n\n mse (preds, target, mask, use_mask=False, padding_mask=None)\n\n\nsource\n\n\nr2_score\n\n r2_score (preds, target, mask, use_mask=False)\n\n\nsource\n\n\nmasked_mae_loss\n\n masked_mae_loss (preds, target, mask, use_mask=False, padding_mask=None)\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars] padding_mask: [bs x num_patch]\n\nsource\n\n\nmasked_mse_loss\n\n masked_mse_loss (preds, target, mask, use_mask=False, padding_mask=None)\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars] padding_mask: [bs x num_patch]\n\nsource\n\n\npatch_continuity_loss\n\n patch_continuity_loss (preds)\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len]\n\nx = torch.randn(2,2, 2, 10)\n\npatch_continuity_loss(x)\n\ntensor(0.2936)\n\n\n\nsource\n\n\nFocalLoss\n\n FocalLoss (weight=None, gamma=2.0, reduction='mean', ignore_index=-100)\n\nadapted from tsai, weighted multiclass focal loss https://github.com/timeseriesAI/tsai/blob/bdff96cc8c4c8ea55bc20d7cffd6a72e402f4cb2/tsai/losses.py#L116C1-L140C20\n\ncriterion = FocalLoss(gamma=0.7, weight=None, ignore_index=0)\nbatch_size = 10\n\nn_patch = 721\nn_class = 5\n#m = torch.nn.Softmax(dim=-1)\nlogits = torch.randn(batch_size, n_class, n_patch)\ntarget = torch.randint(0, n_class, size=(batch_size, n_patch))\ncriterion(logits, target)\n\ntensor(8.4217)\n\n\n\nsource\n\n\nKLDivLoss\n\n KLDivLoss (weight=None, ignore_index=-100)\n\n*Kullback-Leibler Divergence Loss with masking for ignore_index. Handles soft labels with ignore_index marked as -100.\nArgs: logits: [bs x n_classes x pred_labels] - model predictions targets: [bs x n_classes x soft_labels] - soft labels, with ignore_index positions marked as -100 or [bs x n_labels] - hard labels*\n\nx = torch.randn(4,5,10)\ny = torch.randint(0,5, size=(4,10))\ny_og = y.clone()\ny[0,0] = -100\n\nKLDivLoss(ignore_index=-100)(x,y)\n\ntensor(0.4065)",
    "crumbs": [
      "Loss functions"
    ]
  },
  {
    "objectID": "augmentations.html",
    "href": "augmentations.html",
    "title": "Augmentations",
    "section": "",
    "text": "source\n\n\n\n create_patch (xb, patch_len, stride, return_patch_num=False,\n               constant_pad=False, constant_pad_value=0, max_seq_len=None)\n\nxb: [bs x n_vars x seq_len]\n\nx = torch.randn(4, 1000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=505, stride=500, constant_pad=False)\nxb_rep = create_patch(x, patch_len=500, stride=500, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)b\n\n(torch.Size([4, 1000]), torch.Size([4, 1, 505]), torch.Size([4, 2, 500]))\n\n\n\nx = torch.randn(1,7,1350000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=1024, stride=1024, constant_pad=False)\nxb_rep = create_patch(x, patch_len=1024, stride=1024, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)\n\n(torch.Size([1, 7, 1350000]),\n torch.Size([1, 1318, 7, 1024]),\n torch.Size([1, 1319, 7, 1024]))\n\n\n\nsource\n\n\n\n\n unpatch (x, seq_len, remove_padding=True)\n\nx: [bs/None x patch_num x n_vars x patch_len] returns x: [bs x n_vars x seq_len]\n\nx = torch.randn(1,1,50)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=6, stride=6, constant_pad=True)\nxb = unpatch(xb, seq_len=50, remove_padding=False)\nxb.shape\n\n(torch.Size([1, 1, 50]), torch.Size([1, 9, 1, 6]))",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#patching",
    "href": "augmentations.html#patching",
    "title": "Augmentations",
    "section": "",
    "text": "source\n\n\n\n create_patch (xb, patch_len, stride, return_patch_num=False,\n               constant_pad=False, constant_pad_value=0, max_seq_len=None)\n\nxb: [bs x n_vars x seq_len]\n\nx = torch.randn(4, 1000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=505, stride=500, constant_pad=False)\nxb_rep = create_patch(x, patch_len=500, stride=500, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)b\n\n(torch.Size([4, 1000]), torch.Size([4, 1, 505]), torch.Size([4, 2, 500]))\n\n\n\nx = torch.randn(1,7,1350000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=1024, stride=1024, constant_pad=False)\nxb_rep = create_patch(x, patch_len=1024, stride=1024, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)\n\n(torch.Size([1, 7, 1350000]),\n torch.Size([1, 1318, 7, 1024]),\n torch.Size([1, 1319, 7, 1024]))\n\n\n\nsource\n\n\n\n\n unpatch (x, seq_len, remove_padding=True)\n\nx: [bs/None x patch_num x n_vars x patch_len] returns x: [bs x n_vars x seq_len]\n\nx = torch.randn(1,1,50)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=6, stride=6, constant_pad=True)\nxb = unpatch(xb, seq_len=50, remove_padding=False)\nxb.shape\n\n(torch.Size([1, 1, 50]), torch.Size([1, 9, 1, 6]))",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#patch-masking",
    "href": "augmentations.html#patch-masking",
    "title": "Augmentations",
    "section": "Patch Masking",
    "text": "Patch Masking\n\nsource\n\nrandom_masking\n\n random_masking (xb, mask_ratio)\n\n\nsource\n\n\nmask_patches_simple\n\n mask_patches_simple (xb, mask_ratio)\n\n*Function that masks patches in a simple way\nxb: [bs x patch_num x n_vars x patch_len] padding_mask [bs x patch_num x 1|num_vars x patch_len]*\n\n\n\n\nDetails\n\n\n\n\nxb\ninput tensor of size 3 or 4 to be masked\n\n\nmask_ratio\nratio of masking of patches\n\n\n\n\nx = torch.randn(50,16,7,50)\nmask_ratio = 0.4\n\nx_new, mask = mask_patches_simple(x,mask_ratio=mask_ratio)",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#value-augmentations",
    "href": "augmentations.html#value-augmentations",
    "title": "Augmentations",
    "section": "Value Augmentations",
    "text": "Value Augmentations\n\nsource\n\njitter_augmentation\n\n jitter_augmentation (x, mask_ratio=0.05, jitter_ratio=0.05)\n\n\nsource\n\n\nremove_values\n\n remove_values (x, mask_ratio)\n\n\n## note that the random number generator advances state...\ntorch.manual_seed(42)\nx = torch.randn(4,7,1000)\n\ntorch.manual_seed(42)\nx_new, n_masks = jitter_augmentation(x)\nn_masks /(4* 7*1000)\n\ntorch.manual_seed(42)\nx_new2, n_masks2 = jitter_augmentation(x)\ntorch.equal(x_new, x_new2)\n\nTrue",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#shuffle-augmentations",
    "href": "augmentations.html#shuffle-augmentations",
    "title": "Augmentations",
    "section": "Shuffle Augmentations",
    "text": "Shuffle Augmentations\n\nsource\n\nshuffle_dim\n\n shuffle_dim (x, dim=1, p=0.5)\n\nshuffles a dimension randomly along dim x: [bs x n channels x n patches x patch len]\n\nsource\n\n\nreverse_sequence\n\n reverse_sequence (x, seq_dim=(-1,), p=0.5)\n\n\nx = torch.randn(4,1,5,5).to('cuda')\n\ntorch.equal(shuffle_dim(x), x)\n\nFalse",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#callbacks",
    "href": "augmentations.html#callbacks",
    "title": "Augmentations",
    "section": "Callbacks",
    "text": "Callbacks\n\nsource\n\nIntraClassCutMix1d\n\n IntraClassCutMix1d (mix_prob=0.5, return_y_every_sec=30, frequency=125,\n                     return_sequence_padding_mask=True)\n\n*Intra-class CutMix for 1D data (e.g., time-series).\nThis is a callback that can be used to apply CutMix to the training data. It is used to mix segments within the same class.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmix_prob\nfloat\n0.5\nprobability of applying cutmix\n\n\nreturn_y_every_sec\nint\n30\nlength of segment to mix, if one value of y corresponds to 30 seconds of signal data, this should be set to 30.\n\n\nfrequency\nint\n125\nfrequency of the data\n\n\nreturn_sequence_padding_mask\nbool\nTrue\nwhether to return the sequence padding mask\n\n\n\n\nx = torch.randn(4,7,90)\nx_c = x.clone()\ny = torch.randint(0, 5, size=(4,90//30))\nxxt = IntraClassCutMix1d(mix_prob=1, frequency=1, return_y_every_sec=30, return_sequence_padding_mask=False)\nbatch = (x,y)\nxxt.on_train_batch_start(None, None, batch, 0)\ntorch.equal(x_c, batch[0]) == False\n\nTrue\n\n\n\nsource\n\n\nIntraClassCutMixBatch\n\n IntraClassCutMixBatch (mix_prob=0.5, return_y_every_sec=30,\n                        frequency=125, return_sequence_padding_mask=True,\n                        intra_class_only=True)\n\n*Intra-class CutMix for 1D data (e.g., time-series).\nThis is a callback that can be used to apply CutMix to the training data. It is used to mix segments within the same class.\nThis is different to IntraClassCutMix1d in that it mixes segments of the same class across batches of data, rather than just at the same segment*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmix_prob\nfloat\n0.5\nprobability of applying cutmix\n\n\nreturn_y_every_sec\nint\n30\nlength of segment to mix, if one value of y corresponds to 30 seconds of signal data, this should be set to 30.\n\n\nfrequency\nint\n125\nfrequency of the data\n\n\nreturn_sequence_padding_mask\nbool\nTrue\nwhether to return the sequence padding mask\n\n\nintra_class_only\nbool\nTrue\nwhether to mix only within same class (True) or across all classes (False)\n\n\n\n\nx = torch.randn(4,7,90)\nx_c = x.clone()\ny = torch.randint(0, 5, size=(4,90//30))\nxxt = IntraClassCutMixBatch(mix_prob=1, frequency=1, return_y_every_sec=30, return_sequence_padding_mask=False)\nbatch = (x,y)\nbatch = xxt.on_train_batch_start(None, None, batch, 0)\ntorch.equal(x_c, batch[0]) == False\n\nintra-class CutMixBatch is being applied!\n\n\nTrue\n\n\n\n# Create a tuple\nbatch = ([1,2,3], [4,5,6])\n\n# Unpack into new variables\nx, y = batch\n\n# Modify x and y\nx[0] = 99  # This modifies the list because lists are mutable\ny[0] = 88  # This modifies the list because lists are mutable\n\nprint(batch)  # Will show ([99,2,3], [88,5,6]) because lists are mutable\n\n([99, 2, 3], [88, 5, 6])\n\n\n\nsource\n\n\nMixupCallback\n\n MixupCallback (num_classes, mixup_alpha=0.4,\n                return_sequence_padding_mask=True, ignore_index=-100)\n\n*Mixup for 1D data (e.g., time-series).\nThis callback applies Mixup to the training data, blending both the input data and the labels.\nSee tsai implementation here: https://github.com/timeseriesAI/tsai/blob/bdff96cc8c4c8ea55bc20d7cffd6a72e402f4cb2/tsai/data/mixed_augmentation.py#L43\nNote that this creates non-integer labels/soft labels. Loss functions should be able to handle this.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_classes\n\n\n\n\n\nmixup_alpha\nfloat\n0.4\nalpha parameter for the beta distribution\n\n\nreturn_sequence_padding_mask\nbool\nTrue\nwhether to return the sequence padding mask\n\n\nignore_index\nint\n-100\nignore index\n\n\n\n\nx = torch.randn(4,7,90)\nx_c = x.clone()\ny_og = torch.randint(0, 5, size=(4,90//30))\ny_og[1,2] = -100\ny_og[2,1] = -100\ny_c = y_og.clone()\nxxt = MixupCallback(num_classes=5, mixup_alpha=0.4, return_sequence_padding_mask=False)\nbatch = (x,y_og)\nbatch = xxt.on_train_batch_start(None, None, batch, 0)\ntorch.equal(x_c, batch[0]) == False, torch.equal(y_c, batch[1]) == False\n\nMixup is being applied!\n\n\n(True, True)",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "heads.html",
    "href": "heads.html",
    "title": "SSL, Fine Tuning, and Linear Probing Heads",
    "section": "",
    "text": "source\n\n\n\n RNNProbingHead (c_in, input_size, hidden_size, n_classes,\n                 contrastive=False, module='GRU', linear_dropout=0.0,\n                 rnn_dropout=0.0, num_rnn_layers=1, act='gelu',\n                 pool='average', temperature=2.0, n_linear_layers=1,\n                 predict_every_n_patches=1, bidirectional=True,\n                 affine=False, shared_embedding=True, augmentations=None,\n                 augmentation_mask_ratio=0.0,\n                 augmentation_dims_to_shuffle=[1, 2, 3], norm=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\ninput_size\n\n\n\n\n\nhidden_size\n\n\n\n\n\nn_classes\n\n\n\n\n\ncontrastive\nbool\nFalse\n\n\n\nmodule\nstr\nGRU\n\n\n\nlinear_dropout\nfloat\n0.0\n\n\n\nrnn_dropout\nfloat\n0.0\n\n\n\nnum_rnn_layers\nint\n1\n\n\n\nact\nstr\ngelu\n\n\n\npool\nstr\naverage\n‘average’ or ‘max’ or ‘majority’\n\n\ntemperature\nfloat\n2.0\nonly used if pool=‘majority’\n\n\nn_linear_layers\nint\n1\n\n\n\npredict_every_n_patches\nint\n1\n\n\n\nbidirectional\nbool\nTrue\n\n\n\naffine\nbool\nFalse\n\n\n\nshared_embedding\nbool\nTrue\n\n\n\naugmentations\nNoneType\nNone\n\n\n\naugmentation_mask_ratio\nfloat\n0.0\n\n\n\naugmentation_dims_to_shuffle\nlist\n[1, 2, 3]\n\n\n\nnorm\nNoneType\nNone\none of [None, ‘pre’, ‘post’]\n\n\n\n\nsource\n\n\n\n\n RNNProbingHeadExperimental (c_in, input_size, hidden_size, n_classes,\n                             contrastive=False, module='GRU',\n                             linear_dropout=0.0, rnn_dropout=0.0,\n                             num_rnn_layers=1, act='gelu', pool='average',\n                             temperature=2.0, predict_every_n_patches=1,\n                             bidirectional=True, affine=False,\n                             augmentations=None,\n                             augmentation_mask_ratio=0.0,\n                             augmentation_dims_to_shuffle=[1, 2, 3],\n                             pre_norm=True, mlp_final_head=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\ninput_size\n\n\n\n\n\nhidden_size\n\n\n\n\n\nn_classes\n\n\n\n\n\ncontrastive\nbool\nFalse\ndeprecated\n\n\nmodule\nstr\nGRU\n\n\n\nlinear_dropout\nfloat\n0.0\n\n\n\nrnn_dropout\nfloat\n0.0\n\n\n\nnum_rnn_layers\nint\n1\n\n\n\nact\nstr\ngelu\n\n\n\npool\nstr\naverage\n‘average’ or ‘max’ or ‘majority’\n\n\ntemperature\nfloat\n2.0\nonly used if pool=‘majority’\n\n\npredict_every_n_patches\nint\n1\n\n\n\nbidirectional\nbool\nTrue\n\n\n\naffine\nbool\nFalse\n\n\n\naugmentations\nNoneType\nNone\n\n\n\naugmentation_mask_ratio\nfloat\n0.0\n\n\n\naugmentation_dims_to_shuffle\nlist\n[1, 2, 3]\n\n\n\npre_norm\nbool\nTrue\none of [None, ‘pre’, ‘post’]\n\n\nmlp_final_head\nbool\nFalse\n\n\n\n\n\nm = RNNProbingHeadExperimental(c_in=7, \n                                pool='average', \n                                input_size = 384, \n                                bidirectional=True,\n                                affine=False, \n                                hidden_size=1200,\n                                module='GRU',\n                                n_classes=4,\n                                predict_every_n_patches=32,\n                                rnn_dropout=0.,\n                                num_rnn_layers=1,\n                                linear_dropout=0.,\n                                mlp_final_head=False,\n                                pre_norm=True)\nx = torch.randn((4,7,384,960))\nsequence_padding_mask = torch.zeros(4,960)\nsequence_padding_mask[:,-32:] = 1\nm(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\nx = torch.randn((4,7,384,960))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\nx = torch.randn((4,7,512*2,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\n TransformerDecoderProbingHead (c_in, d_model, n_classes,\n                                norm='BatchNorm', dropout=0.0, act='gelu',\n                                d_ff=2048, num_layers=1, n_heads=2,\n                                predict_every_n_patches=1, affine=False,\n                                shared_embedding=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nlayer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\nx = torch.randn((4, 7, 512, 3600))\n\nlayer(x).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\n DecoderFeedForward (c_in, predict_every_n_patches, num_layers, d_ff,\n                     attn_dropout, res_attention, pre_norm, store_attn,\n                     n_heads, shared_embedding, affine, n_classes,\n                     d_model, norm='BatchNorm', act='gelu', dropout=0.0)\n\ntransformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\npredict_every_n_patches\n\n\nfor a given sequence of length m with frequency f, number of predictions\n\n\nnum_layers\n\n\n\n\n\nd_ff\n\n\n\n\n\nattn_dropout\n\n\n\n\n\nres_attention\n\n\n\n\n\npre_norm\n\n\n\n\n\nstore_attn\n\n\n\n\n\nn_heads\n\n\n\n\n\nshared_embedding\n\n\n\n\n\naffine\n\n\n\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\nd_model\n\n\nthe dimension of the transformer model\n\n\nnorm\nstr\nBatchNorm\nbatchnorm or layernorm between linear and convolutional layers\n\n\nact\nstr\ngelu\nactivation function to use between layers, ‘gelu’ or ‘relu’\n\n\ndropout\nfloat\n0.0\ndropout in between linear layers\n\n\n\n\nc_in = 7\nfrequency = 125\nwin_length=750 \noverlap = 0.\nhop_length=win_length - int(overlap*win_length)\nmax_seq_len_sec = (6*3600) # for dataloader\n#seq_len_sec = sample_stride = 3*3600 # for dataloader\nmax_seq_len = max_seq_len_sec*frequency # for model\n#n_patches = n_fft // 2 + 1\nn_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n\n#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\nx = torch.randn(2,c_in,512,n_patches)\n\nmodel = DecoderFeedForward(c_in=c_in,\n                           predict_every_n_patches=5,\n                           num_layers=1,\n                           d_ff = 2048,\n                           attn_dropout=0.,\n                           res_attention = False,\n                           pre_norm = False,\n                           store_attn = False,\n                           n_heads=2,\n                           affine=False,\n                           shared_embedding=False,\n                           n_classes=5,\n                           d_model=512,\n                           norm='BatchNorm',\n                           act='gelu',\n                           dropout=0.\n                           )\n\nmodel(x).shape\n\ntorch.Size([2, 5, 720])\n\n\n\nsource\n\n\n\n\n TimeDistributedConvolutionalFeedForward (c_in, frequency,\n                                          predict_every_seconds,\n                                          n_classes, win_length, d_model,\n                                          affine=False,\n                                          shared_embedding=True)\n\nConvolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension. Then, a convolutional transpose is used to extrapolate the data to its original form. Finally, a final convolution is used to predict the classes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\nfrequency\n\n\nthe frequency of the original channels\n\n\npredict_every_seconds\n\n\nfor a given sequence of length m with frequency f, number of predictions\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\nwin_length\n\n\nthe convolved patch length, the first step in this is to do a linear layer to this dimension\n\n\nd_model\n\n\nthe dimension of the transformer model\n\n\naffine\nbool\nFalse\n\n\n\nshared_embedding\nbool\nTrue\n\n\n\n\n\nsource\n\n\n\n\n LinearProbingHead (c_in, predict_every_n_patches, n_classes, input_size,\n                    n_layers, num_patch, shared_embedding=True,\n                    affine=True, norm='BatchNorm', act='gelu',\n                    dropout=0.0)\n\nA linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels in the original input\n\n\npredict_every_n_patches\n\n\nfor a given sequence of length m with frequency f, number of predictions\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\ninput_size\n\n\nthe dimension of the transformer model\n\n\nn_layers\n\n\nthe number of linear layers to use in the prediciton head, with RELU activation and dropout\n\n\nnum_patch\n\n\n\n\n\nshared_embedding\nbool\nTrue\nwhether or not to have a dense layer per channel or one layer per channel\n\n\naffine\nbool\nTrue\ninclude learnable parameters to weight predictions\n\n\nnorm\nstr\nBatchNorm\nbatchnorm or layernorm between linear and convolutional layers\n\n\nact\nstr\ngelu\nactivation function to use between layers, ‘gelu’ or ‘relu’\n\n\ndropout\nfloat\n0.0\ndropout in between linear layers\n\n\n\n\nm = LinearProbingHead(c_in=7, \n                      input_size = 512, \n                      predict_every_n_patches=5,\n                      n_classes=5,\n                      n_layers=3,\n                      shared_embedding=True,\n                      affine=True,\n                      num_patch=3600,\n                      dropout=0.1)\n\nx = torch.randn((4,7,512,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\n TimeDistributedFeedForward (c_in, n_classes, n_patches, d_model,\n                             pred_len_seconds, n_linear_layers,\n                             conv_kernel_stride_size, dropout=0.0)\n\nFeed forward head that uses a convolutional layer to reduce channel dimensionality Followed by a feedforward network to make\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\nn_patches\n\n\nthe number of stft or time patches\n\n\nd_model\n\n\nthe dimension of the transformer model\n\n\npred_len_seconds\n\n\nthe sequence multiclass prediction length in seconds\n\n\nn_linear_layers\n\n\nthe number of linear layers to use in the prediciton head, with RELU activation and dropout\n\n\nconv_kernel_stride_size\n\n\nthe 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n\n\ndropout\nfloat\n0.0\ndropout in between linear layers\n\n\n\n\nsource\n\n\n\n\n ConvBiGRU (input_size, hidden_sizes, kernel_sizes, n_layers, d_model,\n            predict_every_n_patches, n_classes)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n ConvGRU1D (input_size, hidden_sizes, kernel_sizes, n_layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n ConvGRU1DCell (input_size, hidden_size, kernel_size)\n\nGenerate a convolutional GRU cell\n\nx = torch.randn((4,7,512,3600))\n\nconvgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n\nout = convgru(x)\nout.shape\n\ntorch.Size([4, 5, 720])",
    "crumbs": [
      "SSL, Fine Tuning, and Linear Probing Heads"
    ]
  },
  {
    "objectID": "heads.html#linear-probing-and-fine-tuning-heads",
    "href": "heads.html#linear-probing-and-fine-tuning-heads",
    "title": "SSL, Fine Tuning, and Linear Probing Heads",
    "section": "",
    "text": "source\n\n\n\n RNNProbingHead (c_in, input_size, hidden_size, n_classes,\n                 contrastive=False, module='GRU', linear_dropout=0.0,\n                 rnn_dropout=0.0, num_rnn_layers=1, act='gelu',\n                 pool='average', temperature=2.0, n_linear_layers=1,\n                 predict_every_n_patches=1, bidirectional=True,\n                 affine=False, shared_embedding=True, augmentations=None,\n                 augmentation_mask_ratio=0.0,\n                 augmentation_dims_to_shuffle=[1, 2, 3], norm=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\ninput_size\n\n\n\n\n\nhidden_size\n\n\n\n\n\nn_classes\n\n\n\n\n\ncontrastive\nbool\nFalse\n\n\n\nmodule\nstr\nGRU\n\n\n\nlinear_dropout\nfloat\n0.0\n\n\n\nrnn_dropout\nfloat\n0.0\n\n\n\nnum_rnn_layers\nint\n1\n\n\n\nact\nstr\ngelu\n\n\n\npool\nstr\naverage\n‘average’ or ‘max’ or ‘majority’\n\n\ntemperature\nfloat\n2.0\nonly used if pool=‘majority’\n\n\nn_linear_layers\nint\n1\n\n\n\npredict_every_n_patches\nint\n1\n\n\n\nbidirectional\nbool\nTrue\n\n\n\naffine\nbool\nFalse\n\n\n\nshared_embedding\nbool\nTrue\n\n\n\naugmentations\nNoneType\nNone\n\n\n\naugmentation_mask_ratio\nfloat\n0.0\n\n\n\naugmentation_dims_to_shuffle\nlist\n[1, 2, 3]\n\n\n\nnorm\nNoneType\nNone\none of [None, ‘pre’, ‘post’]\n\n\n\n\nsource\n\n\n\n\n RNNProbingHeadExperimental (c_in, input_size, hidden_size, n_classes,\n                             contrastive=False, module='GRU',\n                             linear_dropout=0.0, rnn_dropout=0.0,\n                             num_rnn_layers=1, act='gelu', pool='average',\n                             temperature=2.0, predict_every_n_patches=1,\n                             bidirectional=True, affine=False,\n                             augmentations=None,\n                             augmentation_mask_ratio=0.0,\n                             augmentation_dims_to_shuffle=[1, 2, 3],\n                             pre_norm=True, mlp_final_head=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\ninput_size\n\n\n\n\n\nhidden_size\n\n\n\n\n\nn_classes\n\n\n\n\n\ncontrastive\nbool\nFalse\ndeprecated\n\n\nmodule\nstr\nGRU\n\n\n\nlinear_dropout\nfloat\n0.0\n\n\n\nrnn_dropout\nfloat\n0.0\n\n\n\nnum_rnn_layers\nint\n1\n\n\n\nact\nstr\ngelu\n\n\n\npool\nstr\naverage\n‘average’ or ‘max’ or ‘majority’\n\n\ntemperature\nfloat\n2.0\nonly used if pool=‘majority’\n\n\npredict_every_n_patches\nint\n1\n\n\n\nbidirectional\nbool\nTrue\n\n\n\naffine\nbool\nFalse\n\n\n\naugmentations\nNoneType\nNone\n\n\n\naugmentation_mask_ratio\nfloat\n0.0\n\n\n\naugmentation_dims_to_shuffle\nlist\n[1, 2, 3]\n\n\n\npre_norm\nbool\nTrue\none of [None, ‘pre’, ‘post’]\n\n\nmlp_final_head\nbool\nFalse\n\n\n\n\n\nm = RNNProbingHeadExperimental(c_in=7, \n                                pool='average', \n                                input_size = 384, \n                                bidirectional=True,\n                                affine=False, \n                                hidden_size=1200,\n                                module='GRU',\n                                n_classes=4,\n                                predict_every_n_patches=32,\n                                rnn_dropout=0.,\n                                num_rnn_layers=1,\n                                linear_dropout=0.,\n                                mlp_final_head=False,\n                                pre_norm=True)\nx = torch.randn((4,7,384,960))\nsequence_padding_mask = torch.zeros(4,960)\nsequence_padding_mask[:,-32:] = 1\nm(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\nx = torch.randn((4,7,384,960))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\nx = torch.randn((4,7,512*2,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\n TransformerDecoderProbingHead (c_in, d_model, n_classes,\n                                norm='BatchNorm', dropout=0.0, act='gelu',\n                                d_ff=2048, num_layers=1, n_heads=2,\n                                predict_every_n_patches=1, affine=False,\n                                shared_embedding=True)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nlayer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\nx = torch.randn((4, 7, 512, 3600))\n\nlayer(x).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\n DecoderFeedForward (c_in, predict_every_n_patches, num_layers, d_ff,\n                     attn_dropout, res_attention, pre_norm, store_attn,\n                     n_heads, shared_embedding, affine, n_classes,\n                     d_model, norm='BatchNorm', act='gelu', dropout=0.0)\n\ntransformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\npredict_every_n_patches\n\n\nfor a given sequence of length m with frequency f, number of predictions\n\n\nnum_layers\n\n\n\n\n\nd_ff\n\n\n\n\n\nattn_dropout\n\n\n\n\n\nres_attention\n\n\n\n\n\npre_norm\n\n\n\n\n\nstore_attn\n\n\n\n\n\nn_heads\n\n\n\n\n\nshared_embedding\n\n\n\n\n\naffine\n\n\n\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\nd_model\n\n\nthe dimension of the transformer model\n\n\nnorm\nstr\nBatchNorm\nbatchnorm or layernorm between linear and convolutional layers\n\n\nact\nstr\ngelu\nactivation function to use between layers, ‘gelu’ or ‘relu’\n\n\ndropout\nfloat\n0.0\ndropout in between linear layers\n\n\n\n\nc_in = 7\nfrequency = 125\nwin_length=750 \noverlap = 0.\nhop_length=win_length - int(overlap*win_length)\nmax_seq_len_sec = (6*3600) # for dataloader\n#seq_len_sec = sample_stride = 3*3600 # for dataloader\nmax_seq_len = max_seq_len_sec*frequency # for model\n#n_patches = n_fft // 2 + 1\nn_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n\n#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\nx = torch.randn(2,c_in,512,n_patches)\n\nmodel = DecoderFeedForward(c_in=c_in,\n                           predict_every_n_patches=5,\n                           num_layers=1,\n                           d_ff = 2048,\n                           attn_dropout=0.,\n                           res_attention = False,\n                           pre_norm = False,\n                           store_attn = False,\n                           n_heads=2,\n                           affine=False,\n                           shared_embedding=False,\n                           n_classes=5,\n                           d_model=512,\n                           norm='BatchNorm',\n                           act='gelu',\n                           dropout=0.\n                           )\n\nmodel(x).shape\n\ntorch.Size([2, 5, 720])\n\n\n\nsource\n\n\n\n\n TimeDistributedConvolutionalFeedForward (c_in, frequency,\n                                          predict_every_seconds,\n                                          n_classes, win_length, d_model,\n                                          affine=False,\n                                          shared_embedding=True)\n\nConvolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension. Then, a convolutional transpose is used to extrapolate the data to its original form. Finally, a final convolution is used to predict the classes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\nfrequency\n\n\nthe frequency of the original channels\n\n\npredict_every_seconds\n\n\nfor a given sequence of length m with frequency f, number of predictions\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\nwin_length\n\n\nthe convolved patch length, the first step in this is to do a linear layer to this dimension\n\n\nd_model\n\n\nthe dimension of the transformer model\n\n\naffine\nbool\nFalse\n\n\n\nshared_embedding\nbool\nTrue\n\n\n\n\n\nsource\n\n\n\n\n LinearProbingHead (c_in, predict_every_n_patches, n_classes, input_size,\n                    n_layers, num_patch, shared_embedding=True,\n                    affine=True, norm='BatchNorm', act='gelu',\n                    dropout=0.0)\n\nA linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels in the original input\n\n\npredict_every_n_patches\n\n\nfor a given sequence of length m with frequency f, number of predictions\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\ninput_size\n\n\nthe dimension of the transformer model\n\n\nn_layers\n\n\nthe number of linear layers to use in the prediciton head, with RELU activation and dropout\n\n\nnum_patch\n\n\n\n\n\nshared_embedding\nbool\nTrue\nwhether or not to have a dense layer per channel or one layer per channel\n\n\naffine\nbool\nTrue\ninclude learnable parameters to weight predictions\n\n\nnorm\nstr\nBatchNorm\nbatchnorm or layernorm between linear and convolutional layers\n\n\nact\nstr\ngelu\nactivation function to use between layers, ‘gelu’ or ‘relu’\n\n\ndropout\nfloat\n0.0\ndropout in between linear layers\n\n\n\n\nm = LinearProbingHead(c_in=7, \n                      input_size = 512, \n                      predict_every_n_patches=5,\n                      n_classes=5,\n                      n_layers=3,\n                      shared_embedding=True,\n                      affine=True,\n                      num_patch=3600,\n                      dropout=0.1)\n\nx = torch.randn((4,7,512,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\n TimeDistributedFeedForward (c_in, n_classes, n_patches, d_model,\n                             pred_len_seconds, n_linear_layers,\n                             conv_kernel_stride_size, dropout=0.0)\n\nFeed forward head that uses a convolutional layer to reduce channel dimensionality Followed by a feedforward network to make\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nthe number of input channels\n\n\nn_classes\n\n\nthe number of classes to predict (for sleep stage - there are 6)\n\n\nn_patches\n\n\nthe number of stft or time patches\n\n\nd_model\n\n\nthe dimension of the transformer model\n\n\npred_len_seconds\n\n\nthe sequence multiclass prediction length in seconds\n\n\nn_linear_layers\n\n\nthe number of linear layers to use in the prediciton head, with RELU activation and dropout\n\n\nconv_kernel_stride_size\n\n\nthe 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n\n\ndropout\nfloat\n0.0\ndropout in between linear layers\n\n\n\n\nsource\n\n\n\n\n ConvBiGRU (input_size, hidden_sizes, kernel_sizes, n_layers, d_model,\n            predict_every_n_patches, n_classes)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n ConvGRU1D (input_size, hidden_sizes, kernel_sizes, n_layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n ConvGRU1DCell (input_size, hidden_size, kernel_size)\n\nGenerate a convolutional GRU cell\n\nx = torch.randn((4,7,512,3600))\n\nconvgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n\nout = convgru(x)\nout.shape\n\ntorch.Size([4, 5, 720])",
    "crumbs": [
      "SSL, Fine Tuning, and Linear Probing Heads"
    ]
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "source\n\n\n\n TSTEncoderLayer (d_model, n_heads, d_ff=256, store_attn=False,\n                  norm='BatchNorm', relative_attn_type='vanilla',\n                  use_flash_attn=False, num_patches=None, attn_dropout=0,\n                  dropout=0.0, bias=True, activation='gelu',\n                  res_attention=False, pre_norm=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd_model\n\n\ndimension of patch embeddings\n\n\nn_heads\n\n\nnumber of attention heads per layer\n\n\nd_ff\nint\n256\ndimension of feedforward layer in each transformer layer\n\n\nstore_attn\nbool\nFalse\nindicator of whether or not to store attention\n\n\nnorm\nstr\nBatchNorm\n\n\n\nrelative_attn_type\nstr\nvanilla\noptions include vaniall or eRPE\n\n\nuse_flash_attn\nbool\nFalse\nindicator to use flash attention\n\n\nnum_patches\nNoneType\nNone\nnum patches required for eRPE attn\n\n\nattn_dropout\nint\n0\n\n\n\ndropout\nfloat\n0.0\n\n\n\nbias\nbool\nTrue\n\n\n\nactivation\nstr\ngelu\n\n\n\nres_attention\nbool\nFalse\n\n\n\npre_norm\nbool\nFalse",
    "crumbs": [
      "Transformers"
    ]
  },
  {
    "objectID": "transformers.html#time-series-transformer-encoder",
    "href": "transformers.html#time-series-transformer-encoder",
    "title": "Transformers",
    "section": "",
    "text": "source\n\n\n\n TSTEncoderLayer (d_model, n_heads, d_ff=256, store_attn=False,\n                  norm='BatchNorm', relative_attn_type='vanilla',\n                  use_flash_attn=False, num_patches=None, attn_dropout=0,\n                  dropout=0.0, bias=True, activation='gelu',\n                  res_attention=False, pre_norm=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd_model\n\n\ndimension of patch embeddings\n\n\nn_heads\n\n\nnumber of attention heads per layer\n\n\nd_ff\nint\n256\ndimension of feedforward layer in each transformer layer\n\n\nstore_attn\nbool\nFalse\nindicator of whether or not to store attention\n\n\nnorm\nstr\nBatchNorm\n\n\n\nrelative_attn_type\nstr\nvanilla\noptions include vaniall or eRPE\n\n\nuse_flash_attn\nbool\nFalse\nindicator to use flash attention\n\n\nnum_patches\nNoneType\nNone\nnum patches required for eRPE attn\n\n\nattn_dropout\nint\n0\n\n\n\ndropout\nfloat\n0.0\n\n\n\nbias\nbool\nTrue\n\n\n\nactivation\nstr\ngelu\n\n\n\nres_attention\nbool\nFalse\n\n\n\npre_norm\nbool\nFalse",
    "crumbs": [
      "Transformers"
    ]
  },
  {
    "objectID": "transformers.html#patch-time-series-and-frequency-transformer",
    "href": "transformers.html#patch-time-series-and-frequency-transformer",
    "title": "Transformers",
    "section": "Patch Time Series and Frequency Transformer",
    "text": "Patch Time Series and Frequency Transformer\n\nsource\n\nPatchTFTSimple\n\n PatchTFTSimple (c_in:int, win_length, hop_length, max_seq_len,\n                 time_domain=True, pos_encoding_type='learned',\n                 relative_attn_type='vanilla', use_flash_attn=False,\n                 use_revin=True, dim1reduce=False, affine=True,\n                 mask_ratio=0.1, augmentations=['patch_mask',\n                 'jitter_zero_mask', 'reverse_sequence',\n                 'shuffle_channels'], n_layers:int=2, d_model=512,\n                 n_heads=2, shared_embedding=False, d_ff:int=2048,\n                 norm:str='BatchNorm', attn_dropout:float=0.0,\n                 dropout:float=0.1, act:str='gelu',\n                 res_attention:bool=True, pre_norm:bool=False,\n                 store_attn:bool=False, pretrain_head=True,\n                 pretrain_head_n_layers=1, pretrain_head_dropout=0.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\nint\n\nthe number of input channels\n\n\nwin_length\n\n\nthe length of the patch of time/interval or short time ft windown length (when time_domain=False)\n\n\nhop_length\n\n\nthe length of the distance between each patch/fft\n\n\nmax_seq_len\n\n\nmaximum sequence len\n\n\ntime_domain\nbool\nTrue\n\n\n\npos_encoding_type\nstr\nlearned\noptions include learned or tAPE\n\n\nrelative_attn_type\nstr\nvanilla\noptions include vanilla or eRPE\n\n\nuse_flash_attn\nbool\nFalse\nindicator to use flash attention\n\n\nuse_revin\nbool\nTrue\nif time_domain is true, whether or not to instance normalize time data\n\n\ndim1reduce\nbool\nFalse\nindicator to normalize by timepoint in revin\n\n\naffine\nbool\nTrue\nif time_domain is true, whether or not to learn revin normalization parameters\n\n\nmask_ratio\nfloat\n0.1\namount of signal to mask\n\n\naugmentations\nlist\n[‘patch_mask’, ‘jitter_zero_mask’, ‘reverse_sequence’, ‘shuffle_channels’]\nthe type of mask to use, options are patch or jitter_zero\n\n\nn_layers\nint\n2\nthe number of transformer encoder layers to use\n\n\nd_model\nint\n512\nthe dimension of the input to the transofmrer encoder\n\n\nn_heads\nint\n2\nthe number of heads in each layer\n\n\nshared_embedding\nbool\nFalse\nindicator for whether or not each channel should be projected with its own set of linear weights to the encoder dimension\n\n\nd_ff\nint\n2048\nthe feedforward layer size in the transformer\n\n\nnorm\nstr\nBatchNorm\nBatchNorm or LayerNorm during trianing\n\n\nattn_dropout\nfloat\n0.0\ndropout in attention\n\n\ndropout\nfloat\n0.1\ndropout for linear layers\n\n\nact\nstr\ngelu\nactivation function\n\n\nres_attention\nbool\nTrue\nwhether to use residual attention\n\n\npre_norm\nbool\nFalse\nindicator to pre batch or layer norm\n\n\nstore_attn\nbool\nFalse\nindicator to store attention\n\n\npretrain_head\nbool\nTrue\nindicator to include a pretraining head\n\n\npretrain_head_n_layers\nint\n1\nhow many linear layers on the pretrained head\n\n\npretrain_head_dropout\nfloat\n0.0\ndropout applied to pretrain head\n\n\n\n\nXX = torch.randn(4,7,1*3600*100)\npad = torch.zeros(4,1*3600*100)\npad[:,0:100] = 1\nmodel = PatchTFTSimple(c_in=7,\n                        win_length=750,\n                        hop_length=750,\n                        max_seq_len=(1*3600*100),\n                        use_revin=True,\n                        time_domain=True,\n                        affine=False,\n                        dim1reduce=False,\n                        act='gelu',\n                        use_flash_attn=True,\n                        relative_attn_type='vanilla',\n                        pos_encoding_type='learned',\n                        mask_ratio=0.1,\n                        augmentations=['jitter_zero_mask'],\n                        n_layers=1,\n                        n_heads=1,\n                        d_model=512,\n                        d_ff=2048,\n                        dropout=0.,\n                        attn_dropout=0.,\n                        pre_norm=False,\n                        res_attention=False,\n                        shared_embedding=False,\n                        pretrain_head=True\n                        )\nr = model(XX, sequence_padding_mask=pad)\nr[0].shape, r[1].shape, r[3].shape\n\ntorch.Size([4, 480])\n\n\n(torch.Size([4, 480, 7, 750]),\n torch.Size([4, 480, 7, 750]),\n torch.Size([4, 480]))",
    "crumbs": [
      "Transformers"
    ]
  },
  {
    "objectID": "signal.html",
    "href": "signal.html",
    "title": "Signal",
    "section": "",
    "text": "source\n\nbutterworth\n\n butterworth (waveform_array, freq_range, btype, fs=125, order=2)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwaveform_array\n\n\n\n\n\nfreq_range\n\n\n\n\n\nbtype\n\n\n\n\n\nfs\nint\n125\n\n\n\norder\nint\n2\nRecommend playing around with the order as well\n\n\n\n\nsource\n\n\nstft\n\n stft (signal_array, n_fft=256, win_length=250, pad_mode='reflect',\n       pad_win_length_to_nfft=True, center=False, hop_length=125,\n       normalized=True, decibel_scale=False, return_complex=True,\n       onesided=True, channel_stft_means=None, channel_stft_stds=None)\n\nin: [bs x n_vars x max_seq_len] out: [bs x n_vars x n_fft // 2 + 1 x stft_len]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsignal_array\n\n\n\n\n\nn_fft\nint\n256\nnumber of ffts to perform\n\n\nwin_length\nint\n250\nwindow of ffts\n\n\npad_mode\nstr\nreflect\n\n\n\npad_win_length_to_nfft\nbool\nTrue\nindicator to pad the end of the sequence with 0s for nfft-win_legnth to ensuure correct size output\n\n\ncenter\nbool\nFalse\n\n\n\nhop_length\nint\n125\n\n\n\nnormalized\nbool\nTrue\n\n\n\ndecibel_scale\nbool\nFalse\n\n\n\nreturn_complex\nbool\nTrue\n\n\n\nonesided\nbool\nTrue\n\n\n\nchannel_stft_means\nNoneType\nNone\nprecalculated stft channel means\n\n\nchannel_stft_stds\nNoneType\nNone\nprecalculated stft channel stds",
    "crumbs": [
      "Signal"
    ]
  },
  {
    "objectID": "slumber.html#edf-helpers",
    "href": "slumber.html#edf-helpers",
    "title": "Slumber",
    "section": "EDF Helpers",
    "text": "EDF Helpers\n\nRead EDFs\n\nsource\n\n\nread_edf\n\n read_edf (file_path, channels=None, frequency=None)\n\nFunction to read an edf file and return a list of signals and header with the option to resample to a passed frequency\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nfile path of edf\n\n\nchannels\nNoneType\nNone\nchannels in edf to read, will raise warning if channels do not exist\n\n\nfrequency\nNoneType\nNone\nfrequency to resample all signals to\n\n\nReturns\nUnion\n\ntuple of signals and header dictionary\n\n\n\n\nsource\n\n\nread_edf_mne\n\n read_edf_mne (file_path, channels=None, frequency=None)\n\nfunction to read edf with mne library i dont recommend using this. Use edfio instead.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nfile path of edf\n\n\nchannels\nNoneType\nNone\nchannels in edf to read, will raise warning if channels do not exist\n\n\nfrequency\nNoneType\nNone\nfrequency to resample all signals to\n\n\nReturns\nUnion\n\ntuple of signals and header dictionary\n\n\n\n\nsource\n\n\nread_edf_edfio\n\n read_edf_edfio (file_path, channels=None, frequency=None)\n\nfunction to read edfs with edfio\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nfile path of edf\n\n\nchannels\nNoneType\nNone\nchannels in edf to read, will raise warning if channels do not exist\n\n\nfrequency\nNoneType\nNone\nfrequency to resample all signals to\n\n\nReturns\nUnion\n\ntuple of signals and header dictionary\n\n\n\n\n\nRead Hypnograms\n\nsource\n\n\nread_hypnogram\n\n read_hypnogram (file, epoch_length=None)\n\nFunction that reads a hypnogram csv and returns a numpy array of the hypnogram with optional repeats\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile\n\n\nfile path of the hypnogram csv\n\n\nepoch_length\nNoneType\nNone\nepoch length of the hypnogram measurements, if passed will repeat this many times at each element\n\n\nReturns\narray\n\nnumpy array of hypnogram\n\n\n\n\n\nEDFs to Zarr\n\nsource\n\n\nedf_signals_to_zarr\n\n edf_signals_to_zarr (edf_file_path, write_data_dir, overwrite=False,\n                      channels=None, channel_name_map=None,\n                      frequency=None, hyp_epoch_length=30,\n                      hyp_data_dir=None)\n\n*Function that converts an edf to a zarr file\ntry_mne: tries to load files with mne instead of pyedflib (if there is an error). This seems dangerous as mne converts units (and potentially resamples, while pyedflib does not)*",
    "crumbs": [
      "Slumber"
    ]
  },
  {
    "objectID": "slumber.html#datasets",
    "href": "slumber.html#datasets",
    "title": "Slumber",
    "section": "Datasets",
    "text": "Datasets\n\nSelf Supervised Dataset\n\nsource\n\n\ntrim_wake_epochs_from_signals\n\n trim_wake_epochs_from_signals (X, hypnogram, sequence_padding_mask,\n                                resampled_hypnogram_length,\n                                mask_x_with_zeros=False,\n                                padding_mask=-100)\n\n*Function to trim wake epochs (if wake is the largest class) from signals\nX: bs, channels, seq_len hypnogram: bs, seq_len / resampled_hypnogram_length sequence_padding_mask: bs, seq_len*\n\nsource\n\n\ntrim_wake_epochs_from_hypnogram\n\n trim_wake_epochs_from_hypnogram (hypnogram, padding_mask=-100)\n\n*Function to trim wake epochs (if wake is the largest class) from hypnograms This function trims the wake epochs from the beginning and/or end of the hypnogram\nAdapted from Phan et al L-SeqSleepNet*\n\nsource\n\n\nSelfSupervisedTimeFrequencyDataset\n\n SelfSupervisedTimeFrequencyDataset (zarr_files, channels,\n                                     max_seq_len_sec, sample_seq_len_sec,\n                                     sample_stride_sec,\n                                     start_offset_sec=0,\n                                     trim_wake_epochs=True,\n                                     include_partial_samples=True,\n                                     sample_df=None, frequency=125,\n                                     return_hypnogram_every_sec=30,\n                                     hypnogram_padding_mask=-100,\n                                     hypnogram_frequency=125,\n                                     butterworth_filters=None,\n                                     median_filter_kernel_size=None,\n                                     voltage_channels=['ECG', 'ECG (LL-\n                                     RA)', 'EKG', 'ECG (L-R)', 'EOG(L)',\n                                     'EOG-L', 'E1', 'LOC', 'E1-M2',\n                                     'E1-AVG', 'EMG', 'cchin_l', 'chin',\n                                     'EMG (L-R)', 'EMG (1-2)', 'EMG\n                                     (1-3)', 'Chin3', 'C4-M1', 'C4_M1',\n                                     'EEG', 'EEG1', 'EEG2', 'EEG3',\n                                     'C3-M2', 'C3_M2', 'C4-AVG'],\n                                     clip_interpolations=None,\n                                     scale_channels=False,\n                                     time_channel_scales=None,\n                                     return_sequence_padding_mask=False)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nzarr_files\n\n\nzarr files that include samples\n\n\nchannels\n\n\nchannels to use\n\n\nmax_seq_len_sec\n\n\nmaximum sequence length (in seconds) to use (this is especially relevant when you are returning both stft and raw ts data to keep them in sync)\n\n\nsample_seq_len_sec\n\n\nif no sample_df, generate sequences of this length in seconds as one sample\n\n\nsample_stride_sec\n\n\nif no sample_df, seconds of overlap for samples from the same array, if seq_len_seconds == overlap_seconds, there is no overlap\n\n\nstart_offset_sec\nint\n0\nnumber of seconds to exclude from beginning of sleep studies\n\n\ntrim_wake_epochs\nbool\nTrue\nindicator to trim wake epochs from hypnograms, if it is the largest class\n\n\ninclude_partial_samples\nbool\nTrue\nindicator to include data from partial samples when return_full_length is false\n\n\nsample_df\nNoneType\nNone\ndataframe indicating which indices within each zarr file includes a sample\n\n\nfrequency\nint\n125\nfrequency of underlying data\n\n\nreturn_hypnogram_every_sec\nint\n30\ninteger value indicating the step in indexing in seconds\n\n\nhypnogram_padding_mask\nint\n-100\npadded value to add to target and indice to ignore when computing loss\n\n\nhypnogram_frequency\nint\n125\nfrequency of underlying y hypnogram data\n\n\nbutterworth_filters\nNoneType\nNone\ndictionary of low pass, high pass, and bandpass dictionary to perform on channels\n\n\nmedian_filter_kernel_size\nNoneType\nNone\nif not none, will apply median filter with kernel size\n\n\nvoltage_channels\nlist\n[‘ECG’, ‘ECG (LL-RA)’, ‘EKG’, ‘ECG (L-R)’, ‘EOG(L)’, ‘EOG-L’, ‘E1’, ‘LOC’, ‘E1-M2’, ‘E1-AVG’, ‘EMG’, ‘cchin_l’, ‘chin’, ‘EMG (L-R)’, ‘EMG (1-2)’, ‘EMG (1-3)’, ‘Chin3’, ‘C4-M1’, ‘C4_M1’, ‘EEG’, ‘EEG1’, ‘EEG2’, ‘EEG3’, ‘C3-M2’, ‘C3_M2’, ‘C4-AVG’]\nif not None, these channels units will be looked at and changed to microvolts from mv uv etc.\n\n\nclip_interpolations\nNoneType\nNone\ndictionary of channels:{‘phys_range’:…, ‘percentiles’:…} for filtering and interpolation of filtered values\n\n\nscale_channels\nbool\nFalse\nindicator to scale channels to the mean and std of the zarr files.\n\n\ntime_channel_scales\nNoneType\nNone\ndictionary of channel:mean and channel:std values for scaling. Should use training statistics\n\n\nreturn_sequence_padding_mask\nbool\nFalse\nindicator to return the key padding mask for attention masking\n\n\n\n\n\nSleep Stage Supervised Dataset\n\nsource\n\n\nHypnogramTimeFrequencyDataset\n\n HypnogramTimeFrequencyDataset (zarr_files, channels, max_seq_len_sec,\n                                sample_seq_len_sec, sample_stride_sec,\n                                start_offset_sec=0, trim_wake_epochs=True,\n                                include_partial_samples=True,\n                                sample_df=None, frequency=125,\n                                return_y_every_sec=30,\n                                y_padding_mask=-100, y_frequency=125,\n                                butterworth_filters=None,\n                                median_filter_kernel_size=None,\n                                voltage_channels=['ECG', 'ECG (LL-RA)',\n                                'EKG', 'ECG (L-R)', 'EOG(L)', 'EOG-L',\n                                'E1', 'LOC', 'E1-M2', 'E1-AVG', 'EMG',\n                                'cchin_l', 'chin', 'EMG (L-R)', 'EMG\n                                (1-2)', 'EMG (1-3)', 'Chin3', 'C4-M1',\n                                'C4_M1', 'EEG', 'EEG1', 'EEG2', 'EEG3',\n                                'C3-M2', 'C3_M2', 'C4-AVG'],\n                                clip_interpolations=None,\n                                scale_channels=False,\n                                time_channel_scales=None,\n                                return_sequence_padding_mask=False)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nzarr_files\n\n\nzarr files that include samples\n\n\nchannels\n\n\nchannels to use\n\n\nmax_seq_len_sec\n\n\nmaximum sequence length (in seconds) to use (this is especially relevant when you are returning both stft and raw ts data to keep them in sync)\n\n\nsample_seq_len_sec\n\n\nif no sample_df, generate sequences of this length in seconds as one sample\n\n\nsample_stride_sec\n\n\nif no sample_df, seconds of overlap for samples from the same array, if seq_len_seconds == overlap_seconds, there is no overlap\n\n\nstart_offset_sec\nint\n0\nnumber of seconds to exclude from beginning of sleep studies\n\n\ntrim_wake_epochs\nbool\nTrue\nindicator to trim wake epochs from hypnograms, if it is the largest class\n\n\ninclude_partial_samples\nbool\nTrue\nindicator to include data from partial samples when return_full_length is false\n\n\nsample_df\nNoneType\nNone\ndataframe indicating which indices within each zarr file includes a sample\n\n\nfrequency\nint\n125\nfrequency of underlying data\n\n\nreturn_y_every_sec\nint\n30\ninteger value indicating the step in indexing in seconds\n\n\ny_padding_mask\nint\n-100\npadded value to add to target and indice to ignore when computing loss\n\n\ny_frequency\nint\n125\nfrequency of underlying y hypnogram data\n\n\nbutterworth_filters\nNoneType\nNone\ndictionary of low pass, high pass, and bandpass dictionary to perform on channels\n\n\nmedian_filter_kernel_size\nNoneType\nNone\nif not none, will apply median filter with kernel size\n\n\nvoltage_channels\nlist\n[‘ECG’, ‘ECG (LL-RA)’, ‘EKG’, ‘ECG (L-R)’, ‘EOG(L)’, ‘EOG-L’, ‘E1’, ‘LOC’, ‘E1-M2’, ‘E1-AVG’, ‘EMG’, ‘cchin_l’, ‘chin’, ‘EMG (L-R)’, ‘EMG (1-2)’, ‘EMG (1-3)’, ‘Chin3’, ‘C4-M1’, ‘C4_M1’, ‘EEG’, ‘EEG1’, ‘EEG2’, ‘EEG3’, ‘C3-M2’, ‘C3_M2’, ‘C4-AVG’]\nif not None, these channels units will be looked at and changed to microvolts from mv uv etc.\n\n\nclip_interpolations\nNoneType\nNone\ndictionary of channels:{‘phys_range’:…, ‘percentiles’:…} for filtering and interpolation of filtered values\n\n\nscale_channels\nbool\nFalse\nindicator to scale channels to the mean and std of the zarr files.\n\n\ntime_channel_scales\nNoneType\nNone\ndictionary of channel:mean and channel:std values for scaling. Should use training statistics\n\n\nreturn_sequence_padding_mask\nbool\nFalse\nindicator to return the key padding mask for attention masking",
    "crumbs": [
      "Slumber"
    ]
  },
  {
    "objectID": "slumber.html#plotting",
    "href": "slumber.html#plotting",
    "title": "Slumber",
    "section": "Plotting",
    "text": "Plotting\n\nsource\n\nplot_edf_signals\n\n plot_edf_signals (signals, signal_names, signal_comparisons=None,\n                   use_resampler=False, normalize=False, title_text='',\n                   colorscale=None)",
    "crumbs": [
      "Slumber"
    ]
  },
  {
    "objectID": "data_preprocessing.html",
    "href": "data_preprocessing.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "source\n\ncalculate_class_weights\n\n calculate_class_weights (dataloader, ignore_index=-100,\n                          returns_padded_mask=True, return_ratio=True)\n\n\nsource\n\n\ninterpolate_nan_clip\n\n interpolate_nan_clip (x_in, physiological_range_clip=None,\n                       percentile_clip=None, return_mask_only=False)\n\nFunction to clip outliers based on percentiles or physiological range and then interpolate nearby values\n\nsource\n\n\ncalculate_stats_all\n\n calculate_stats_all (zarr_files, channels, sample_wise=True,\n                      clip_interpolations=None,\n                      channel_magnitude_multiple=None)\n\n\nsource\n\n\ncalculate_stats\n\n calculate_stats (idx, zarr_file, channels, clip_interpolations=None,\n                  channel_magnitude_multiple=None)\n\nFunction to caluclate stats on an individual zarr array, including a clip interpolate range\n\nsource\n\n\ncalculate_samples_mp\n\n calculate_samples_mp (zarr_files, channels, frequency,\n                       sample_seq_len_sec, stride_sec,\n                       start_offset_sec=None, max_seq_len_sec=None,\n                       include_partial_samples=True, nan_tolerance=0.0)\n\nMultiprocessing function to generate samples\n\nsource\n\n\ncalculate_samples\n\n calculate_samples (idx, zarr_file, channels, frequency,\n                    sample_seq_len_sec, stride_sec, start_offset_sec=None,\n                    max_seq_len_sec=None, include_partial_samples=True,\n                    nan_tolerance=0.0)\n\nFunction to create a dataframe of samples and their sequence indices",
    "crumbs": [
      "Data Preprocessing"
    ]
  },
  {
    "objectID": "train.html",
    "href": "train.html",
    "title": "Train",
    "section": "",
    "text": "source\n\n\n\n PatchTFTSimpleLightning (learning_rate, train_size, batch_size, channels,\n                          metrics, precalculate_onebatch_stft_stats=False,\n                          use_sequence_padding_mask=False,\n                          loss_func='mse', max_lr=0.01, weight_decay=0.0,\n                          epochs=100, one_cycle_scheduler=True,\n                          optimizer_type='Adam',\n                          scheduler_type='OneCycle',\n                          cross_attention=False, use_mask=False,\n                          patch_continuity_loss=0, huber_delta=None,\n                          **patchmeup_kwargs)\n\nHooks to be used in LightningModule.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlearning_rate\n\n\n\n\n\ntrain_size\n\n\n\n\n\nbatch_size\n\n\n\n\n\nchannels\n\n\n\n\n\nmetrics\n\n\n\n\n\nprecalculate_onebatch_stft_stats\nbool\nFalse\n\n\n\nuse_sequence_padding_mask\nbool\nFalse\n\n\n\nloss_func\nstr\nmse\n\n\n\nmax_lr\nfloat\n0.01\n\n\n\nweight_decay\nfloat\n0.0\n\n\n\nepochs\nint\n100\n\n\n\none_cycle_scheduler\nbool\nTrue\n\n\n\noptimizer_type\nstr\nAdam\n\n\n\nscheduler_type\nstr\nOneCycle\n\n\n\ncross_attention\nbool\nFalse\nnot implemented\n\n\nuse_mask\nbool\nFalse\n\n\n\npatch_continuity_loss\nint\n0\nindicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities\n\n\nhuber_delta\nNoneType\nNone\nhuber loss delta, not used otherwise\n\n\npatchmeup_kwargs\nVAR_KEYWORD\n\n\n\n\n\n\nclass EncoderTestFreezingWeights(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(750,512)\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n\nclass DecoderTest(pl.LightningModule):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        self.encoder.freeze()\n    def training_step(self, batch, batch_idx):\n        print(list(self.encoder.parameters()))\n        print(self.encoder.training)\n        # x = self.encoder(x)\n        # return x\n\nencoder = EncoderTestFreezingWeights()\ndecoder = DecoderTest(encoder)\n\ndecoder.training_step(0,0)",
    "crumbs": [
      "Train"
    ]
  },
  {
    "objectID": "train.html#self-supervised-patchtft-lightning",
    "href": "train.html#self-supervised-patchtft-lightning",
    "title": "Train",
    "section": "",
    "text": "source\n\n\n\n PatchTFTSimpleLightning (learning_rate, train_size, batch_size, channels,\n                          metrics, precalculate_onebatch_stft_stats=False,\n                          use_sequence_padding_mask=False,\n                          loss_func='mse', max_lr=0.01, weight_decay=0.0,\n                          epochs=100, one_cycle_scheduler=True,\n                          optimizer_type='Adam',\n                          scheduler_type='OneCycle',\n                          cross_attention=False, use_mask=False,\n                          patch_continuity_loss=0, huber_delta=None,\n                          **patchmeup_kwargs)\n\nHooks to be used in LightningModule.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlearning_rate\n\n\n\n\n\ntrain_size\n\n\n\n\n\nbatch_size\n\n\n\n\n\nchannels\n\n\n\n\n\nmetrics\n\n\n\n\n\nprecalculate_onebatch_stft_stats\nbool\nFalse\n\n\n\nuse_sequence_padding_mask\nbool\nFalse\n\n\n\nloss_func\nstr\nmse\n\n\n\nmax_lr\nfloat\n0.01\n\n\n\nweight_decay\nfloat\n0.0\n\n\n\nepochs\nint\n100\n\n\n\none_cycle_scheduler\nbool\nTrue\n\n\n\noptimizer_type\nstr\nAdam\n\n\n\nscheduler_type\nstr\nOneCycle\n\n\n\ncross_attention\nbool\nFalse\nnot implemented\n\n\nuse_mask\nbool\nFalse\n\n\n\npatch_continuity_loss\nint\n0\nindicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities\n\n\nhuber_delta\nNoneType\nNone\nhuber loss delta, not used otherwise\n\n\npatchmeup_kwargs\nVAR_KEYWORD\n\n\n\n\n\n\nclass EncoderTestFreezingWeights(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(750,512)\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n\nclass DecoderTest(pl.LightningModule):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        self.encoder.freeze()\n    def training_step(self, batch, batch_idx):\n        print(list(self.encoder.parameters()))\n        print(self.encoder.training)\n        # x = self.encoder(x)\n        # return x\n\nencoder = EncoderTestFreezingWeights()\ndecoder = DecoderTest(encoder)\n\ndecoder.training_step(0,0)",
    "crumbs": [
      "Train"
    ]
  },
  {
    "objectID": "train.html#supervised-training-with-linear-probing",
    "href": "train.html#supervised-training-with-linear-probing",
    "title": "Train",
    "section": "Supervised Training with Linear Probing",
    "text": "Supervised Training with Linear Probing\n\nPatchTFT Masked Autoregression SS Prediction\n\nsource\n\n\nPatchTFTSleepStage\n\n PatchTFTSleepStage (learning_rate, train_size, batch_size,\n                     linear_probing_head, metrics=[], fine_tune=False,\n                     loss_fxn='CrossEntropy', class_weights=None,\n                     gamma=2.0, label_smoothing=0,\n                     use_sequence_padding_mask=False, y_padding_mask=-100,\n                     max_lr=0.01, epochs=100, one_cycle_scheduler=True,\n                     weight_decay=0.0, pretrained_encoder_path=None,\n                     optimizer_type='Adam', scheduler_type='OneCycle',\n                     preloaded_model=None, torch_model_name='model',\n                     remove_pretrain_layers=['head', 'mask'],\n                     return_softmax=True)\n\nHooks to be used in LightningModule.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlearning_rate\n\n\ndesired learning rate, initial learning rate in if one_cycle_scheduler\n\n\ntrain_size\n\n\nthe training data size (for one_cycle_scheduler=True)\n\n\nbatch_size\n\n\nthe batch size (for one_cycle_scheduler=True)\n\n\nlinear_probing_head\n\n\nmodel head to train\n\n\nmetrics\nlist\n[]\nmetrics to calculate\n\n\nfine_tune\nbool\nFalse\nindicator to finetune encoder model or perform linear probing and freeze encoder weights\n\n\nloss_fxn\nstr\nCrossEntropy\nloss function to use, can be CrossEntropy or FocalLoss\n\n\nclass_weights\nNoneType\nNone\nweights of classes to use in CE loss fxn\n\n\ngamma\nfloat\n2.0\nfor focal loss\n\n\nlabel_smoothing\nint\n0\nlabel smoothing for cross entropy loss\n\n\nuse_sequence_padding_mask\nbool\nFalse\nindicator to use the sequence padding mask when training/in the loss fxn\n\n\ny_padding_mask\nint\n-100\npadded value that was added to target and indice to ignore when computing loss\n\n\nmax_lr\nfloat\n0.01\nmaximum learning rate for one_cycle_scheduler\n\n\nepochs\nint\n100\nnumber of epochs for one_cycle_scheduler\n\n\none_cycle_scheduler\nbool\nTrue\nindicator to use a one cycle scheduler to vary the learning rate\n\n\nweight_decay\nfloat\n0.0\nweight decay for Adam optimizer\n\n\npretrained_encoder_path\nNoneType\nNone\npath of the pretrained model to use for linear probing\n\n\noptimizer_type\nstr\nAdam\noptimizer to use, ‘Adam’ or ‘AdamW’\n\n\nscheduler_type\nstr\nOneCycle\nscheduler to use, ‘OneCycle’ or ‘CosineAnnealingWarmRestarts’\n\n\npreloaded_model\nNoneType\nNone\nloaded pretrained model to use for linear probing\n\n\ntorch_model_name\nstr\nmodel\nname of the pytorch model within the lightning model module, this is to remove layers (for example lightning_model.pytorch_model.head = nn.Identity())\n\n\nremove_pretrain_layers\nlist\n[‘head’, ‘mask’]\nlayers within the lightning model or lightning model.pytorch_model to remove\n\n\nreturn_softmax\nbool\nTrue\nindicator to return softmax probabilities in forward and predict_step",
    "crumbs": [
      "Train"
    ]
  }
]