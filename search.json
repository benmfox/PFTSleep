[
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "source\n\n\n\ndef TSTEncoderLayer(\n    d_model, # dimension of patch embeddings\n    n_heads, # number of attention heads per layer\n    d_ff:int=256, # dimension of feedforward layer in each transformer layer\n    store_attn:bool=False, # indicator of whether or not to store attention\n    norm:str='BatchNorm', relative_attn_type:str='vanilla', # options include vaniall or eRPE\n    use_flash_attn:bool=False, # indicator to use flash attention\n    num_patches:NoneType=None, # num patches required for eRPE attn\n    attn_dropout:int=0, dropout:float=0.0, bias:bool=True, activation:str='gelu', res_attention:bool=False,\n    pre_norm:bool=False\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Transformers"
    ]
  },
  {
    "objectID": "transformers.html#time-series-transformer-encoder",
    "href": "transformers.html#time-series-transformer-encoder",
    "title": "Transformers",
    "section": "",
    "text": "source\n\n\n\ndef TSTEncoderLayer(\n    d_model, # dimension of patch embeddings\n    n_heads, # number of attention heads per layer\n    d_ff:int=256, # dimension of feedforward layer in each transformer layer\n    store_attn:bool=False, # indicator of whether or not to store attention\n    norm:str='BatchNorm', relative_attn_type:str='vanilla', # options include vaniall or eRPE\n    use_flash_attn:bool=False, # indicator to use flash attention\n    num_patches:NoneType=None, # num patches required for eRPE attn\n    attn_dropout:int=0, dropout:float=0.0, bias:bool=True, activation:str='gelu', res_attention:bool=False,\n    pre_norm:bool=False\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Transformers"
    ]
  },
  {
    "objectID": "transformers.html#patch-time-series-and-frequency-transformer",
    "href": "transformers.html#patch-time-series-and-frequency-transformer",
    "title": "Transformers",
    "section": "Patch Time Series and Frequency Transformer",
    "text": "Patch Time Series and Frequency Transformer\n\nsource\n\nPatchTFTSimple\n\ndef PatchTFTSimple(\n    c_in:int, # the number of input channels\n    win_length, # the length of the patch of time/interval or short time ft windown length (when time_domain=False)\n    hop_length, # the length of the distance between each patch/fft\n    max_seq_len, # maximum sequence len\n    time_domain:bool=True, pos_encoding_type:str='learned', # options include learned or tAPE\n    relative_attn_type:str='vanilla', # options include vanilla or eRPE\n    use_flash_attn:bool=False, # indicator to use flash attention\n    use_revin:bool=True, # if time_domain is true, whether or not to instance normalize time data\n    dim1reduce:bool=False, # indicator to normalize by timepoint in revin\n    affine:bool=True, # if time_domain is true, whether or not to learn revin normalization parameters\n    mask_ratio:float=0.1, # amount of signal to mask\n    augmentations:list=['patch_mask', 'jitter_zero_mask', 'reverse_sequence', 'shuffle_channels'], # the type of mask to use, options are patch or jitter_zero\n    n_layers:int=2, # the number of transformer encoder layers to use\n    d_model:int=512, # the dimension of the input to the transofmrer encoder\n    n_heads:int=2, # the number of heads in each layer\n    shared_embedding:bool=False, # indicator for whether or not each channel should be projected with its own set of linear weights to the encoder dimension\n    d_ff:int=2048, # the feedforward layer size in the transformer\n    norm:str='BatchNorm', # BatchNorm or LayerNorm during trianing\n    attn_dropout:float=0.0, # dropout in attention\n    dropout:float=0.1, # dropout for linear layers\n    act:str='gelu', # activation function\n    res_attention:bool=True, # whether to use residual attention\n    pre_norm:bool=False, # indicator to pre batch or layer norm\n    store_attn:bool=False, # indicator to store attention\n    pretrain_head:bool=True, # indicator to include a pretraining head\n    pretrain_head_n_layers:int=1, # how many linear layers on the pretrained head\n    pretrain_head_dropout:float=0.0, # dropout applied to pretrain head\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nXX = torch.randn(4,7,1*3600*100)\npad = torch.zeros(4,1*3600*100)\npad[:,0:100] = 1\nmodel = PatchTFTSimple(c_in=7,\n                        win_length=750,\n                        hop_length=750,\n                        max_seq_len=(1*3600*100),\n                        use_revin=True,\n                        time_domain=True,\n                        affine=False,\n                        dim1reduce=False,\n                        act='gelu',\n                        use_flash_attn=True,\n                        relative_attn_type='vanilla',\n                        pos_encoding_type='learned',\n                        mask_ratio=0.1,\n                        augmentations=['jitter_zero_mask'],\n                        n_layers=1,\n                        n_heads=1,\n                        d_model=512,\n                        d_ff=2048,\n                        dropout=0.,\n                        attn_dropout=0.,\n                        pre_norm=False,\n                        res_attention=False,\n                        shared_embedding=False,\n                        pretrain_head=True\n                        )\nr = model(XX, sequence_padding_mask=pad)\nr[0].shape, r[1].shape, r[3].shape\n\ntorch.Size([4, 480])\n\n\n(torch.Size([4, 480, 7, 750]),\n torch.Size([4, 480, 7, 750]),\n torch.Size([4, 480]))",
    "crumbs": [
      "Transformers"
    ]
  },
  {
    "objectID": "train.html",
    "href": "train.html",
    "title": "Train",
    "section": "",
    "text": "source\n\n\n\ndef PatchTFTSimpleLightning(\n    learning_rate, train_size, batch_size, channels, metrics, precalculate_onebatch_stft_stats:bool=False,\n    use_sequence_padding_mask:bool=False, loss_func:str='mse', max_lr:float=0.01, weight_decay:float=0.0,\n    epochs:int=100, one_cycle_scheduler:bool=True, optimizer_type:str='Adam', scheduler_type:str='OneCycle',\n    cross_attention:bool=False, # not implemented\n    use_mask:bool=False,\n    patch_continuity_loss:int=0, # indicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities\n    huber_delta:NoneType=None, # huber loss delta, not used otherwise\n    patchmeup_kwargs:VAR_KEYWORD\n):\n\nHooks to be used in LightningModule.\n\nclass EncoderTestFreezingWeights(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(750,512)\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n\nclass DecoderTest(pl.LightningModule):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        self.encoder.freeze()\n    def training_step(self, batch, batch_idx):\n        print(list(self.encoder.parameters()))\n        print(self.encoder.training)\n        # x = self.encoder(x)\n        # return x\n\nencoder = EncoderTestFreezingWeights()\ndecoder = DecoderTest(encoder)\n\ndecoder.training_step(0,0)",
    "crumbs": [
      "Train"
    ]
  },
  {
    "objectID": "train.html#self-supervised-patchtft-lightning",
    "href": "train.html#self-supervised-patchtft-lightning",
    "title": "Train",
    "section": "",
    "text": "source\n\n\n\ndef PatchTFTSimpleLightning(\n    learning_rate, train_size, batch_size, channels, metrics, precalculate_onebatch_stft_stats:bool=False,\n    use_sequence_padding_mask:bool=False, loss_func:str='mse', max_lr:float=0.01, weight_decay:float=0.0,\n    epochs:int=100, one_cycle_scheduler:bool=True, optimizer_type:str='Adam', scheduler_type:str='OneCycle',\n    cross_attention:bool=False, # not implemented\n    use_mask:bool=False,\n    patch_continuity_loss:int=0, # indicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities\n    huber_delta:NoneType=None, # huber loss delta, not used otherwise\n    patchmeup_kwargs:VAR_KEYWORD\n):\n\nHooks to be used in LightningModule.\n\nclass EncoderTestFreezingWeights(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(750,512)\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n\nclass DecoderTest(pl.LightningModule):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        self.encoder.freeze()\n    def training_step(self, batch, batch_idx):\n        print(list(self.encoder.parameters()))\n        print(self.encoder.training)\n        # x = self.encoder(x)\n        # return x\n\nencoder = EncoderTestFreezingWeights()\ndecoder = DecoderTest(encoder)\n\ndecoder.training_step(0,0)",
    "crumbs": [
      "Train"
    ]
  },
  {
    "objectID": "train.html#supervised-training-with-linear-probing",
    "href": "train.html#supervised-training-with-linear-probing",
    "title": "Train",
    "section": "Supervised Training with Linear Probing",
    "text": "Supervised Training with Linear Probing\n\nPatchTFT Masked Autoregression SS Prediction\n\nsource\n\n\nPatchTFTSleepStage\n\ndef PatchTFTSleepStage(\n    learning_rate, # desired learning rate, initial learning rate in if one_cycle_scheduler\n    train_size, # the training data size (for one_cycle_scheduler=True)\n    batch_size, # the batch size (for one_cycle_scheduler=True)\n    linear_probing_head, # model head to train\n    metrics:list=[], # metrics to calculate\n    fine_tune:bool=False, # indicator to finetune encoder model or perform linear probing and freeze encoder weights\n    loss_fxn:str='CrossEntropy', # loss function to use, can be CrossEntropy or FocalLoss\n    class_weights:NoneType=None, # weights of classes to use in CE loss fxn\n    gamma:float=2.0, # for focal loss\n    label_smoothing:int=0, # label smoothing for cross entropy loss\n    use_sequence_padding_mask:bool=False, # indicator to use the sequence padding mask when training/in the loss fxn\n    y_padding_mask:int=-100, # padded value that was added to target and indice to ignore when computing loss\n    max_lr:float=0.01, # maximum learning rate for one_cycle_scheduler\n    epochs:int=100, # number of epochs for one_cycle_scheduler\n    one_cycle_scheduler:bool=True, # indicator to use a one cycle scheduler to vary the learning rate\n    weight_decay:float=0.0, # weight decay for Adam optimizer\n    pretrained_encoder_path:NoneType=None, # path of the pretrained model to use for linear probing\n    optimizer_type:str='Adam', # optimizer to use, 'Adam' or 'AdamW'\n    scheduler_type:str='OneCycle', # scheduler to use, 'OneCycle' or 'CosineAnnealingWarmRestarts'\n    preloaded_model:NoneType=None, # loaded pretrained model to use for linear probing\n    torch_model_name:str='model', # name of the pytorch model within the lightning model module, this is to remove layers (for example lightning_model.pytorch_model.head = nn.Identity())\n    remove_pretrain_layers:list=['head', 'mask'], # layers within the lightning model or lightning model.pytorch_model to remove\n    return_softmax:bool=True, # indicator to return softmax probabilities in forward and predict_step\n):\n\nHooks to be used in LightningModule.",
    "crumbs": [
      "Train"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PFTSleep",
    "section": "",
    "text": "PFTSleep is a Python package for sleep stage classification using a pre-trained foundational transformer. The repository is built using nbdev, which means the package is developed in Jupyter notebooks.\nSee the publication in SLEEP and original preprint for more details.",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "PFTSleep",
    "section": "",
    "text": "PFTSleep is a Python package for sleep stage classification using a pre-trained foundational transformer. The repository is built using nbdev, which means the package is developed in Jupyter notebooks.\nSee the publication in SLEEP and original preprint for more details.",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "PFTSleep",
    "section": "Install",
    "text": "Install\npip install PFTSleep",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#inference",
    "href": "index.html#inference",
    "title": "PFTSleep",
    "section": "Inference",
    "text": "Inference\nTo perform inference on unseen data (using EDF file paths as input), use the pft_sleep_inference.sh, pft_sleep_inference.py and pftsleep_inference_config.yaml files.\nPlease create an account on Hugging Face and request access to the models here. You will also need to create a personal access token, with read access. Read more here.\nFollowing, download the encoder and sleep stage classifier models with:\nfrom pftsleep.inference import download_pftsleep_models\n\ndownload_pftsleep_models(models_dir='', token=YOUR_HF_TOKEN)\nYou will also be prompted to download the files when running the inference script, if they are not in models_dir.\nAfter the models are downloaded, update the pftsleep_inference_config.yaml file with a path to your edf or edf directory. You can pass both a single edf file or a directory of edfs. You can also use glob syntax if specifying edfs within single sub directories (e.g. /path/to/base/directory/**/).\nUnfortunately, due to differing naming conventions for signal channels, if you are passing multiple edfs, but they have different channel names, the dataloader will fail. It is recommended in this case to rewrite the edfs to a consistent channel name format, or perform inference on them one by one.\nIf a specific channel is not available for a given edf or set of edfs, pass the keyword “null” or “dummy” to that channels name parameter in the yaml if you’d like to see how the model performs with that channel set as all zeros.\nThe model expects referenced EEG, Left EOG, EMG, and ECG channels. If your channels are unreferenced, you may pass the corresponding reference channels. The model was trained with the following referenced channels (priority was given to the ones listed first in the below list):\nEEG: C4-M1 or C3-M2\nEMG: Chin1-Chin2 or Chin1-Chin3\nECG: Augmented lead 2 (or ECG (LL) - ECG (RA))\nLeft EOG: E1-M2\nCheck the slumber.py source code for NSRR specific channels (under the SHHS_CHANNELS, MROS_CHANNELS, WSC_CHANNELS, etc. variables) if the above is confusing.\nFor the device parameter, use “cpu” (slowest), GPU (e.g. “cuda:0”), or MPS (“mps” for Mac OS X).\nPrediction logits of shape [bs x 5 x 960] are outputted. The first dimension indicates individual sleep stage logits where the 0 index is wake, 1 is N1, 2 is N2, 3 is N3, and 4 is REM. To retrieve probabilities, use torch’s softmax function on the 1st dimension of the tensor. Note that the model expects an 8 hour input and returns 960 class predictions for each 30 second sleep epoch within the 8 hours. If the sleep study is longer than 8 hours, stages after the 8 hour time point will not be predicted. If the sleep study is shorter than 8 hours, stages predicted after the true length of the study should be ignored (despite the model outputted a size of 960). Please file an issue if this becomes a major problem and we can work on a solution. We are also working on models that will accept variable length input.\nTo finally run the predictions on a single edf file or directory of edf files, give permissions to the shell script:\nchmod +x pftsleep_inference.sh\nThen run it, passing the config file as the main argument:\n./pftsleep_inference.sh pftsleep_inference_config.yaml\nPredictions will be output to a torch tensor file .pt at the location specified in the yaml.\nAdditionally, you can pass the save_hypjson parameter in the yaml as true. This will perform softmax, max index selecting, and save the predictions as a HYPJSON file (with the same filename as the edf file, + ’_pftsleep.HYPJSON’). You can also use the write_pred_to_hypjson function for individual files. Sleep stages are mapped in this function to typical HYPJSON standards (for example, REM is mapped to the integer “5”).",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#repository-structure-and-usage",
    "href": "index.html#repository-structure-and-usage",
    "title": "PFTSleep",
    "section": "Repository Structure and Usage",
    "text": "Repository Structure and Usage\nThis is an nbdev repository, which means the package is developed in Jupyter notebooks located in the nbs/ directory. Any modifications or additions to the PFTSleep package should be made by editing these notebooks.\nTo build the package, run nbdev_prepare in the terminal. This will generate the PFTSleep package in the PFTSleep/ directory and all python modules, which can be imported and used in other Python projects.\nTo add new functionality, create a new notebook or add to exisitng in the nbs/ directory and follow the instructions in the nbdev documentation to add the new functionality. Then, run nbdev_prepare to generate the PFTSleep package with the new functionality.\n\nDirectory Structure:\n\nnbs/: Contains the source notebooks that generate the Python package\njobs/: Contains processing and training scripts\n\napples/: Processing scripts for the apples dataset\nmesa/: Processing scripts for the mesa dataset\nmros/: Processing scripts for the mros dataset\nshhs/: Processing scripts for the shhs dataset\nwsc/: Processing scripts for the wsc dataset\nmodel_training/:\n\ntrain_transformer.py: Trains the initial foundational transformer model\ntrain_classifier.py: Trains the probing head for sleep stage classification\n\n\n\nEach dataset directory contains scripts to: - Create hypnogram CSVs from annotations - Build zarr files from EDF files - Process and standardize the data for model training\n\n\nModel Training Pipeline\n\nFoundation Model Training (jobs/model_training/train_transformer.py)\n\nTrains the base transformer model on sleep data zarr files\nCreates general purpose representations of sleep signals\n\nProbe Training (jobs/model_training/train_classifier.py)\n\nTrains a classification head on top of the foundation model",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "PFTSleep",
    "section": "Technical Details",
    "text": "Technical Details\n\nWe trained the foundational model on 2x H100 80gb GPUs using PyTorch Lightning.\nWe monitored training using the Weights and Biases platform.\nWe performed hyperparameter optimization using Optuna.",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "PFTSleep",
    "section": "Citation",
    "text": "Citation\nIf you use PFTSleep in your research, please cite:\n@ARTICLE{Fox2025-zc,\n  title     = \"A foundational transformer leveraging full night, multichannel\n               sleep study data accurately classifies sleep stages\",\n  author    = \"Fox, Benjamin and Jiang, Joy and Wickramaratne, Sajila and\n               Kovatch, Patricia and Suarez-Farinas, Mayte and Shah, Neomi A\n               and Parekh, Ankit and Nadkarni, Girish N\",\n  journal   = \"Sleep\",\n  publisher = \"Oxford University Press (OUP)\",\n  month     =  mar,\n  year      =  2025,\n  language  = \"en\"\n}",
    "crumbs": [
      "PFTSleep"
    ]
  },
  {
    "objectID": "data_preprocessing.html",
    "href": "data_preprocessing.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "source\n\ncalculate_class_weights\n\ndef calculate_class_weights(\n    dataloader, ignore_index:int=-100, returns_padded_mask:bool=True, return_ratio:bool=True\n):\n\n\nsource\n\n\ninterpolate_nan_clip\n\ndef interpolate_nan_clip(\n    x_in, physiological_range_clip:NoneType=None, percentile_clip:NoneType=None, return_mask_only:bool=False\n):\n\nFunction to clip outliers based on percentiles or physiological range and then interpolate nearby values\n\nsource\n\n\ncalculate_stats_all\n\ndef calculate_stats_all(\n    zarr_files, channels, sample_wise:bool=True, clip_interpolations:NoneType=None,\n    channel_magnitude_multiple:NoneType=None\n):\n\n\nsource\n\n\ncalculate_stats\n\ndef calculate_stats(\n    idx, zarr_file, channels, clip_interpolations:NoneType=None, channel_magnitude_multiple:NoneType=None\n):\n\nFunction to caluclate stats on an individual zarr array, including a clip interpolate range\n\nsource\n\n\ncalculate_samples_mp\n\ndef calculate_samples_mp(\n    zarr_files, channels, frequency, sample_seq_len_sec, stride_sec, start_offset_sec:NoneType=None,\n    max_seq_len_sec:NoneType=None, include_partial_samples:bool=True, nan_tolerance:float=0.0\n):\n\nMultiprocessing function to generate samples\n\nsource\n\n\ncalculate_samples\n\ndef calculate_samples(\n    idx, zarr_file, channels, frequency, sample_seq_len_sec, stride_sec, start_offset_sec:NoneType=None,\n    max_seq_len_sec:NoneType=None, include_partial_samples:bool=True, nan_tolerance:float=0.0\n):\n\nFunction to create a dataframe of samples and their sequence indices",
    "crumbs": [
      "Data Preprocessing"
    ]
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "source\n\n\n\ndef PatchEncoder(\n    c_in, # the number of input channels\n    patch_len, # the length of the patches (either stft or interval length)\n    d_model, # the dimension of the initial linear layers for inputting patches into transformer\n    shared_embedding, # indicator of whether to project each channel individually or together\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#linear-layers-for-patches",
    "href": "layers.html#linear-layers-for-patches",
    "title": "Layers",
    "section": "",
    "text": "source\n\n\n\ndef PatchEncoder(\n    c_in, # the number of input channels\n    patch_len, # the length of the patches (either stft or interval length)\n    d_model, # the dimension of the initial linear layers for inputting patches into transformer\n    shared_embedding, # indicator of whether to project each channel individually or together\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#positional-encoding-layers",
    "href": "layers.html#positional-encoding-layers",
    "title": "Layers",
    "section": "Positional Encoding Layers",
    "text": "Positional Encoding Layers\n\nsource\n\nPositionalEncoding\n\ndef PositionalEncoding(\n    num_patch, # number of patches of time series or stft in input\n    d_model, # dimension of patch embeddings\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\ntAPE\n\ndef tAPE(\n    d_model:int, # the embedding dimension\n    seq_len:int, # the max. length of the incoming sequence or num patches\n    scale_factor:float=1.0, # dropout:float=0., # dropout value\n):\n\ntime Absolute Position Encoding Adapted from tsai",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#mask-and-augmentation-layers",
    "href": "layers.html#mask-and-augmentation-layers",
    "title": "Layers",
    "section": "Mask and Augmentation Layers",
    "text": "Mask and Augmentation Layers\n\nsource\n\nMask\n\ndef Mask(\n    mask_type, mask_ratio, return_mask:bool=True\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\ntorch.manual_seed(125)\nm = Mask(mask_type='jitter_zero', mask_ratio=0.5)\nx = torch.randn((9))\n\nm(x), m(x)\n\n((tensor([ 0.0000,  0.0000,  0.0000,  1.0975,  0.0000,  1.4248,  0.0000, -0.1104,\n           1.1865]),\n  tensor(8)),\n (tensor([ 1.1462, -1.8379, -0.2368,  2.0488,  0.0000,  0.0000, -0.3927, -1.0523,\n           0.1294]),\n  tensor(4)))\n\n\n\nx = torch.randn((4))\nx_aug = x.clone()\nmask = torch.tensor([True,True,False,False])\n\nx_aug[mask] = 1\nx_aug, x\n\n(tensor([ 1.0000,  1.0000,  1.9390, -0.0338]),\n tensor([-1.7385, -0.5780,  1.9390, -0.0338]))\n\n\n\nsource\n\n\nPatchAugmentations\n\ndef PatchAugmentations(\n    augmentations:list=['patch_mask', 'jitter_zero_mask', 'reverse_sequence', 'shuffle_channels'],\n    patch_mask_ratio:float=0.0, jitter_zero_mask_ratio:float=0.0\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nx = torch.randn(4,3600,7,750)\n\ns=PatchAugmentations(patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\ns(x).shape\n\ntorch.Size([4, 3600, 7, 750])\n\n\n\nsource\n\n\nEmbeddingAugmentations\n\ndef EmbeddingAugmentations(\n    augmentations:list=['shuffle_dims', 'jitter_zero_mask', 'patch_mask'], dims_to_shuffle:list=[1, 2, 3],\n    patch_mask_ratio:float=0.0, jitter_zero_mask_ratio:float=0.0\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nx = torch.randn(4,7,512,3600)\n\ns = EmbeddingAugmentations(augmentations=['jitter_zero_mask'], dims_to_shuffle=[1], patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\ns(x).shape\n\ntorch.Size([4, 7, 512, 3600])",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#patch-and-fourier-layers",
    "href": "layers.html#patch-and-fourier-layers",
    "title": "Layers",
    "section": "Patch and Fourier Layers",
    "text": "Patch and Fourier Layers\n\nsource\n\nPatch\n\ndef Patch(\n    patch_len, stride, max_seq_len:NoneType=None\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nSTFT\n\ndef STFT(\n    n_fft, win_length, hop_length, stft_norm, decibel_scale, channel_stft_means:NoneType=None,\n    channel_stft_stds:NoneType=None, pad_win_length_to_nfft:bool=True, pad_mode:str='reflect', center:bool=False,\n    return_complex:bool=True\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nFFT\n\ndef FFT(\n    dim:int=-1, # dimension to calculate fft over\n    norm:str='backward', # \"forward\" - normalize by 1/n, \"backward\" - no normalization, \"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#reversible-instance-normalization",
    "href": "layers.html#reversible-instance-normalization",
    "title": "Layers",
    "section": "Reversible Instance Normalization",
    "text": "Reversible Instance Normalization\n\nsource\n\nRevIN\n\ndef RevIN(\n    num_features:int, # the number of channels or features in the input\n    eps:float=1e-05, # added to avoid division by zero errors\n    dim_to_reduce:int=-1, # the dimension to reduce,\n    affine:bool=True, # learning affine parameters bias and weight per channel\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nx = torch.randn(4,7,1000)\n\nrevin = RevIN(x.shape[1], dim_to_reduce=-1, affine=True)\nx_norm = revin(x, mode=True)\nx_denorm = revin(x_norm, mode=False)\n\nx.shape, x_norm.shape, x_denorm.shape, revin.mean.shape, revin.stdev.shape\n\n(torch.Size([4, 7, 1000]),\n torch.Size([4, 7, 1000]),\n torch.Size([4, 7, 1000]),\n torch.Size([4, 7, 1]),\n torch.Size([4, 7, 1]))",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#attention",
    "href": "layers.html#attention",
    "title": "Layers",
    "section": "Attention",
    "text": "Attention\n\nsource\n\nMultiheadFlashAttention\n\ndef MultiheadFlashAttention(\n    d_model:int, n_heads:int, qkv_bias:bool=True, is_causal:bool=False, attn_dropout:float=0.0,\n    proj_dropout:float=0.0\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nmha = MultiheadFlashAttention(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0.)\nx = torch.randn(2*7,100,512) # [bs * n_vars x n_patches x d_model]\nkey_padding_mask = torch.zeros(2*7, 100, dtype=torch.bool)\nkey_padding_mask[:, -2:] = True  # mask last 2 positions\noutput = mha(x, key_padding_mask=key_padding_mask)\noutput.shape\n\ntorch.Size([14, 100, 512])\n\n\n\nsource\n\n\nScaledDotProductAttention\n\ndef ScaledDotProductAttention(\n    d_model, n_heads, attn_dropout:float=0.0, res_attention:bool=False, lsa:bool=False\n):\n\nScaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets by Lee et al, 2021)\n\nsource\n\n\nMultiheadAttentionCustom\n\ndef MultiheadAttentionCustom(\n    d_model, n_heads, d_k:NoneType=None, d_v:NoneType=None, res_attention:bool=False, attn_dropout:float=0.0,\n    proj_dropout:float=0.0, qkv_bias:bool=True, lsa:bool=False\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nmha_attn = MultiheadAttentionCustom(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0., res_attention=False)\nmha_attn\n\nMultiheadAttentionCustom(\n  (W_Q): Linear(in_features=512, out_features=512, bias=True)\n  (W_K): Linear(in_features=512, out_features=512, bias=True)\n  (W_V): Linear(in_features=512, out_features=512, bias=True)\n  (sdp_attn): ScaledDotProductAttention(\n    (attn_dropout): Dropout(p=0.0, inplace=False)\n  )\n  (to_out): Sequential(\n    (0): Linear(in_features=512, out_features=512, bias=True)\n    (1): Dropout(p=0.0, inplace=False)\n  )\n)\n\n\n\ndef test_attention_equivalence():\n    # Set random seed for reproducibility\n    torch.manual_seed(42)\n    \n    # Test parameters\n    batch_size = 2\n    seq_len = 10\n    d_model = 64\n    n_heads = 4\n    \n    # Create input tensor (only need one since we're using self-attention)\n    x = torch.randn(batch_size, seq_len, d_model)\n    \n    # Create key padding mask\n    key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)\n    key_padding_mask[:, -2:] = True  # mask last 2 positions\n\n    # Initialize both implementations\n    custom_mha = MultiheadAttentionCustom(d_model=d_model, n_heads=n_heads)\n    flash_mha = MultiheadFlashAttention(d_model=d_model, n_heads=n_heads)\n    \n    # Set both models to eval mode to disable dropout\n    custom_mha.eval()\n    flash_mha.eval()\n    \n    # Copy weights to ensure identical parameters\n    # Combine QKV weights from custom implementation into single matrix for flash attention\n    combined_weight = torch.cat([\n        custom_mha.W_Q.weight,\n        custom_mha.W_K.weight,\n        custom_mha.W_V.weight\n    ], dim=0)\n    combined_bias = torch.cat([\n        custom_mha.W_Q.bias,\n        custom_mha.W_K.bias,\n        custom_mha.W_V.bias\n    ], dim=0)\n    \n    # Copy combined weights to flash attention\n    flash_mha.c_attn.weight.data = combined_weight\n    flash_mha.c_attn.bias.data = combined_bias\n    \n    # Output projection weights\n    flash_mha.c_proj.weight.data = custom_mha.to_out[0].weight.data.clone()\n    flash_mha.c_proj.bias.data = custom_mha.to_out[0].bias.data.clone()\n    \n    # Forward pass\n    with torch.no_grad():\n        custom_output, custom_attn = custom_mha(x, key_padding_mask=key_padding_mask)\n        \n        flash_output = flash_mha(x, attn_mask=key_padding_mask)\n    \n    # Compare outputs\n    print(f\"Custom output shape: {custom_output.shape}\")\n    print(f\"Flash output shape: {flash_output.shape}\")\n    \n    output_close = torch.allclose(custom_output, flash_output, rtol=0, atol=0)\n    print(f\"Outputs match: {output_close}\")\n    \n    if not output_close:\n        print(\"\\nOutput differences:\")\n        print(f\"Max difference: {(custom_output - flash_output).abs().max().item()}\")\n        print(f\"Mean difference: {(custom_output - flash_output).abs().mean().item()}\")\n    \n    return custom_output, flash_output\n\ncustom_output, flash_output = test_attention_equivalence()\n#: 8.940696716308594e-08\n#Mean difference: 1.0550138540565968e-08\n\nCustom output shape: torch.Size([2, 10, 64])\nFlash output shape: torch.Size([2, 10, 64])\nOutputs match: True\n\n\n\nd_model=512\nn_heads=8\nd_k = d_v = d_model // n_heads\nattn = ScaledDotProductAttention(d_model=d_model, n_heads=n_heads)\nmha_attn = MultiheadAttentionCustom(d_model, n_heads)\n\nW_Q = nn.Linear(d_model, d_k * n_heads)\nW_K = nn.Linear(d_model, d_k * n_heads)\nW_V = nn.Linear(d_model, d_v * n_heads)\nX,_,_ = ds[0]\n\nX = create_patch(X, patch_len=(10*50), stride=(5*50), constant_pad=True)\n\npatch_len = X.shape[-1]\n\nX = X[None, ...].permute(0,2,1,3)  # simulate batch size of 1 [bs x n_vars x num_patch x patch_len]\n\nprint(f'X input shape: {X.shape}')\nW_P = nn.Linear(patch_len, d_model)\n\nX = W_P(X) # project to d_model\nprint(f\"Projected X shape to d_model: {X.shape}\")\n\nX = torch.reshape(X, (X.shape[0]*X.shape[1],X.shape[2],X.shape[3]))\nprint(f\"Reshape for attention: {X.shape}\")\n\n# test multihead attention\nprint(\"\\nTesting MHA and SDA attention, with just 50 elements.\")\nmha_output, mha_attn_weights = mha_attn(Q=X[:,:50,:])\nprint(f\"MHA attention output shape: {mha_output.shape}, mha attn weight shape: {mha_attn_weights.shape}\")\n\n# test scaled dot product attn\nK = Q = V = X\n\n# # Linear (+ split in multiple heads)\nbs = 1 # 1 * 16\nq_s = W_Q(Q).reshape(bs, -1, n_heads, d_k).transpose(1, 2)\nk_s = W_K(K).reshape(bs, -1, n_heads, d_k).permute(0, 2, 3, 1)\nv_s = W_V(V).reshape(bs, -1, n_heads, d_v).transpose(1, 2)\nprint(f\"Q shape: {q_s.shape}, K shape: {k_s.shape}, V shape: {v_s.shape}\")\n\nto_out = nn.Linear(n_heads * d_v, d_model)\noutput, attn_weights = attn(q_s[:,:,:50,:],k_s[:,:,:,:50], v_s[:,:,:50,:])\noutput = output.transpose(1, 2).contiguous().view(bs, -1, n_heads * d_v)\nprint(f\"Attn output shape {output.shape}, attn weight shape: {attn_weights.shape}\")\n\nX input shape: torch.Size([1, 7, 10799, 500])\nProjected X shape to d_model: torch.Size([1, 7, 10799, 512])\nReshape for attention: torch.Size([7, 10799, 512])\n\nTesting MHA and SDA attention, with just 50 elements.\nMHA attention output shape: torch.Size([7, 50, 512]), mha attn weight shape: torch.Size([7, 8, 50, 50])\nQ shape: torch.Size([1, 8, 75593, 64]), K shape: torch.Size([1, 8, 64, 75593]), V shape: torch.Size([1, 8, 75593, 64])\nAttn output shape torch.Size([1, 50, 512]), attn weight shape: torch.Size([1, 8, 50, 50])\n\n\n\nsource\n\n\nAttention_Rel_Scl\n\ndef Attention_Rel_Scl(\n    d_model:int, # Embedding dimension\n    n_heads:int, # number of attention heads\n    seq_len:int, # sequence length or num patches\n    d_k:int=None, # key dimension\n    d_v:int=None, # value dimension\n    res_attention:bool=False, # whether to use residual attention\n    attn_dropout:float=0.0, # dropout for attention\n    lsa:bool=False, # whether to use LSA, trainable paramater for scaling\n    proj_dropout:float=0.0, # dropout for projection\n    qkv_bias:bool=True, # bias for q, k, v\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nd_model=16\nc_in = 2\nseq_len = 1*360*10\nx = torch.randn(4,c_in,seq_len)\nembed_layer = nn.Sequential(nn.Conv2d(1, d_model*4, kernel_size=[1, 7], padding='same'), nn.BatchNorm2d(d_model*4), nn.GELU())\nembed_layer2 = nn.Sequential(nn.Conv2d(d_model*4, d_model, kernel_size=[c_in, 1], padding='valid'), nn.BatchNorm2d(d_model), nn.GELU())\nabs_position = tAPE(d_model, seq_len=seq_len)\nx_emb = embed_layer2(embed_layer(x.unsqueeze(1))).squeeze(2)\nx_emb = x_emb.permute(0,2,1)\nx_emb_pos = abs_position(x_emb)\n\nmodel = Attention_Rel_Scl(d_model=d_model,\n        n_heads=2, # number of attention heads\n        seq_len=seq_len, # sequence length or num patches\n        )\n\nout, attn_weights = model(x_emb)\n\n\n## test w patches [bs *c_in x num_patches x d_model]\nd_model=512\nc_in = 2\nnum_patches = 10\nx_emb = torch.randn(4*c_in,num_patches, d_model)\nabs_position = tAPE(d_model, seq_len=num_patches)\nx_emb_pos = abs_position(x_emb)\n\nmodel = Attention_Rel_Scl(d_model=d_model,\n        n_heads=2, # number of attention heads\n        seq_len=num_patches, # sequence length or num patches\n        )\n\nout, attn_weights = model(x_emb)",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#pretraining-heads",
    "href": "layers.html#pretraining-heads",
    "title": "Layers",
    "section": "Pretraining Heads",
    "text": "Pretraining Heads\n\nsource\n\nMaskedAutogressionFeedForward2\n\ndef MaskedAutogressionFeedForward2(\n    d_model, patch_len, n_layers, dropout\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMaskedAutogressionFeedForward\n\ndef MaskedAutogressionFeedForward(\n    c_in, # the number of input channels\n    patch_len, # the length of the patches (either stft or interval length)\n    d_model, # the dimension of the initial linear layers for inputting patches into transformer\n    shared_recreation:bool=True, # indicator of whether to project each channel individually or together\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "layers.html#miscellaneous",
    "href": "layers.html#miscellaneous",
    "title": "Layers",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nsource\n\nIdentity\n\ndef Identity(\n    \n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nget_activation_fn\n\ndef get_activation_fn(\n    activation\n):\n\n\nsource\n\n\nTranspose\n\ndef Transpose(\n    dims:VAR_POSITIONAL, contiguous:bool=False\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "Layers"
    ]
  },
  {
    "objectID": "heads.html",
    "href": "heads.html",
    "title": "SSL, Fine Tuning, and Linear Probing Heads",
    "section": "",
    "text": "source\n\n\n\ndef RNNProbingHead(\n    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, module:str='GRU', linear_dropout:float=0.0,\n    rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n    pool:str='average', # 'average' or 'max' or 'majority'\n    temperature:float=2.0, # only used if pool='majority'\n    n_linear_layers:int=1, predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False,\n    shared_embedding:bool=True, augmentations:NoneType=None, augmentation_mask_ratio:float=0.0,\n    augmentation_dims_to_shuffle:list=[1, 2, 3], norm:NoneType=None, # one of [None, 'pre', 'post']\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\ndef RNNProbingHeadExperimental(\n    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, # deprecated\n    module:str='GRU', linear_dropout:float=0.0, rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n    pool:str='average', # 'average' or 'max' or 'majority'\n    temperature:float=2.0, # only used if pool='majority'\n    predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False, augmentations:NoneType=None,\n    augmentation_mask_ratio:float=0.0, augmentation_dims_to_shuffle:list=[1, 2, 3],\n    pre_norm:bool=True, # one of [None, 'pre', 'post']\n    mlp_final_head:bool=False\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm = RNNProbingHeadExperimental(c_in=7, \n                                pool='average', \n                                input_size = 384, \n                                bidirectional=True,\n                                affine=False, \n                                hidden_size=1200,\n                                module='GRU',\n                                n_classes=4,\n                                predict_every_n_patches=32,\n                                rnn_dropout=0.,\n                                num_rnn_layers=1,\n                                linear_dropout=0.,\n                                mlp_final_head=False,\n                                pre_norm=True)\nx = torch.randn((4,7,384,960))\nsequence_padding_mask = torch.zeros(4,960)\nsequence_padding_mask[:,-32:] = 1\nm(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\nx = torch.randn((4,7,384,960))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\nx = torch.randn((4,7,512*2,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\ndef TransformerDecoderProbingHead(\n    c_in, d_model, n_classes, norm:str='BatchNorm', dropout:float=0.0, act:str='gelu', d_ff:int=2048,\n    num_layers:int=1, n_heads:int=2, predict_every_n_patches:int=1, affine:bool=False, shared_embedding:bool=True\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nlayer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\nx = torch.randn((4, 7, 512, 3600))\n\nlayer(x).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\ndef DecoderFeedForward(\n    c_in, # the number of input channels\n    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n    num_layers, d_ff, attn_dropout, res_attention, pre_norm, store_attn, n_heads, shared_embedding, affine,\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    d_model, # the dimension of the transformer model\n    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n    dropout:float=0.0, # dropout in between linear layers\n):\n\ntransformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.\n\nc_in = 7\nfrequency = 125\nwin_length=750 \noverlap = 0.\nhop_length=win_length - int(overlap*win_length)\nmax_seq_len_sec = (6*3600) # for dataloader\n#seq_len_sec = sample_stride = 3*3600 # for dataloader\nmax_seq_len = max_seq_len_sec*frequency # for model\n#n_patches = n_fft // 2 + 1\nn_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n\n#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\nx = torch.randn(2,c_in,512,n_patches)\n\nmodel = DecoderFeedForward(c_in=c_in,\n                           predict_every_n_patches=5,\n                           num_layers=1,\n                           d_ff = 2048,\n                           attn_dropout=0.,\n                           res_attention = False,\n                           pre_norm = False,\n                           store_attn = False,\n                           n_heads=2,\n                           affine=False,\n                           shared_embedding=False,\n                           n_classes=5,\n                           d_model=512,\n                           norm='BatchNorm',\n                           act='gelu',\n                           dropout=0.\n                           )\n\nmodel(x).shape\n\ntorch.Size([2, 5, 720])\n\n\n\nsource\n\n\n\n\ndef TimeDistributedConvolutionalFeedForward(\n    c_in, # the number of input channels\n    frequency, # the frequency of the original channels\n    predict_every_seconds, # for a given sequence of length m with frequency f, number of predictions\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    win_length, # the convolved patch length, the first step in this is to do a linear layer to this dimension\n    d_model, # the dimension of the transformer model\n    affine:bool=False, shared_embedding:bool=True\n):\n\nConvolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension. Then, a convolutional transpose is used to extrapolate the data to its original form. Finally, a final convolution is used to predict the classes.\n\nsource\n\n\n\n\ndef LinearProbingHead(\n    c_in, # the number of input channels in the original input\n    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    input_size, # the dimension of the transformer model\n    n_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n    num_patch,\n    shared_embedding:bool=True, # whether or not to have a dense layer per channel or one layer per channel\n    affine:bool=True, # include learnable parameters to weight predictions\n    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n    dropout:float=0.0, # dropout in between linear layers\n):\n\nA linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results\n\nm = LinearProbingHead(c_in=7, \n                      input_size = 512, \n                      predict_every_n_patches=5,\n                      n_classes=5,\n                      n_layers=3,\n                      shared_embedding=True,\n                      affine=True,\n                      num_patch=3600,\n                      dropout=0.1)\n\nx = torch.randn((4,7,512,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\ndef TimeDistributedFeedForward(\n    c_in, # the number of input channels\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    n_patches, # the number of stft or time patches\n    d_model, # the dimension of the transformer model\n    pred_len_seconds, # the sequence multiclass prediction length in seconds\n    n_linear_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n    conv_kernel_stride_size, # the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n    dropout:float=0.0, # dropout in between linear layers\n):\n\nFeed forward head that uses a convolutional layer to reduce channel dimensionality Followed by a feedforward network to make\n\nsource\n\n\n\n\ndef ConvBiGRU(\n    input_size, hidden_sizes, kernel_sizes, n_layers, d_model, predict_every_n_patches, n_classes\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\ndef ConvGRU1D(\n    input_size, hidden_sizes, # if integer, the same hidden size is used for all cells.\n    kernel_sizes, # if integer, the same kernel size is used for all cells.\n    n_layers\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\ndef ConvGRU1DCell(\n    input_size, hidden_size, kernel_size\n):\n\nGenerate a convolutional GRU cell\n\nx = torch.randn((4,7,512,3600))\n\nconvgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n\nout = convgru(x)\nout.shape\n\ntorch.Size([4, 5, 720])",
    "crumbs": [
      "SSL, Fine Tuning, and Linear Probing Heads"
    ]
  },
  {
    "objectID": "heads.html#linear-probing-and-fine-tuning-heads",
    "href": "heads.html#linear-probing-and-fine-tuning-heads",
    "title": "SSL, Fine Tuning, and Linear Probing Heads",
    "section": "",
    "text": "source\n\n\n\ndef RNNProbingHead(\n    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, module:str='GRU', linear_dropout:float=0.0,\n    rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n    pool:str='average', # 'average' or 'max' or 'majority'\n    temperature:float=2.0, # only used if pool='majority'\n    n_linear_layers:int=1, predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False,\n    shared_embedding:bool=True, augmentations:NoneType=None, augmentation_mask_ratio:float=0.0,\n    augmentation_dims_to_shuffle:list=[1, 2, 3], norm:NoneType=None, # one of [None, 'pre', 'post']\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\ndef RNNProbingHeadExperimental(\n    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, # deprecated\n    module:str='GRU', linear_dropout:float=0.0, rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n    pool:str='average', # 'average' or 'max' or 'majority'\n    temperature:float=2.0, # only used if pool='majority'\n    predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False, augmentations:NoneType=None,\n    augmentation_mask_ratio:float=0.0, augmentation_dims_to_shuffle:list=[1, 2, 3],\n    pre_norm:bool=True, # one of [None, 'pre', 'post']\n    mlp_final_head:bool=False\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nm = RNNProbingHeadExperimental(c_in=7, \n                                pool='average', \n                                input_size = 384, \n                                bidirectional=True,\n                                affine=False, \n                                hidden_size=1200,\n                                module='GRU',\n                                n_classes=4,\n                                predict_every_n_patches=32,\n                                rnn_dropout=0.,\n                                num_rnn_layers=1,\n                                linear_dropout=0.,\n                                mlp_final_head=False,\n                                pre_norm=True)\nx = torch.randn((4,7,384,960))\nsequence_padding_mask = torch.zeros(4,960)\nsequence_padding_mask[:,-32:] = 1\nm(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\nx = torch.randn((4,7,384,960))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 4, 30])\n\n\n\nm = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\nx = torch.randn((4,7,512*2,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\ndef TransformerDecoderProbingHead(\n    c_in, d_model, n_classes, norm:str='BatchNorm', dropout:float=0.0, act:str='gelu', d_ff:int=2048,\n    num_layers:int=1, n_heads:int=2, predict_every_n_patches:int=1, affine:bool=False, shared_embedding:bool=True\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nlayer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\nx = torch.randn((4, 7, 512, 3600))\n\nlayer(x).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\ndef DecoderFeedForward(\n    c_in, # the number of input channels\n    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n    num_layers, d_ff, attn_dropout, res_attention, pre_norm, store_attn, n_heads, shared_embedding, affine,\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    d_model, # the dimension of the transformer model\n    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n    dropout:float=0.0, # dropout in between linear layers\n):\n\ntransformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.\n\nc_in = 7\nfrequency = 125\nwin_length=750 \noverlap = 0.\nhop_length=win_length - int(overlap*win_length)\nmax_seq_len_sec = (6*3600) # for dataloader\n#seq_len_sec = sample_stride = 3*3600 # for dataloader\nmax_seq_len = max_seq_len_sec*frequency # for model\n#n_patches = n_fft // 2 + 1\nn_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n\n#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\nx = torch.randn(2,c_in,512,n_patches)\n\nmodel = DecoderFeedForward(c_in=c_in,\n                           predict_every_n_patches=5,\n                           num_layers=1,\n                           d_ff = 2048,\n                           attn_dropout=0.,\n                           res_attention = False,\n                           pre_norm = False,\n                           store_attn = False,\n                           n_heads=2,\n                           affine=False,\n                           shared_embedding=False,\n                           n_classes=5,\n                           d_model=512,\n                           norm='BatchNorm',\n                           act='gelu',\n                           dropout=0.\n                           )\n\nmodel(x).shape\n\ntorch.Size([2, 5, 720])\n\n\n\nsource\n\n\n\n\ndef TimeDistributedConvolutionalFeedForward(\n    c_in, # the number of input channels\n    frequency, # the frequency of the original channels\n    predict_every_seconds, # for a given sequence of length m with frequency f, number of predictions\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    win_length, # the convolved patch length, the first step in this is to do a linear layer to this dimension\n    d_model, # the dimension of the transformer model\n    affine:bool=False, shared_embedding:bool=True\n):\n\nConvolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension. Then, a convolutional transpose is used to extrapolate the data to its original form. Finally, a final convolution is used to predict the classes.\n\nsource\n\n\n\n\ndef LinearProbingHead(\n    c_in, # the number of input channels in the original input\n    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    input_size, # the dimension of the transformer model\n    n_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n    num_patch,\n    shared_embedding:bool=True, # whether or not to have a dense layer per channel or one layer per channel\n    affine:bool=True, # include learnable parameters to weight predictions\n    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n    dropout:float=0.0, # dropout in between linear layers\n):\n\nA linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results\n\nm = LinearProbingHead(c_in=7, \n                      input_size = 512, \n                      predict_every_n_patches=5,\n                      n_classes=5,\n                      n_layers=3,\n                      shared_embedding=True,\n                      affine=True,\n                      num_patch=3600,\n                      dropout=0.1)\n\nx = torch.randn((4,7,512,3600))\n\nm(x, return_softmax=True).shape\n\ntorch.Size([4, 5, 720])\n\n\n\nsource\n\n\n\n\ndef TimeDistributedFeedForward(\n    c_in, # the number of input channels\n    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n    n_patches, # the number of stft or time patches\n    d_model, # the dimension of the transformer model\n    pred_len_seconds, # the sequence multiclass prediction length in seconds\n    n_linear_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n    conv_kernel_stride_size, # the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n    dropout:float=0.0, # dropout in between linear layers\n):\n\nFeed forward head that uses a convolutional layer to reduce channel dimensionality Followed by a feedforward network to make\n\nsource\n\n\n\n\ndef ConvBiGRU(\n    input_size, hidden_sizes, kernel_sizes, n_layers, d_model, predict_every_n_patches, n_classes\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\ndef ConvGRU1D(\n    input_size, hidden_sizes, # if integer, the same hidden size is used for all cells.\n    kernel_sizes, # if integer, the same kernel size is used for all cells.\n    n_layers\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\ndef ConvGRU1DCell(\n    input_size, hidden_size, kernel_size\n):\n\nGenerate a convolutional GRU cell\n\nx = torch.randn((4,7,512,3600))\n\nconvgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n\nout = convgru(x)\nout.shape\n\ntorch.Size([4, 5, 720])",
    "crumbs": [
      "SSL, Fine Tuning, and Linear Probing Heads"
    ]
  },
  {
    "objectID": "augmentations.html",
    "href": "augmentations.html",
    "title": "Augmentations",
    "section": "",
    "text": "source\n\n\n\ndef create_patch(\n    xb, patch_len, stride, return_patch_num:bool=False, constant_pad:bool=False, constant_pad_value:int=0,\n    max_seq_len:NoneType=None\n):\n\nxb: [bs x n_vars x seq_len]\n\nx = torch.randn(4, 1000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=505, stride=500, constant_pad=False)\nxb_rep = create_patch(x, patch_len=500, stride=500, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)b\n\n(torch.Size([4, 1000]), torch.Size([4, 1, 505]), torch.Size([4, 2, 500]))\n\n\n\nx = torch.randn(1,7,1350000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=1024, stride=1024, constant_pad=False)\nxb_rep = create_patch(x, patch_len=1024, stride=1024, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)\n\n(torch.Size([1, 7, 1350000]),\n torch.Size([1, 1318, 7, 1024]),\n torch.Size([1, 1319, 7, 1024]))\n\n\n\nsource\n\n\n\n\ndef unpatch(\n    x, seq_len, remove_padding:bool=True\n):\n\nx: [bs/None x patch_num x n_vars x patch_len] returns x: [bs x n_vars x seq_len]\n\nx = torch.randn(1,1,50)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=6, stride=6, constant_pad=True)\nxb = unpatch(xb, seq_len=50, remove_padding=False)\nxb.shape\n\n(torch.Size([1, 1, 50]), torch.Size([1, 9, 1, 6]))",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#patching",
    "href": "augmentations.html#patching",
    "title": "Augmentations",
    "section": "",
    "text": "source\n\n\n\ndef create_patch(\n    xb, patch_len, stride, return_patch_num:bool=False, constant_pad:bool=False, constant_pad_value:int=0,\n    max_seq_len:NoneType=None\n):\n\nxb: [bs x n_vars x seq_len]\n\nx = torch.randn(4, 1000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=505, stride=500, constant_pad=False)\nxb_rep = create_patch(x, patch_len=500, stride=500, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)b\n\n(torch.Size([4, 1000]), torch.Size([4, 1, 505]), torch.Size([4, 2, 500]))\n\n\n\nx = torch.randn(1,7,1350000)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=1024, stride=1024, constant_pad=False)\nxb_rep = create_patch(x, patch_len=1024, stride=1024, constant_pad=True)\nx.shape, xb.shape, xb_rep.shape\n#xb_rep_short = create_patch(x_short, patch_en=502, stride=500, replication_pad=False)\n\n(torch.Size([1, 7, 1350000]),\n torch.Size([1, 1318, 7, 1024]),\n torch.Size([1, 1319, 7, 1024]))\n\n\n\nsource\n\n\n\n\ndef unpatch(\n    x, seq_len, remove_padding:bool=True\n):\n\nx: [bs/None x patch_num x n_vars x patch_len] returns x: [bs x n_vars x seq_len]\n\nx = torch.randn(1,1,50)\n\n# test seq_len &gt; patch len == stride \nxb = create_patch(x, patch_len=6, stride=6, constant_pad=True)\nxb = unpatch(xb, seq_len=50, remove_padding=False)\nxb.shape\n\n(torch.Size([1, 1, 50]), torch.Size([1, 9, 1, 6]))",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#patch-masking",
    "href": "augmentations.html#patch-masking",
    "title": "Augmentations",
    "section": "Patch Masking",
    "text": "Patch Masking\n\nsource\n\nrandom_masking\n\ndef random_masking(\n    xb, mask_ratio\n):\n\n\nsource\n\n\nmask_patches_simple\n\ndef mask_patches_simple(\n    xb, # input tensor of size 3 or 4 to be masked\n    mask_ratio, # ratio of masking of patches\n):\n\nFunction that masks patches in a simple way\nxb: [bs x patch_num x n_vars x patch_len] padding_mask [bs x patch_num x 1|num_vars x patch_len]\n\nx = torch.randn(50,16,7,50)\nmask_ratio = 0.4\n\nx_new, mask = mask_patches_simple(x,mask_ratio=mask_ratio)",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#value-augmentations",
    "href": "augmentations.html#value-augmentations",
    "title": "Augmentations",
    "section": "Value Augmentations",
    "text": "Value Augmentations\n\nsource\n\njitter_augmentation\n\ndef jitter_augmentation(\n    x, mask_ratio:float=0.05, jitter_ratio:float=0.05\n):\n\n\nsource\n\n\nremove_values\n\ndef remove_values(\n    x, mask_ratio\n):\n\n\n## note that the random number generator advances state...\ntorch.manual_seed(42)\nx = torch.randn(4,7,1000)\n\ntorch.manual_seed(42)\nx_new, n_masks = jitter_augmentation(x)\nn_masks /(4* 7*1000)\n\ntorch.manual_seed(42)\nx_new2, n_masks2 = jitter_augmentation(x)\ntorch.equal(x_new, x_new2)\n\nTrue",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#shuffle-augmentations",
    "href": "augmentations.html#shuffle-augmentations",
    "title": "Augmentations",
    "section": "Shuffle Augmentations",
    "text": "Shuffle Augmentations\n\nsource\n\nshuffle_dim\n\ndef shuffle_dim(\n    x, dim:int=1, p:float=0.5\n):\n\nshuffles a dimension randomly along dim x: [bs x n channels x n patches x patch len]\n\nsource\n\n\nreverse_sequence\n\ndef reverse_sequence(\n    x, seq_dim:tuple=(-1,), p:float=0.5\n):\n\n\nx = torch.randn(4,1,5,5).to('cuda')\n\ntorch.equal(shuffle_dim(x), x)\n\nFalse",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "augmentations.html#callbacks",
    "href": "augmentations.html#callbacks",
    "title": "Augmentations",
    "section": "Callbacks",
    "text": "Callbacks\n\nsource\n\nIntraClassCutMix1d\n\ndef IntraClassCutMix1d(\n    mix_prob:float=0.5, # probability of applying cutmix\n    return_y_every_sec:int=30, # length of segment to mix, if one value of y corresponds to 30 seconds of signal data, this should be set to 30.\n    frequency:int=125, # frequency of the data\n    return_sequence_padding_mask:bool=True, # whether to return the sequence padding mask\n):\n\nIntra-class CutMix for 1D data (e.g., time-series). \nThis is a callback that can be used to apply CutMix to the training data. It is used to mix segments within the same class.\n\nx = torch.randn(4,7,90)\nx_c = x.clone()\ny = torch.randint(0, 5, size=(4,90//30))\nxxt = IntraClassCutMix1d(mix_prob=1, frequency=1, return_y_every_sec=30, return_sequence_padding_mask=False)\nbatch = (x,y)\nxxt.on_train_batch_start(None, None, batch, 0)\ntorch.equal(x_c, batch[0]) == False\n\nTrue\n\n\n\nsource\n\n\nIntraClassCutMixBatch\n\ndef IntraClassCutMixBatch(\n    mix_prob:float=0.5, # probability of applying cutmix\n    return_y_every_sec:int=30, # length of segment to mix, if one value of y corresponds to 30 seconds of signal data, this should be set to 30.\n    frequency:int=125, # frequency of the data\n    return_sequence_padding_mask:bool=True, # whether to return the sequence padding mask\n    intra_class_only:bool=True, # whether to mix only within same class (True) or across all classes (False)\n):\n\nIntra-class CutMix for 1D data (e.g., time-series). \nThis is a callback that can be used to apply CutMix to the training data. It is used to mix segments within the same class.\nThis is different to IntraClassCutMix1d in that it mixes segments of the same class across batches of data, rather than just at the same segment\n\nx = torch.randn(4,7,90)\nx_c = x.clone()\ny = torch.randint(0, 5, size=(4,90//30))\nxxt = IntraClassCutMixBatch(mix_prob=1, frequency=1, return_y_every_sec=30, return_sequence_padding_mask=False)\nbatch = (x,y)\nbatch = xxt.on_train_batch_start(None, None, batch, 0)\ntorch.equal(x_c, batch[0]) == False\n\nintra-class CutMixBatch is being applied!\n\n\nTrue\n\n\n\n# Create a tuple\nbatch = ([1,2,3], [4,5,6])\n\n# Unpack into new variables\nx, y = batch\n\n# Modify x and y\nx[0] = 99  # This modifies the list because lists are mutable\ny[0] = 88  # This modifies the list because lists are mutable\n\nprint(batch)  # Will show ([99,2,3], [88,5,6]) because lists are mutable\n\n([99, 2, 3], [88, 5, 6])\n\n\n\nsource\n\n\nMixupCallback\n\ndef MixupCallback(\n    num_classes, mixup_alpha:float=0.4, # alpha parameter for the beta distribution\n    return_sequence_padding_mask:bool=True, # whether to return the sequence padding mask\n    ignore_index:int=-100, # ignore index\n):\n\nMixup for 1D data (e.g., time-series).\nThis callback applies Mixup to the training data, blending both the input data and the labels.\nSee tsai implementation here: https://github.com/timeseriesAI/tsai/blob/bdff96cc8c4c8ea55bc20d7cffd6a72e402f4cb2/tsai/data/mixed_augmentation.py#L43\nNote that this creates non-integer labels/soft labels. Loss functions should be able to handle this.\n\nx = torch.randn(4,7,90)\nx_c = x.clone()\ny_og = torch.randint(0, 5, size=(4,90//30))\ny_og[1,2] = -100\ny_og[2,1] = -100\ny_c = y_og.clone()\nxxt = MixupCallback(num_classes=5, mixup_alpha=0.4, return_sequence_padding_mask=False)\nbatch = (x,y_og)\nbatch = xxt.on_train_batch_start(None, None, batch, 0)\ntorch.equal(x_c, batch[0]) == False, torch.equal(y_c, batch[1]) == False\n\nMixup is being applied!\n\n\n(True, True)",
    "crumbs": [
      "Augmentations"
    ]
  },
  {
    "objectID": "slumber.html#edf-helpers",
    "href": "slumber.html#edf-helpers",
    "title": "Slumber",
    "section": "EDF Helpers",
    "text": "EDF Helpers\n\nRead EDFs\n\nsource\n\n\nread_edf\n\ndef read_edf(\n    file_path, # file path of edf\n    channels:NoneType=None, # channels in edf to read, will raise warning if channels do not exist\n    frequency:NoneType=None, # frequency to resample all signals to\n):\n\nFunction to read an edf file and return a list of signals and header with the option to resample to a passed frequency\n\nsource\n\n\nread_edf_mne\n\ndef read_edf_mne(\n    file_path, # file path of edf\n    channels:NoneType=None, # channels in edf to read, will raise warning if channels do not exist\n    frequency:NoneType=None, # frequency to resample all signals to\n)-&gt;Union: # tuple of signals and header dictionary\n\nfunction to read edf with mne library i dont recommend using this. Use edfio instead.\n\nsource\n\n\nread_edf_edfio\n\ndef read_edf_edfio(\n    file_path, # file path of edf\n    channels:NoneType=None, # channels in edf to read, will raise warning if channels do not exist\n    frequency:NoneType=None, # frequency to resample all signals to\n)-&gt;Union: # tuple of signals and header dictionary\n\nfunction to read edfs with edfio\n\n\nRead Hypnograms\n\nsource\n\n\nread_hypnogram\n\ndef read_hypnogram(\n    file, # file path of the hypnogram csv\n    epoch_length:NoneType=None, # epoch length of the hypnogram measurements, if passed will repeat this many times at each element\n)-&gt;array: # numpy array of hypnogram\n\nFunction that reads a hypnogram csv and returns a numpy array of the hypnogram with optional repeats\n\n\nEDFs to Zarr\n\nsource\n\n\nedf_signals_to_zarr\n\ndef edf_signals_to_zarr(\n    edf_file_path, write_data_dir, overwrite:bool=False, channels:NoneType=None, channel_name_map:NoneType=None,\n    frequency:NoneType=None, hyp_epoch_length:int=30, hyp_data_dir:NoneType=None\n):\n\nFunction that converts an edf to a zarr file\ntry_mne: tries to load files with mne instead of pyedflib (if there is an error). This seems dangerous as mne converts units (and potentially resamples, while pyedflib does not)",
    "crumbs": [
      "Slumber"
    ]
  },
  {
    "objectID": "slumber.html#datasets",
    "href": "slumber.html#datasets",
    "title": "Slumber",
    "section": "Datasets",
    "text": "Datasets\n\nSelf Supervised Dataset\n\nsource\n\n\ntrim_wake_epochs_from_signals\n\ndef trim_wake_epochs_from_signals(\n    X, hypnogram, sequence_padding_mask, resampled_hypnogram_length, mask_x_with_zeros:bool=False,\n    padding_mask:int=-100\n):\n\nFunction to trim wake epochs (if wake is the largest class) from signals\nX: bs, channels, seq_len hypnogram: bs, seq_len / resampled_hypnogram_length sequence_padding_mask: bs, seq_len\n\nsource\n\n\ntrim_wake_epochs_from_hypnogram\n\ndef trim_wake_epochs_from_hypnogram(\n    hypnogram, padding_mask:int=-100\n):\n\nFunction to trim wake epochs (if wake is the largest class) from hypnograms This function trims the wake epochs from the beginning and/or end of the hypnogram\nAdapted from Phan et al L-SeqSleepNet\n\nsource\n\n\nSelfSupervisedTimeFrequencyDataset\n\ndef SelfSupervisedTimeFrequencyDataset(\n    zarr_files, # zarr files that include samples\n    channels, # channels to use\n    max_seq_len_sec, # maximum sequence length (in seconds) to use (this is especially relevant when you are returning both stft and raw ts data to keep them in sync)\n    sample_seq_len_sec, # if no sample_df, generate sequences of this length in seconds as one sample\n    sample_stride_sec, # if no sample_df, seconds of overlap for samples from the same array, if seq_len_seconds == overlap_seconds, there is no overlap\n    start_offset_sec:int=0, # number of seconds to exclude from beginning of sleep studies\n    trim_wake_epochs:bool=True, # indicator to trim wake epochs from hypnograms, if it is the largest class\n    include_partial_samples:bool=True, # indicator to include data from partial samples when return_full_length is false\n    sample_df:NoneType=None, # dataframe indicating which indices within each zarr file includes a sample\n    frequency:int=125, # frequency of underlying data\n    return_hypnogram_every_sec:int=30, # integer value indicating the step in indexing in seconds\n    hypnogram_padding_mask:int=-100, # padded value to add to target and indice to ignore when computing loss\n    hypnogram_frequency:int=125, # frequency of underlying y hypnogram data\n    butterworth_filters:NoneType=None, # dictionary of low pass, high pass, and bandpass dictionary to perform on channels\n    median_filter_kernel_size:NoneType=None, # if not none, will apply median filter with kernel size\n    voltage_channels:list=['ECG', 'ECG (LL-RA)', 'EKG', 'ECG (L-R)', 'EOG(L)', 'EOG-L', 'E1', 'LOC', 'E1-M2', 'E1-AVG', 'EMG', 'cchin_l', 'chin', 'EMG (L-R)', 'EMG (1-2)', 'EMG (1-3)', 'Chin3', 'C4-M1', 'C4_M1', 'EEG', 'EEG1', 'EEG2', 'EEG3', 'C3-M2', 'C3_M2', 'C4-AVG'], # if not None, these channels units will be looked at and changed to microvolts from mv uv etc.\n    clip_interpolations:NoneType=None, # dictionary of channels:{'phys_range':..., 'percentiles':...} for filtering and interpolation of filtered values\n    scale_channels:bool=False, # indicator to scale channels to the mean and std of the zarr files.\n    time_channel_scales:NoneType=None, # dictionary of channel:mean and channel:std values for scaling. Should use training statistics\n    return_sequence_padding_mask:bool=False, # indicator to return the key padding mask for attention masking\n):\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\nSleep Stage Supervised Dataset\n\nsource\n\n\nHypnogramTimeFrequencyDataset\n\ndef HypnogramTimeFrequencyDataset(\n    zarr_files, # zarr files that include samples\n    channels, # channels to use\n    max_seq_len_sec, # maximum sequence length (in seconds) to use (this is especially relevant when you are returning both stft and raw ts data to keep them in sync)\n    sample_seq_len_sec, # if no sample_df, generate sequences of this length in seconds as one sample\n    sample_stride_sec, # if no sample_df, seconds of overlap for samples from the same array, if seq_len_seconds == overlap_seconds, there is no overlap\n    start_offset_sec:int=0, # number of seconds to exclude from beginning of sleep studies\n    trim_wake_epochs:bool=True, # indicator to trim wake epochs from hypnograms, if it is the largest class\n    include_partial_samples:bool=True, # indicator to include data from partial samples when return_full_length is false\n    sample_df:NoneType=None, # dataframe indicating which indices within each zarr file includes a sample\n    frequency:int=125, # frequency of underlying data\n    return_y_every_sec:int=30, # integer value indicating the step in indexing in seconds\n    y_padding_mask:int=-100, # padded value to add to target and indice to ignore when computing loss\n    y_frequency:int=125, # frequency of underlying y hypnogram data\n    butterworth_filters:NoneType=None, # dictionary of low pass, high pass, and bandpass dictionary to perform on channels\n    median_filter_kernel_size:NoneType=None, # if not none, will apply median filter with kernel size\n    voltage_channels:list=['ECG', 'ECG (LL-RA)', 'EKG', 'ECG (L-R)', 'EOG(L)', 'EOG-L', 'E1', 'LOC', 'E1-M2', 'E1-AVG', 'EMG', 'cchin_l', 'chin', 'EMG (L-R)', 'EMG (1-2)', 'EMG (1-3)', 'Chin3', 'C4-M1', 'C4_M1', 'EEG', 'EEG1', 'EEG2', 'EEG3', 'C3-M2', 'C3_M2', 'C4-AVG'], # if not None, these channels units will be looked at and changed to microvolts from mv uv etc.\n    clip_interpolations:NoneType=None, # dictionary of channels:{'phys_range':..., 'percentiles':...} for filtering and interpolation of filtered values\n    scale_channels:bool=False, # indicator to scale channels to the mean and std of the zarr files.\n    time_channel_scales:NoneType=None, # dictionary of channel:mean and channel:std values for scaling. Should use training statistics\n    return_sequence_padding_mask:bool=False, # indicator to return the key padding mask for attention masking\n):\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.",
    "crumbs": [
      "Slumber"
    ]
  },
  {
    "objectID": "slumber.html#plotting",
    "href": "slumber.html#plotting",
    "title": "Slumber",
    "section": "Plotting",
    "text": "Plotting\n\nsource\n\nplot_edf_signals\n\ndef plot_edf_signals(\n    signals, signal_names, signal_comparisons:NoneType=None, use_resampler:bool=False, normalize:bool=False,\n    title_text:str='', colorscale:NoneType=None\n):",
    "crumbs": [
      "Slumber"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Inference",
    "section": "",
    "text": "For you!\n\n\nsource\n\nload_pftsleep_models\n\ndef load_pftsleep_models(\n    models_dir:str='', # The directory of the saved models\n    encoder_model_name:str='pft_sleep_encoder.ckpt', # the name of the encoder model\n    classifier_model_name:str='pft_sleep_classifier.ckpt', # the name of the classifier model\n    classifier_head_defaults:dict={'c_in': 7, 'input_size': 512, 'hidden_size': 1024, 'predict_every_n_patches': 5, 'n_classes': 5, 'num_rnn_layers': 2, 'contrastive': False, 'rnn_dropout': 0.1, 'module': 'GRU', 'bidirectional': True, 'affine': True, 'pool': 'average', 'pre_norm': False, 'mlp_final_head': True, 'linear_dropout': 0.1, 'temperature': 2}, # the defaults for the classifier head, DO NOT CHANGE!\n):\n\nLoads the pftsleep models from the models directory\nArgs: models_dir (str): The directory of the saved models encoder_model_name (str): The name of the encoder model classifier_model_name (str): The name of the classifier model classifier_head_defaults (dict): The defaults for the classifier head, DO NOT CHANGE! Returns: encoder (PatchTFTSimpleLightning): The encoder model ss_classifier (PatchTFTSleepStage): The classifier model\n\nsource\n\n\ndownload_pftsleep_models\n\ndef download_pftsleep_models(\n    write_dir:str='', # The directory to write the models to\n    token:NoneType=None, # Your hugging face token to use to download the models\n):\n\nFunction to download pftsleep models from hugging face\nArgs: write_dir (str): The directory to write the models to token (str): Your hugging face token to use to download the models\n\nsource\n\n\nprocess_edf\n\ndef process_edf(\n    edf_file_path, # The edf file path to perform inference on\n    channels, # the channels to read from the edf file\n    reference_channels_dict:dict={}, # the reference channels to subtract from the channels. The keys are the channels to subtract from, and the values are the reference channels.\n    frequency:int=125, # the frequency to resample the channels to. Do not change this!\n    sample_length:int=28800, # the length of the sequence to pad the channels to, expected by the model. Do not change this!\n    frequency_filters_ordered:list=[[0.5, 40], [0.3, 30], [0.3, 30], [0.3, 30], [0.4, None], [0.5, None], [0.5, None]], # the frequency filters to apply to the channels. Do not change this!\n    median_filter_kernel_size:int=3, # the kernel size for the median filter. Do not change this!\n    overwrite_edf_duration:bool=False, # whether to overwrite the duration of the edf file to the sample length, if the edf file duration key is corrupted.\n    verbose:bool=True, # whether to print the verbose output.\n):\n\nProcess the edf file to prepare it for inference.  This function is used to prepare the edf file for inference by reading the channels, resampling them to the correct frequency, filtering them, and padding them to the correct length. Do not change the default parameters (frequency, sample_length, frequency_filters_ordered, median_filter_kernel_size) of this function!\nArgs: edf_file_path (str): The path to the edf file to perform inference on channels (list): The channels to read from the edf file reference_channels_dict (dict): The reference channels to subtract from the channels. The keys are the channels to subtract from, and the values are the reference channels. frequency (int): The frequency to resample the channels to. Do not change this! sample_length (int): The length of the sequence to pad the channels to, expected by the model. Do not change this! frequency_filters_ordered (list): The frequency filters to apply to the channels. Do not change this! median_filter_kernel_size (int): The kernel size for the median filter. Do not change this! overwrite_edf_duration (bool): Whether to overwrite the duration of the edf file to the sample length, if the edf file duration key is corrupted. verbose (bool): Whether to print the verbose output.\nReturns: signals (torch.Tensor): The processed signals sequence_padding_mask (torch.Tensor): The sequence padding mask\n\nsource\n\n\nview_edf_channels\n\ndef view_edf_channels(\n    edf_file_path, # The path to the edf file to view the channels of\n    uppercase:bool=True, # Whether to return the channels in uppercase\n):\n\nView the channels of an edf file.\nArgs: edf_file_path (str): The path to the edf file to view the channels of uppercase (bool): Whether to return the channels in uppercase\nReturns: channels (list): The channels in the edf file\n\nsource\n\n\nEDFDataset\n\ndef EDFDataset(\n    edf_file_paths, # The paths to the edf files to perform inference on\n    eeg_channel, # the EEG channel name in the EDF. The model was trained with C4-M1 and C3-M2 referenced EEG channels. However,\n    left_eog_channel, # the left EOG channel name in the EDF. The model was trained with M2 referenced left EOG channels.\n    chin_emg_channel, # the chin EMG channel name in the EDF. The model was trained with chin refenced (chin 2 or chin 3) EMG channels.\n    ecg_channel, # the ECG channel name in the EDF. The model was trained with augmented lead 2 ecg channels\n    spo2_channel, # the SpO2 channel name in the EDF.\n    abdomen_rr_channel, # the abdomen RR channel name in the EDF.\n    thoracic_rr_channel, # the thoracic RR channel name in the EDF.\n    eeg_reference_channel:NoneType=None, # the EEG reference channel name in the EDF. The model was trained with C4-M1 and C3-M2 referenced EEG channels. This will reference the channels, if they havent already been referenced.\n    left_eog_reference_channel:NoneType=None, # the left EOG reference channel name in the EDF. The model was trained with M2 referenced left EOG channels. This will reference the channels, if they havent already been referenced.\n    chin_emg_reference_channel:NoneType=None, # the chin EMG reference channel name in the EDF. The model was trained with chin refenced (chin 2 or chin 3) EMG channels. This will reference the channels, if they havent already been referenced.\n    ecg_reference_channel:NoneType=None, # the ECG reference channel name in the EDF. The model was trained with augmented lead 2 ecg channels. This will reference the channels, if they havent already been referenced.\n    kwargs:VAR_KEYWORD\n):\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\nsource\n\n\nwrite_pred_to_hypjson\n\ndef write_pred_to_hypjson(\n    predictions, hypjson_path\n):\n\nFunction to write the predictions to a hypjson file.\n\nsource\n\n\ncreate_hypjson\n\ndef create_hypjson(\n    epochs\n):\n\n\nsource\n\n\nmap_stage\n\ndef map_stage(\n    stage\n):\n\n\nsource\n\n\ninfer_on_edf\n\ndef infer_on_edf(\n    edf_file_path, # The edf file path to perform inference on\n    eeg_channel, # the EEG channel name in the EDF. The model was trained with C4-M1 and C3-M2 referenced EEG channels. However,\n    left_eog_channel, # the left EOG channel name in the EDF. The model was trained with M2 referenced left EOG channels.\n    chin_emg_channel, # the chin EMG channel name in the EDF. The model was trained with chin refenced (chin 2 or chin 3) EMG channels.\n    ecg_channel, # the ECG channel name in the EDF. The model was trained with augmented lead 2 ecg channels\n    spo2_channel, # the SpO2 channel name in the EDF.\n    abdomen_rr_channel, # the abdomen RR channel name in the EDF.\n    thoracic_rr_channel, # the thoracic RR channel name in the EDF.\n    eeg_reference_channel:NoneType=None, # the EEG reference channel name in the EDF. The model was trained with C4-M1 and C3-M2 referenced EEG channels. This will reference the channels, if they havent already been referenced.\n    left_eog_reference_channel:NoneType=None, # the left EOG reference channel name in the EDF. The model was trained with M2 referenced left EOG channels. This will reference the channels, if they havent already been referenced.\n    chin_emg_reference_channel:NoneType=None, # the chin EMG reference channel name in the EDF. The model was trained with chin refenced (chin 2 or chin 3) EMG channels. This will reference the channels, if they havent already been referenced.\n    ecg_reference_channel:NoneType=None, # the ECG reference channel name in the EDF. The model was trained with augmented lead 2 ecg channels. This will reference the channels, if they havent already been referenced.\n    models_dir:str='', # the directory of the saved models\n    encoder_model_name:str='pft_sleep_encoder.ckpt', # the name of the encoder model\n    classifier_model_name:str='pft_sleep_classifier.ckpt', # the name of the classifier model\n    device:str='cpu', # the device to run the model on\n    kwargs:VAR_KEYWORD\n):\n\nPerforms inference on a single edf file using the pftsleep models.  If you specify a channel as None or ‘dummy’, the channel will be passed through as a zero vector. This allows you to use the model even if some channels are not present in the edf file.\nArgs: edf_file_path (str): The path to the edf file to perform inference on eeg_channel (str): The name of the EEG channel in the EDF left_eog_channel (str): The name of the left EOG channel in the EDF chin_emg_channel (str): The name of the chin EMG channel in the EDF ecg_channel (str): The name of the ECG channel in the EDF spo2_channel (str): The name of the SpO2 channel in the EDF abdomen_rr_channel (str): The name of the abdomen RR channel in the EDF thoracic_rr_channel (str): The name of the thoracic RR channel in the EDF eeg_reference_channel (str): The name of the EEG reference channel in the EDF left_eog_reference_channel (str): The name of the left EOG reference channel in the EDF chin_emg_reference_channel (str): The name of the chin EMG reference channel in the EDF ecg_reference_channel (str): The name of the ECG reference channel in the EDF models_dir (str): The directory of the saved models encoder_model_name (str): The name of the encoder model classifier_model_name (str): The name of the classifier model device (str): The device to run the model on **kwargs: Additional keyword arguments for process_edf function\nReturns: out (torch.Tensor): The sleep stage logit outputs of the classifier for each sleep epoch in the edf file\n\n# out = infer_on_edf(edf_file_path=''\n#              eeg_channel='C4-M1', \n#              left_eog_channel='E1-M2', \n#              chin_emg_channel='EMG1-EMG2', \n#              ecg_channel='ECG1-ECG2', \n#              spo2_channel='SPO2', \n#              abdomen_rr_channel='ABDO', \n#              thoracic_rr_channel='dummy',\n#              device='mps',\n#              models_dir='../'\n#              )\n\n\nsource\n\n\ninfer_on_edf_dataset\n\ndef infer_on_edf_dataset(\n    edf_dataloader, # the edf dataset to perform inference on\n    device:str='cpu', # the device to run the model on\n    models_dir:str='', # the directory of the saved models\n    encoder_model_name:str='pft_sleep_encoder.ckpt', # the name of the encoder model\n    classifier_model_name:str='pft_sleep_classifier.ckpt', # the name of the classifier model\n):\n\nPerforms inference on an EDFDataset.\nArgs: edf_dataloader (DataLoader): The dataloader (from EDFDataset) to perform inference on device (str): The device to run the model on batch_size (int): The batch size to use for inference models_dir (str): The directory of the saved models encoder_model_name (str): The name of the encoder model classifier_model_name (str): The name of the classifier model\nReturns: preds (list): The predicted sleep stage logits for each edf file",
    "crumbs": [
      "Inference"
    ]
  },
  {
    "objectID": "loss.html",
    "href": "loss.html",
    "title": "Loss functions",
    "section": "",
    "text": "source\n\nhuber_loss\n\ndef huber_loss(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None, delta:int=1\n):\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars] padding_mask: [bs x num_patch]\n\nsource\n\n\ncosine_similarity_loss\n\ndef cosine_similarity_loss(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars]\n\nsource\n\n\ncosine_similarity\n\ndef cosine_similarity(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\n\nsource\n\n\nmape\n\ndef mape(\n    preds, target, mask, use_mask:bool=False\n):\n\n\nsource\n\n\nmae\n\ndef mae(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\n\nsource\n\n\nrmse\n\ndef rmse(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\n\nsource\n\n\nmse\n\ndef mse(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\n\nsource\n\n\nr2_score\n\ndef r2_score(\n    preds, target, mask, use_mask:bool=False\n):\n\n\nsource\n\n\nmasked_mae_loss\n\ndef masked_mae_loss(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars] padding_mask: [bs x num_patch]\n\nsource\n\n\nmasked_mse_loss\n\ndef masked_mse_loss(\n    preds, target, mask, use_mask:bool=False, padding_mask:NoneType=None\n):\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len] mask: [bs x num_patch x n_vars] padding_mask: [bs x num_patch]\n\nsource\n\n\npatch_continuity_loss\n\ndef patch_continuity_loss(\n    preds\n):\n\npreds: [bs x num_patch x n_vars x patch_len] targets: [bs x num_patch x n_vars x patch_len]\n\nx = torch.randn(2,2, 2, 10)\n\npatch_continuity_loss(x)\n\ntensor(0.2936)\n\n\n\nsource\n\n\nFocalLoss\n\ndef FocalLoss(\n    weight:NoneType=None, gamma:float=2.0, reduction:str='mean', ignore_index:int=-100\n):\n\nadapted from tsai, weighted multiclass focal loss https://github.com/timeseriesAI/tsai/blob/bdff96cc8c4c8ea55bc20d7cffd6a72e402f4cb2/tsai/losses.py#L116C1-L140C20\n\ncriterion = FocalLoss(gamma=0.7, weight=None, ignore_index=0)\nbatch_size = 10\n\nn_patch = 721\nn_class = 5\n#m = torch.nn.Softmax(dim=-1)\nlogits = torch.randn(batch_size, n_class, n_patch)\ntarget = torch.randint(0, n_class, size=(batch_size, n_patch))\ncriterion(logits, target)\n\ntensor(8.4217)\n\n\n\nsource\n\n\nKLDivLoss\n\ndef KLDivLoss(\n    weight:NoneType=None, ignore_index:int=-100\n):\n\nKullback-Leibler Divergence Loss with masking for ignore_index. Handles soft labels with ignore_index marked as -100.\nArgs: logits: [bs x n_classes x pred_labels] - model predictions targets: [bs x n_classes x soft_labels] - soft labels, with ignore_index positions marked as -100 or [bs x n_labels] - hard labels\n\nx = torch.randn(4,5,10)\ny = torch.randint(0,5, size=(4,10))\ny_og = y.clone()\ny[0,0] = -100\n\nKLDivLoss(ignore_index=-100)(x,y)\n\ntensor(0.4065)",
    "crumbs": [
      "Loss functions"
    ]
  },
  {
    "objectID": "signal.html",
    "href": "signal.html",
    "title": "Signal",
    "section": "",
    "text": "source\n\nbutterworth\n\ndef butterworth(\n    waveform_array, freq_range, btype, fs:int=125, order:int=2, # Recommend playing around with the order as well\n):\n\n\nsource\n\n\nstft\n\ndef stft(\n    signal_array, n_fft:int=256, # number of ffts to perform\n    win_length:int=250, # window of ffts\n    pad_mode:str='reflect',\n    pad_win_length_to_nfft:bool=True, # indicator to pad the end of the sequence with 0s for nfft-win_legnth to ensuure correct size output\n    center:bool=False, hop_length:int=125, normalized:bool=True, decibel_scale:bool=False, return_complex:bool=True,\n    onesided:bool=True, channel_stft_means:NoneType=None, # precalculated stft channel means\n    channel_stft_stds:NoneType=None, # precalculated stft channel stds\n):\n\nin: [bs x n_vars x max_seq_len] out: [bs x n_vars x n_fft // 2 + 1 x stft_len]",
    "crumbs": [
      "Signal"
    ]
  }
]