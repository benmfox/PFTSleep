{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: index.html\n",
    "title: PFTSleep\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "PFTSleep is a Python package for sleep stage classification using a pre-trained foundational transformer. The repository is built using nbdev, which means the package is developed in Jupyter notebooks.\n",
    "\n",
    "See the publication in [SLEEP](https://doi.org/10.1093/sleep/zsaf061) and original [preprint](https://www.medrxiv.org/content/10.1101/2024.08.02.24311417v1) for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install PFTSleep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To perform inference on unseen data (using EDF file paths as input), use the `pft_sleep_inference.sh`, `pft_sleep_inference.py` and `pftsleep_inference_config.yaml` files. \n",
    "\n",
    "Please create an account on Hugging Face and request access to the models [here](https://huggingface.co/benmfox/PFTSleep). You will also need to create a personal access token, with read access. **Read** more [here](https://huggingface.co/docs/hub/en/security-tokens).\n",
    "\n",
    "Following, download the encoder and sleep stage classifier models with:\n",
    "\n",
    "```\n",
    "from pftsleep.inference import download_pftsleep_models\n",
    "\n",
    "download_pftsleep_models(models_dir='', token=YOUR_HF_TOKEN)\n",
    "```\n",
    "\n",
    "You will also be prompted to download the files when running the inference script, if they are not in `models_dir`. \n",
    "\n",
    "After the models are downloaded, update the `pftsleep_inference_config.yaml` file with a path to your edf or edf directory. You can pass both a single edf file or a directory of edfs. You can also use glob syntax if specifying edfs within single sub directories (e.g. /path/to/base/directory/**/).\n",
    "\n",
    "Unfortunately, due to differing naming conventions for signal channels, if you are passing multiple edfs, but they have different channel names, the dataloader will fail. It is recommended in this case to rewrite the edfs to a consistent channel name format, or perform inference on them one by one. \n",
    "\n",
    "If a specific channel is not available for a given edf or set of edfs, pass the keyword \"null\" or \"dummy\" to that channels name parameter in the yaml if you'd like to see how the model performs with that channel set as all zeros. \n",
    "\n",
    "The model expects referenced EEG, Left EOG, EMG, and ECG channels. If your channels are unreferenced, you may pass the corresponding reference channels. The model was trained with the following referenced channels (priority was given to the ones listed first in the below list):\n",
    "```\n",
    "EEG: C4-M1 or C3-M2\n",
    "EMG: Chin1-Chin2 or Chin1-Chin3\n",
    "ECG: Augmented lead 2 (or ECG (LL) - ECG (RA))\n",
    "Left EOG: E1-M2\n",
    "```\n",
    "\n",
    "Check the `slumber.py` source code for NSRR specific channels (under the `SHHS_CHANNELS`, `MROS_CHANNELS`, `WSC_CHANNELS`, etc. variables) if the above is confusing. \n",
    "\n",
    "For the `device` parameter, use \"cpu\" (slowest), GPU (e.g. \"cuda:0\"), or MPS (\"mps\" for Mac OS X).\n",
    "\n",
    "Prediction logits of shape [bs x 5 x 960] are outputted. The first dimension indicates individual sleep stage logits where the 0 index is wake, 1 is N1, 2 is N2, 3 is N3, and 4 is REM. To retrieve probabilities, use torch's softmax function on the 1st dimension of the tensor. Note that the model expects an 8 hour input and returns 960 class predictions for each 30 second sleep epoch within the 8 hours. If the sleep study is longer than 8 hours, stages after the 8 hour time point will not be predicted. If the sleep study is shorter than 8 hours, stages predicted after the true length of the study should be ignored (despite the model outputted a size of 960). Please file an issue if this becomes a major problem and we can work on a solution. We are also working on models that will accept variable length input.\n",
    "\n",
    "To finally run the predictions on a single edf file or directory of edf files, give permissions to the shell script:\n",
    "```\n",
    "chmod +x pftsleep_inference.sh\n",
    "```\n",
    "\n",
    "Then run it, passing the config file as the main argument:\n",
    "```\n",
    "./pftsleep_inference.sh pftsleep_inference_config.yaml\n",
    "```\n",
    "\n",
    "Predictions will be output to a torch tensor file `.pt` at the location specified in the yaml. \n",
    "\n",
    "Additionally, you can pass the `save_hypjson` parameter in the yaml as `true`. This will perform softmax, max index selecting, and save the predictions as a HYPJSON file (with the same filename as the edf file, + '_pftsleep.HYPJSON'). You can also use the `write_pred_to_hypjson` function for individual files. Sleep stages are mapped in this function to typical HYPJSON standards (for example, REM is mapped to the integer \"5\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository Structure and Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an [nbdev](https://nbdev.fast.ai/) repository, which means the package is developed in Jupyter notebooks located in the `nbs/` directory. Any modifications or additions to the `PFTSleep` package should be made by editing these notebooks.\n",
    "\n",
    "To build the package, run `nbdev_prepare` in the terminal. This will generate the `PFTSleep` package in the `PFTSleep/` directory and all python modules, which can be imported and used in other Python projects.\n",
    "\n",
    "To add new functionality, create a new notebook or add to exisitng in the `nbs/` directory and follow the instructions in the [nbdev documentation](https://nbdev.fast.ai/getting_started.html) to add the new functionality. Then, run `nbdev_prepare` to generate the `PFTSleep` package with the new functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Structure:\n",
    "- `nbs/`: Contains the source notebooks that generate the Python package\n",
    "- `jobs/`: Contains processing and training scripts\n",
    "  - `apples/`: Processing scripts for the apples dataset\n",
    "  - `mesa/`: Processing scripts for the mesa dataset\n",
    "  - `mros/`: Processing scripts for the mros dataset\n",
    "  - `shhs/`: Processing scripts for the shhs dataset\n",
    "  - `wsc/`: Processing scripts for the wsc dataset\n",
    "  - `model_training/`:\n",
    "    - `train_transformer.py`: Trains the initial foundational transformer model\n",
    "    - `train_classifier.py`: Trains the probing head for sleep stage classification\n",
    "\n",
    "Each dataset directory contains scripts to:\n",
    "- Create hypnogram CSVs from annotations\n",
    "- Build zarr files from EDF files\n",
    "- Process and standardize the data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Pipeline\n",
    "\n",
    "1. Foundation Model Training (`jobs/model_training/train_transformer.py`)\n",
    "   - Trains the base transformer model on sleep data zarr files\n",
    "   - Creates general purpose representations of sleep signals\n",
    "\n",
    "2. Probe Training (`jobs/model_training/train_classifier.py`)\n",
    "   - Trains a classification head on top of the foundation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Details\n",
    "\n",
    "- We trained the foundational model on 2x H100 80gb GPUs using PyTorch Lightning. \n",
    "- We monitored training using the [Weights and Biases](https://wandb.ai/) platform.\n",
    "- We performed hyperparameter optimization using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you use PFTSleep in your research, please cite:\n",
    "\n",
    "```bibtex\n",
    "@ARTICLE{Fox2025-zc,\n",
    "  title     = \"A foundational transformer leveraging full night, multichannel\n",
    "               sleep study data accurately classifies sleep stages\",\n",
    "  author    = \"Fox, Benjamin and Jiang, Joy and Wickramaratne, Sajila and\n",
    "               Kovatch, Patricia and Suarez-Farinas, Mayte and Shah, Neomi A\n",
    "               and Parekh, Ankit and Nadkarni, Girish N\",\n",
    "  journal   = \"Sleep\",\n",
    "  publisher = \"Oxford University Press (OUP)\",\n",
    "  month     =  mar,\n",
    "  year      =  2025,\n",
    "  language  = \"en\"\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
