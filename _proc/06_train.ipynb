{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Pew pew pew\n",
    "output-file: train.html\n",
    "title: Train\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Supervised PatchTFT Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PatchTFTSimpleLightning\n",
       "\n",
       ">      PatchTFTSimpleLightning (learning_rate, train_size, batch_size, channels,\n",
       ">                               metrics, precalculate_onebatch_stft_stats=False,\n",
       ">                               use_sequence_padding_mask=False,\n",
       ">                               loss_func='mse', max_lr=0.01, weight_decay=0.0,\n",
       ">                               epochs=100, one_cycle_scheduler=True,\n",
       ">                               optimizer_type='Adam',\n",
       ">                               scheduler_type='OneCycle',\n",
       ">                               cross_attention=False, use_mask=False,\n",
       ">                               patch_continuity_loss=0, huber_delta=None,\n",
       ">                               **patchmeup_kwargs)\n",
       "\n",
       "*Hooks to be used in LightningModule.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learning_rate |  |  |  |\n",
       "| train_size |  |  |  |\n",
       "| batch_size |  |  |  |\n",
       "| channels |  |  |  |\n",
       "| metrics |  |  |  |\n",
       "| precalculate_onebatch_stft_stats | bool | False |  |\n",
       "| use_sequence_padding_mask | bool | False |  |\n",
       "| loss_func | str | mse |  |\n",
       "| max_lr | float | 0.01 |  |\n",
       "| weight_decay | float | 0.0 |  |\n",
       "| epochs | int | 100 |  |\n",
       "| one_cycle_scheduler | bool | True |  |\n",
       "| optimizer_type | str | Adam |  |\n",
       "| scheduler_type | str | OneCycle |  |\n",
       "| cross_attention | bool | False | not implemented |\n",
       "| use_mask | bool | False |  |\n",
       "| patch_continuity_loss | int | 0 | indicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities |\n",
       "| huber_delta | NoneType | None | huber loss delta, not used otherwise |\n",
       "| patchmeup_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PatchTFTSimpleLightning\n",
       "\n",
       ">      PatchTFTSimpleLightning (learning_rate, train_size, batch_size, channels,\n",
       ">                               metrics, precalculate_onebatch_stft_stats=False,\n",
       ">                               use_sequence_padding_mask=False,\n",
       ">                               loss_func='mse', max_lr=0.01, weight_decay=0.0,\n",
       ">                               epochs=100, one_cycle_scheduler=True,\n",
       ">                               optimizer_type='Adam',\n",
       ">                               scheduler_type='OneCycle',\n",
       ">                               cross_attention=False, use_mask=False,\n",
       ">                               patch_continuity_loss=0, huber_delta=None,\n",
       ">                               **patchmeup_kwargs)\n",
       "\n",
       "*Hooks to be used in LightningModule.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learning_rate |  |  |  |\n",
       "| train_size |  |  |  |\n",
       "| batch_size |  |  |  |\n",
       "| channels |  |  |  |\n",
       "| metrics |  |  |  |\n",
       "| precalculate_onebatch_stft_stats | bool | False |  |\n",
       "| use_sequence_padding_mask | bool | False |  |\n",
       "| loss_func | str | mse |  |\n",
       "| max_lr | float | 0.01 |  |\n",
       "| weight_decay | float | 0.0 |  |\n",
       "| epochs | int | 100 |  |\n",
       "| one_cycle_scheduler | bool | True |  |\n",
       "| optimizer_type | str | Adam |  |\n",
       "| scheduler_type | str | OneCycle |  |\n",
       "| cross_attention | bool | False | not implemented |\n",
       "| use_mask | bool | False |  |\n",
       "| patch_continuity_loss | int | 0 | indicator and ratio of patch continuity loss function, which examines ensures patches dont have large discontinuities |\n",
       "| huber_delta | NoneType | None | huber loss delta, not used otherwise |\n",
       "| patchmeup_kwargs |  |  |  |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PatchTFTSimpleLightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class EncoderTestFreezingWeights(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(750,512)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class DecoderTest(pl.LightningModule):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.freeze()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(list(self.encoder.parameters()))\n",
    "        print(self.encoder.training)\n",
    "        # x = self.encoder(x)\n",
    "        # return x\n",
    "\n",
    "encoder = EncoderTestFreezingWeights()\n",
    "decoder = DecoderTest(encoder)\n",
    "\n",
    "decoder.training_step(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Training with Linear Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchTFT Masked Autoregression SS Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PatchTFTSleepStage\n",
       "\n",
       ">      PatchTFTSleepStage (learning_rate, train_size, batch_size,\n",
       ">                          linear_probing_head, metrics=[], fine_tune=False,\n",
       ">                          loss_fxn='CrossEntropy', class_weights=None,\n",
       ">                          gamma=2.0, label_smoothing=0,\n",
       ">                          use_sequence_padding_mask=False, y_padding_mask=-100,\n",
       ">                          max_lr=0.01, epochs=100, one_cycle_scheduler=True,\n",
       ">                          weight_decay=0.0, pretrained_encoder_path=None,\n",
       ">                          optimizer_type='Adam', scheduler_type='OneCycle',\n",
       ">                          preloaded_model=None, torch_model_name='model',\n",
       ">                          remove_pretrain_layers=['head', 'mask'],\n",
       ">                          return_softmax=True)\n",
       "\n",
       "*Hooks to be used in LightningModule.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learning_rate |  |  | desired learning rate, initial learning rate in if one_cycle_scheduler |\n",
       "| train_size |  |  | the training data size (for one_cycle_scheduler=True) |\n",
       "| batch_size |  |  | the batch size (for one_cycle_scheduler=True) |\n",
       "| linear_probing_head |  |  | model head to train |\n",
       "| metrics | list | [] | metrics to calculate |\n",
       "| fine_tune | bool | False | indicator to finetune encoder model or perform linear probing and freeze encoder weights |\n",
       "| loss_fxn | str | CrossEntropy | loss function to use, can be CrossEntropy or FocalLoss |\n",
       "| class_weights | NoneType | None | weights of classes to use in CE loss fxn |\n",
       "| gamma | float | 2.0 | for focal loss |\n",
       "| label_smoothing | int | 0 | label smoothing for cross entropy loss |\n",
       "| use_sequence_padding_mask | bool | False | indicator to use the sequence padding mask when training/in the loss fxn |\n",
       "| y_padding_mask | int | -100 | padded value that was added to target and indice to ignore when computing loss |\n",
       "| max_lr | float | 0.01 | maximum learning rate for one_cycle_scheduler |\n",
       "| epochs | int | 100 | number of epochs for one_cycle_scheduler |\n",
       "| one_cycle_scheduler | bool | True | indicator to use a one cycle scheduler to vary the learning rate |\n",
       "| weight_decay | float | 0.0 | weight decay for Adam optimizer |\n",
       "| pretrained_encoder_path | NoneType | None | path of the pretrained model to use for linear probing |\n",
       "| optimizer_type | str | Adam | optimizer to use, 'Adam' or 'AdamW' |\n",
       "| scheduler_type | str | OneCycle | scheduler to use, 'OneCycle' or 'CosineAnnealingWarmRestarts' |\n",
       "| preloaded_model | NoneType | None | loaded pretrained model to use for linear probing |\n",
       "| torch_model_name | str | model | name of the pytorch model within the lightning model module, this is to remove layers (for example lightning_model.pytorch_model.head = nn.Identity()) |\n",
       "| remove_pretrain_layers | list | ['head', 'mask'] | layers within the lightning model or lightning model.pytorch_model to remove |\n",
       "| return_softmax | bool | True | indicator to return softmax probabilities in forward and predict_step |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PatchTFTSleepStage\n",
       "\n",
       ">      PatchTFTSleepStage (learning_rate, train_size, batch_size,\n",
       ">                          linear_probing_head, metrics=[], fine_tune=False,\n",
       ">                          loss_fxn='CrossEntropy', class_weights=None,\n",
       ">                          gamma=2.0, label_smoothing=0,\n",
       ">                          use_sequence_padding_mask=False, y_padding_mask=-100,\n",
       ">                          max_lr=0.01, epochs=100, one_cycle_scheduler=True,\n",
       ">                          weight_decay=0.0, pretrained_encoder_path=None,\n",
       ">                          optimizer_type='Adam', scheduler_type='OneCycle',\n",
       ">                          preloaded_model=None, torch_model_name='model',\n",
       ">                          remove_pretrain_layers=['head', 'mask'],\n",
       ">                          return_softmax=True)\n",
       "\n",
       "*Hooks to be used in LightningModule.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learning_rate |  |  | desired learning rate, initial learning rate in if one_cycle_scheduler |\n",
       "| train_size |  |  | the training data size (for one_cycle_scheduler=True) |\n",
       "| batch_size |  |  | the batch size (for one_cycle_scheduler=True) |\n",
       "| linear_probing_head |  |  | model head to train |\n",
       "| metrics | list | [] | metrics to calculate |\n",
       "| fine_tune | bool | False | indicator to finetune encoder model or perform linear probing and freeze encoder weights |\n",
       "| loss_fxn | str | CrossEntropy | loss function to use, can be CrossEntropy or FocalLoss |\n",
       "| class_weights | NoneType | None | weights of classes to use in CE loss fxn |\n",
       "| gamma | float | 2.0 | for focal loss |\n",
       "| label_smoothing | int | 0 | label smoothing for cross entropy loss |\n",
       "| use_sequence_padding_mask | bool | False | indicator to use the sequence padding mask when training/in the loss fxn |\n",
       "| y_padding_mask | int | -100 | padded value that was added to target and indice to ignore when computing loss |\n",
       "| max_lr | float | 0.01 | maximum learning rate for one_cycle_scheduler |\n",
       "| epochs | int | 100 | number of epochs for one_cycle_scheduler |\n",
       "| one_cycle_scheduler | bool | True | indicator to use a one cycle scheduler to vary the learning rate |\n",
       "| weight_decay | float | 0.0 | weight decay for Adam optimizer |\n",
       "| pretrained_encoder_path | NoneType | None | path of the pretrained model to use for linear probing |\n",
       "| optimizer_type | str | Adam | optimizer to use, 'Adam' or 'AdamW' |\n",
       "| scheduler_type | str | OneCycle | scheduler to use, 'OneCycle' or 'CosineAnnealingWarmRestarts' |\n",
       "| preloaded_model | NoneType | None | loaded pretrained model to use for linear probing |\n",
       "| torch_model_name | str | model | name of the pytorch model within the lightning model module, this is to remove layers (for example lightning_model.pytorch_model.head = nn.Identity()) |\n",
       "| remove_pretrain_layers | list | ['head', 'mask'] | layers within the lightning model or lightning model.pytorch_model to remove |\n",
       "| return_softmax | bool | True | indicator to return softmax probabilities in forward and predict_step |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PatchTFTSleepStage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
