{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: A good use of time, no doubt.\n",
    "output-file: transformers.html\n",
    "title: Transformers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TSTEncoderLayer\n",
       "\n",
       ">      TSTEncoderLayer (d_model, n_heads, d_ff=256, store_attn=False,\n",
       ">                       norm='BatchNorm', relative_attn_type='vanilla',\n",
       ">                       use_flash_attn=False, num_patches=None, attn_dropout=0,\n",
       ">                       dropout=0.0, bias=True, activation='gelu',\n",
       ">                       res_attention=False, pre_norm=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| d_model |  |  | dimension of patch embeddings |\n",
       "| n_heads |  |  | number of attention heads per layer |\n",
       "| d_ff | int | 256 | dimension of feedforward layer in each transformer layer |\n",
       "| store_attn | bool | False | indicator of whether or not to store attention |\n",
       "| norm | str | BatchNorm |  |\n",
       "| relative_attn_type | str | vanilla | options include vaniall or eRPE |\n",
       "| use_flash_attn | bool | False | indicator to use flash attention |\n",
       "| num_patches | NoneType | None | num patches required for eRPE attn |\n",
       "| attn_dropout | int | 0 |  |\n",
       "| dropout | float | 0.0 |  |\n",
       "| bias | bool | True |  |\n",
       "| activation | str | gelu |  |\n",
       "| res_attention | bool | False |  |\n",
       "| pre_norm | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TSTEncoderLayer\n",
       "\n",
       ">      TSTEncoderLayer (d_model, n_heads, d_ff=256, store_attn=False,\n",
       ">                       norm='BatchNorm', relative_attn_type='vanilla',\n",
       ">                       use_flash_attn=False, num_patches=None, attn_dropout=0,\n",
       ">                       dropout=0.0, bias=True, activation='gelu',\n",
       ">                       res_attention=False, pre_norm=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| d_model |  |  | dimension of patch embeddings |\n",
       "| n_heads |  |  | number of attention heads per layer |\n",
       "| d_ff | int | 256 | dimension of feedforward layer in each transformer layer |\n",
       "| store_attn | bool | False | indicator of whether or not to store attention |\n",
       "| norm | str | BatchNorm |  |\n",
       "| relative_attn_type | str | vanilla | options include vaniall or eRPE |\n",
       "| use_flash_attn | bool | False | indicator to use flash attention |\n",
       "| num_patches | NoneType | None | num patches required for eRPE attn |\n",
       "| attn_dropout | int | 0 |  |\n",
       "| dropout | float | 0.0 |  |\n",
       "| bias | bool | True |  |\n",
       "| activation | str | gelu |  |\n",
       "| res_attention | bool | False |  |\n",
       "| pre_norm | bool | False |  |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TSTEncoderLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Time Series and Frequency Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PatchTFTSimple\n",
       "\n",
       ">      PatchTFTSimple (c_in:int, win_length, hop_length, max_seq_len,\n",
       ">                      time_domain=True, pos_encoding_type='learned',\n",
       ">                      relative_attn_type='vanilla', use_flash_attn=False,\n",
       ">                      use_revin=True, dim1reduce=False, affine=True,\n",
       ">                      mask_ratio=0.1, augmentations=['patch_mask',\n",
       ">                      'jitter_zero_mask', 'reverse_sequence',\n",
       ">                      'shuffle_channels'], n_layers:int=2, d_model=512,\n",
       ">                      n_heads=2, shared_embedding=False, d_ff:int=2048,\n",
       ">                      norm:str='BatchNorm', attn_dropout:float=0.0,\n",
       ">                      dropout:float=0.1, act:str='gelu',\n",
       ">                      res_attention:bool=True, pre_norm:bool=False,\n",
       ">                      store_attn:bool=False, pretrain_head=True,\n",
       ">                      pretrain_head_n_layers=1, pretrain_head_dropout=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in | int |  | the number of input channels |\n",
       "| win_length |  |  | the length of the patch of time/interval or short time ft windown length (when time_domain=False) |\n",
       "| hop_length |  |  | the length of the distance between each patch/fft |\n",
       "| max_seq_len |  |  | maximum sequence len |\n",
       "| time_domain | bool | True |  |\n",
       "| pos_encoding_type | str | learned | options include learned or tAPE |\n",
       "| relative_attn_type | str | vanilla | options include vanilla or eRPE |\n",
       "| use_flash_attn | bool | False | indicator to use flash attention |\n",
       "| use_revin | bool | True | if time_domain is true, whether or not to instance normalize time data |\n",
       "| dim1reduce | bool | False | indicator to normalize by timepoint in revin |\n",
       "| affine | bool | True | if time_domain is true, whether or not to learn revin normalization parameters |\n",
       "| mask_ratio | float | 0.1 | amount of signal to mask |\n",
       "| augmentations | list | ['patch_mask', 'jitter_zero_mask', 'reverse_sequence', 'shuffle_channels'] | the type of mask to use, options are patch or jitter_zero |\n",
       "| n_layers | int | 2 | the number of transformer encoder layers to use |\n",
       "| d_model | int | 512 | the dimension of the input to the transofmrer encoder |\n",
       "| n_heads | int | 2 | the number of heads in each layer |\n",
       "| shared_embedding | bool | False | indicator for whether or not each channel should be projected with its own set of linear weights to the encoder dimension |\n",
       "| d_ff | int | 2048 | the feedforward layer size in the transformer |\n",
       "| norm | str | BatchNorm | BatchNorm or LayerNorm during trianing |\n",
       "| attn_dropout | float | 0.0 | dropout in attention |\n",
       "| dropout | float | 0.1 | dropout for linear layers |\n",
       "| act | str | gelu | activation function |\n",
       "| res_attention | bool | True | whether to use residual attention |\n",
       "| pre_norm | bool | False | indicator to pre batch or layer norm |\n",
       "| store_attn | bool | False | indicator to store attention |\n",
       "| pretrain_head | bool | True | indicator to include a pretraining head |\n",
       "| pretrain_head_n_layers | int | 1 | how many linear layers on the pretrained head |\n",
       "| pretrain_head_dropout | float | 0.0 | dropout applied to pretrain head |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PatchTFTSimple\n",
       "\n",
       ">      PatchTFTSimple (c_in:int, win_length, hop_length, max_seq_len,\n",
       ">                      time_domain=True, pos_encoding_type='learned',\n",
       ">                      relative_attn_type='vanilla', use_flash_attn=False,\n",
       ">                      use_revin=True, dim1reduce=False, affine=True,\n",
       ">                      mask_ratio=0.1, augmentations=['patch_mask',\n",
       ">                      'jitter_zero_mask', 'reverse_sequence',\n",
       ">                      'shuffle_channels'], n_layers:int=2, d_model=512,\n",
       ">                      n_heads=2, shared_embedding=False, d_ff:int=2048,\n",
       ">                      norm:str='BatchNorm', attn_dropout:float=0.0,\n",
       ">                      dropout:float=0.1, act:str='gelu',\n",
       ">                      res_attention:bool=True, pre_norm:bool=False,\n",
       ">                      store_attn:bool=False, pretrain_head=True,\n",
       ">                      pretrain_head_n_layers=1, pretrain_head_dropout=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in | int |  | the number of input channels |\n",
       "| win_length |  |  | the length of the patch of time/interval or short time ft windown length (when time_domain=False) |\n",
       "| hop_length |  |  | the length of the distance between each patch/fft |\n",
       "| max_seq_len |  |  | maximum sequence len |\n",
       "| time_domain | bool | True |  |\n",
       "| pos_encoding_type | str | learned | options include learned or tAPE |\n",
       "| relative_attn_type | str | vanilla | options include vanilla or eRPE |\n",
       "| use_flash_attn | bool | False | indicator to use flash attention |\n",
       "| use_revin | bool | True | if time_domain is true, whether or not to instance normalize time data |\n",
       "| dim1reduce | bool | False | indicator to normalize by timepoint in revin |\n",
       "| affine | bool | True | if time_domain is true, whether or not to learn revin normalization parameters |\n",
       "| mask_ratio | float | 0.1 | amount of signal to mask |\n",
       "| augmentations | list | ['patch_mask', 'jitter_zero_mask', 'reverse_sequence', 'shuffle_channels'] | the type of mask to use, options are patch or jitter_zero |\n",
       "| n_layers | int | 2 | the number of transformer encoder layers to use |\n",
       "| d_model | int | 512 | the dimension of the input to the transofmrer encoder |\n",
       "| n_heads | int | 2 | the number of heads in each layer |\n",
       "| shared_embedding | bool | False | indicator for whether or not each channel should be projected with its own set of linear weights to the encoder dimension |\n",
       "| d_ff | int | 2048 | the feedforward layer size in the transformer |\n",
       "| norm | str | BatchNorm | BatchNorm or LayerNorm during trianing |\n",
       "| attn_dropout | float | 0.0 | dropout in attention |\n",
       "| dropout | float | 0.1 | dropout for linear layers |\n",
       "| act | str | gelu | activation function |\n",
       "| res_attention | bool | True | whether to use residual attention |\n",
       "| pre_norm | bool | False | indicator to pre batch or layer norm |\n",
       "| store_attn | bool | False | indicator to store attention |\n",
       "| pretrain_head | bool | True | indicator to include a pretraining head |\n",
       "| pretrain_head_n_layers | int | 1 | how many linear layers on the pretrained head |\n",
       "| pretrain_head_dropout | float | 0.0 | dropout applied to pretrain head |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PatchTFTSimple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 480])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 480, 7, 750]),\n",
       " torch.Size([4, 480, 7, 750]),\n",
       " torch.Size([4, 480]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX = torch.randn(4,7,1*3600*100)\n",
    "pad = torch.zeros(4,1*3600*100)\n",
    "pad[:,0:100] = 1\n",
    "model = PatchTFTSimple(c_in=7,\n",
    "                        win_length=750,\n",
    "                        hop_length=750,\n",
    "                        max_seq_len=(1*3600*100),\n",
    "                        use_revin=True,\n",
    "                        time_domain=True,\n",
    "                        affine=False,\n",
    "                        dim1reduce=False,\n",
    "                        act='gelu',\n",
    "                        use_flash_attn=True,\n",
    "                        relative_attn_type='vanilla',\n",
    "                        pos_encoding_type='learned',\n",
    "                        mask_ratio=0.1,\n",
    "                        augmentations=['jitter_zero_mask'],\n",
    "                        n_layers=1,\n",
    "                        n_heads=1,\n",
    "                        d_model=512,\n",
    "                        d_ff=2048,\n",
    "                        dropout=0.,\n",
    "                        attn_dropout=0.,\n",
    "                        pre_norm=False,\n",
    "                        res_attention=False,\n",
    "                        shared_embedding=False,\n",
    "                        pretrain_head=True\n",
    "                        )\n",
    "r = model(XX, sequence_padding_mask=pad)\n",
    "r[0].shape, r[1].shape, r[3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
