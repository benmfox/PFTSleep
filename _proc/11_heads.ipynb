{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: none of these work!\n",
    "output-file: heads.html\n",
    "title: SSL, Fine Tuning, and Linear Probing Heads\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37998f1e",
   "metadata": {},
   "source": [
    "## Linear Probing and Fine Tuning Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RNNProbingHead\n",
       "\n",
       "```python\n",
       "\n",
       "def RNNProbingHead(\n",
       "    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, module:str='GRU', linear_dropout:float=0.0,\n",
       "    rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n",
       "    pool:str='average', # 'average' or 'max' or 'majority'\n",
       "    temperature:float=2.0, # only used if pool='majority'\n",
       "    n_linear_layers:int=1, predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False,\n",
       "    shared_embedding:bool=True, augmentations:NoneType=None, augmentation_mask_ratio:float=0.0,\n",
       "    augmentation_dims_to_shuffle:list=[1, 2, 3], norm:NoneType=None, # one of [None, 'pre', 'post']\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def RNNProbingHead(\n",
       "    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, module:str='GRU', linear_dropout:float=0.0,\n",
       "    rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n",
       "    pool:str='average', # 'average' or 'max' or 'majority'\n",
       "    temperature:float=2.0, # only used if pool='majority'\n",
       "    n_linear_layers:int=1, predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False,\n",
       "    shared_embedding:bool=True, augmentations:NoneType=None, augmentation_mask_ratio:float=0.0,\n",
       "    augmentation_dims_to_shuffle:list=[1, 2, 3], norm:NoneType=None, # one of [None, 'pre', 'post']\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RNNProbingHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L186){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RNNProbingHeadExperimental\n",
       "\n",
       "```python\n",
       "\n",
       "def RNNProbingHeadExperimental(\n",
       "    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, # deprecated\n",
       "    module:str='GRU', linear_dropout:float=0.0, rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n",
       "    pool:str='average', # 'average' or 'max' or 'majority'\n",
       "    temperature:float=2.0, # only used if pool='majority'\n",
       "    predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False, augmentations:NoneType=None,\n",
       "    augmentation_mask_ratio:float=0.0, augmentation_dims_to_shuffle:list=[1, 2, 3],\n",
       "    pre_norm:bool=True, # one of [None, 'pre', 'post']\n",
       "    mlp_final_head:bool=False\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def RNNProbingHeadExperimental(\n",
       "    c_in, input_size, hidden_size, n_classes, contrastive:bool=False, # deprecated\n",
       "    module:str='GRU', linear_dropout:float=0.0, rnn_dropout:float=0.0, num_rnn_layers:int=1, act:str='gelu',\n",
       "    pool:str='average', # 'average' or 'max' or 'majority'\n",
       "    temperature:float=2.0, # only used if pool='majority'\n",
       "    predict_every_n_patches:int=1, bidirectional:bool=True, affine:bool=False, augmentations:NoneType=None,\n",
       "    augmentation_mask_ratio:float=0.0, augmentation_dims_to_shuffle:list=[1, 2, 3],\n",
       "    pre_norm:bool=True, # one of [None, 'pre', 'post']\n",
       "    mlp_final_head:bool=False\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RNNProbingHeadExperimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5ba25",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 30])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNProbingHeadExperimental(c_in=7, \n",
    "                                pool='average', \n",
    "                                input_size = 384, \n",
    "                                bidirectional=True,\n",
    "                                affine=False, \n",
    "                                hidden_size=1200,\n",
    "                                module='GRU',\n",
    "                                n_classes=4,\n",
    "                                predict_every_n_patches=32,\n",
    "                                rnn_dropout=0.,\n",
    "                                num_rnn_layers=1,\n",
    "                                linear_dropout=0.,\n",
    "                                mlp_final_head=False,\n",
    "                                pre_norm=True)\n",
    "x = torch.randn((4,7,384,960))\n",
    "sequence_padding_mask = torch.zeros(4,960)\n",
    "sequence_padding_mask[:,-32:] = 1\n",
    "m(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f7774",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 30])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\n",
    "x = torch.randn((4,7,384,960))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90572d34",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\n",
    "x = torch.randn((4,7,512*2,3600))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L327){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TransformerDecoderProbingHead\n",
       "\n",
       "```python\n",
       "\n",
       "def TransformerDecoderProbingHead(\n",
       "    c_in, d_model, n_classes, norm:str='BatchNorm', dropout:float=0.0, act:str='gelu', d_ff:int=2048,\n",
       "    num_layers:int=1, n_heads:int=2, predict_every_n_patches:int=1, affine:bool=False, shared_embedding:bool=True\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def TransformerDecoderProbingHead(\n",
       "    c_in, d_model, n_classes, norm:str='BatchNorm', dropout:float=0.0, act:str='gelu', d_ff:int=2048,\n",
       "    num_layers:int=1, n_heads:int=2, predict_every_n_patches:int=1, affine:bool=False, shared_embedding:bool=True\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TransformerDecoderProbingHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d7c8d",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\n",
    "x = torch.randn((4, 7, 512, 3600))\n",
    "\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L411){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DecoderFeedForward\n",
       "\n",
       "```python\n",
       "\n",
       "def DecoderFeedForward(\n",
       "    c_in, # the number of input channels\n",
       "    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n",
       "    num_layers, d_ff, attn_dropout, res_attention, pre_norm, store_attn, n_heads, shared_embedding, affine,\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    d_model, # the dimension of the transformer model\n",
       "    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n",
       "    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n",
       "    dropout:float=0.0, # dropout in between linear layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*transformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by*\n",
       "a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful."
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def DecoderFeedForward(\n",
       "    c_in, # the number of input channels\n",
       "    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n",
       "    num_layers, d_ff, attn_dropout, res_attention, pre_norm, store_attn, n_heads, shared_embedding, affine,\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    d_model, # the dimension of the transformer model\n",
       "    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n",
       "    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n",
       "    dropout:float=0.0, # dropout in between linear layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*transformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by*\n",
       "a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DecoderFeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265e3d3",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in = 7\n",
    "frequency = 125\n",
    "win_length=750 \n",
    "overlap = 0.\n",
    "hop_length=win_length - int(overlap*win_length)\n",
    "max_seq_len_sec = (6*3600) # for dataloader\n",
    "#seq_len_sec = sample_stride = 3*3600 # for dataloader\n",
    "max_seq_len = max_seq_len_sec*frequency # for model\n",
    "#n_patches = n_fft // 2 + 1\n",
    "n_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n",
    "\n",
    "#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\n",
    "x = torch.randn(2,c_in,512,n_patches)\n",
    "\n",
    "model = DecoderFeedForward(c_in=c_in,\n",
    "                           predict_every_n_patches=5,\n",
    "                           num_layers=1,\n",
    "                           d_ff = 2048,\n",
    "                           attn_dropout=0.,\n",
    "                           res_attention = False,\n",
    "                           pre_norm = False,\n",
    "                           store_attn = False,\n",
    "                           n_heads=2,\n",
    "                           affine=False,\n",
    "                           shared_embedding=False,\n",
    "                           n_classes=5,\n",
    "                           d_model=512,\n",
    "                           norm='BatchNorm',\n",
    "                           act='gelu',\n",
    "                           dropout=0.\n",
    "                           )\n",
    "\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L523){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeDistributedConvolutionalFeedForward\n",
       "\n",
       "```python\n",
       "\n",
       "def TimeDistributedConvolutionalFeedForward(\n",
       "    c_in, # the number of input channels\n",
       "    frequency, # the frequency of the original channels\n",
       "    predict_every_seconds, # for a given sequence of length m with frequency f, number of predictions\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    win_length, # the convolved patch length, the first step in this is to do a linear layer to this dimension\n",
       "    d_model, # the dimension of the transformer model\n",
       "    affine:bool=False, shared_embedding:bool=True\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Convolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension.*\n",
       "Then, a convolutional transpose is used to extrapolate the data to its original form.\n",
       "Finally, a final convolution is used to predict the classes."
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def TimeDistributedConvolutionalFeedForward(\n",
       "    c_in, # the number of input channels\n",
       "    frequency, # the frequency of the original channels\n",
       "    predict_every_seconds, # for a given sequence of length m with frequency f, number of predictions\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    win_length, # the convolved patch length, the first step in this is to do a linear layer to this dimension\n",
       "    d_model, # the dimension of the transformer model\n",
       "    affine:bool=False, shared_embedding:bool=True\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Convolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension.*\n",
       "Then, a convolutional transpose is used to extrapolate the data to its original form.\n",
       "Finally, a final convolution is used to predict the classes."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TimeDistributedConvolutionalFeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L594){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LinearProbingHead\n",
       "\n",
       "```python\n",
       "\n",
       "def LinearProbingHead(\n",
       "    c_in, # the number of input channels in the original input\n",
       "    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    input_size, # the dimension of the transformer model\n",
       "    n_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n",
       "    num_patch,\n",
       "    shared_embedding:bool=True, # whether or not to have a dense layer per channel or one layer per channel\n",
       "    affine:bool=True, # include learnable parameters to weight predictions\n",
       "    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n",
       "    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n",
       "    dropout:float=0.0, # dropout in between linear layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*A linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def LinearProbingHead(\n",
       "    c_in, # the number of input channels in the original input\n",
       "    predict_every_n_patches, # for a given sequence of length m with frequency f, number of predictions\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    input_size, # the dimension of the transformer model\n",
       "    n_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n",
       "    num_patch,\n",
       "    shared_embedding:bool=True, # whether or not to have a dense layer per channel or one layer per channel\n",
       "    affine:bool=True, # include learnable parameters to weight predictions\n",
       "    norm:str='BatchNorm', # batchnorm or layernorm between linear and convolutional layers\n",
       "    act:str='gelu', # activation function to use between layers, 'gelu' or 'relu'\n",
       "    dropout:float=0.0, # dropout in between linear layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*A linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(LinearProbingHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b22b7",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LinearProbingHead(c_in=7, \n",
    "                      input_size = 512, \n",
    "                      predict_every_n_patches=5,\n",
    "                      n_classes=5,\n",
    "                      n_layers=3,\n",
    "                      shared_embedding=True,\n",
    "                      affine=True,\n",
    "                      num_patch=3600,\n",
    "                      dropout=0.1)\n",
    "\n",
    "x = torch.randn((4,7,512,3600))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L677){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TimeDistributedFeedForward\n",
       "\n",
       "```python\n",
       "\n",
       "def TimeDistributedFeedForward(\n",
       "    c_in, # the number of input channels\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    n_patches, # the number of stft or time patches\n",
       "    d_model, # the dimension of the transformer model\n",
       "    pred_len_seconds, # the sequence multiclass prediction length in seconds\n",
       "    n_linear_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n",
       "    conv_kernel_stride_size, # the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n",
       "    dropout:float=0.0, # dropout in between linear layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Feed forward head that uses a convolutional layer to reduce channel dimensionality*\n",
       "Followed by a feedforward network to make"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def TimeDistributedFeedForward(\n",
       "    c_in, # the number of input channels\n",
       "    n_classes, # the number of classes to predict (for sleep stage - there are 6)\n",
       "    n_patches, # the number of stft or time patches\n",
       "    d_model, # the dimension of the transformer model\n",
       "    pred_len_seconds, # the sequence multiclass prediction length in seconds\n",
       "    n_linear_layers, # the number of linear layers to use in the prediciton head, with RELU activation and dropout\n",
       "    conv_kernel_stride_size, # the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here.\n",
       "    dropout:float=0.0, # dropout in between linear layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Feed forward head that uses a convolutional layer to reduce channel dimensionality*\n",
       "Followed by a feedforward network to make"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TimeDistributedFeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L850){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConvBiGRU\n",
       "\n",
       "```python\n",
       "\n",
       "def ConvBiGRU(\n",
       "    input_size, hidden_sizes, kernel_sizes, n_layers, d_model, predict_every_n_patches, n_classes\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def ConvBiGRU(\n",
       "    input_size, hidden_sizes, kernel_sizes, n_layers, d_model, predict_every_n_patches, n_classes\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConvBiGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L776){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConvGRU1D\n",
       "\n",
       "```python\n",
       "\n",
       "def ConvGRU1D(\n",
       "    input_size, hidden_sizes, # if integer, the same hidden size is used for all cells.\n",
       "    kernel_sizes, # if integer, the same kernel size is used for all cells.\n",
       "    n_layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def ConvGRU1D(\n",
       "    input_size, hidden_sizes, # if integer, the same hidden size is used for all cells.\n",
       "    kernel_sizes, # if integer, the same kernel size is used for all cells.\n",
       "    n_layers\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Base class for all neural network modules.*\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConvGRU1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/heads.py#L733){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConvGRU1DCell\n",
       "\n",
       "```python\n",
       "\n",
       "def ConvGRU1DCell(\n",
       "    input_size, hidden_size, kernel_size\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Generate a convolutional GRU cell*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def ConvGRU1DCell(\n",
       "    input_size, hidden_size, kernel_size\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Generate a convolutional GRU cell*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConvGRU1DCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f289111",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4,7,512,3600))\n",
    "\n",
    "convgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n",
    "\n",
    "out = convgru(x)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
