{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: none of these work!\n",
    "output-file: heads.html\n",
    "title: SSL, Fine Tuning, and Linear Probing Heads\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Probing and Fine Tuning Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### RNNProbingHead\n",
       "\n",
       ">      RNNProbingHead (c_in, input_size, hidden_size, n_classes,\n",
       ">                      contrastive=False, module='GRU', linear_dropout=0.0,\n",
       ">                      rnn_dropout=0.0, num_rnn_layers=1, act='gelu',\n",
       ">                      pool='average', temperature=2.0, n_linear_layers=1,\n",
       ">                      predict_every_n_patches=1, bidirectional=True,\n",
       ">                      affine=False, shared_embedding=True, augmentations=None,\n",
       ">                      augmentation_mask_ratio=0.0,\n",
       ">                      augmentation_dims_to_shuffle=[1, 2, 3], norm=None)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  |  |\n",
       "| input_size |  |  |  |\n",
       "| hidden_size |  |  |  |\n",
       "| n_classes |  |  |  |\n",
       "| contrastive | bool | False |  |\n",
       "| module | str | GRU |  |\n",
       "| linear_dropout | float | 0.0 |  |\n",
       "| rnn_dropout | float | 0.0 |  |\n",
       "| num_rnn_layers | int | 1 |  |\n",
       "| act | str | gelu |  |\n",
       "| pool | str | average | 'average' or 'max' or 'majority' |\n",
       "| temperature | float | 2.0 | only used if pool='majority' |\n",
       "| n_linear_layers | int | 1 |  |\n",
       "| predict_every_n_patches | int | 1 |  |\n",
       "| bidirectional | bool | True |  |\n",
       "| affine | bool | False |  |\n",
       "| shared_embedding | bool | True |  |\n",
       "| augmentations | NoneType | None |  |\n",
       "| augmentation_mask_ratio | float | 0.0 |  |\n",
       "| augmentation_dims_to_shuffle | list | [1, 2, 3] |  |\n",
       "| norm | NoneType | None | one of [None, 'pre', 'post'] |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### RNNProbingHead\n",
       "\n",
       ">      RNNProbingHead (c_in, input_size, hidden_size, n_classes,\n",
       ">                      contrastive=False, module='GRU', linear_dropout=0.0,\n",
       ">                      rnn_dropout=0.0, num_rnn_layers=1, act='gelu',\n",
       ">                      pool='average', temperature=2.0, n_linear_layers=1,\n",
       ">                      predict_every_n_patches=1, bidirectional=True,\n",
       ">                      affine=False, shared_embedding=True, augmentations=None,\n",
       ">                      augmentation_mask_ratio=0.0,\n",
       ">                      augmentation_dims_to_shuffle=[1, 2, 3], norm=None)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  |  |\n",
       "| input_size |  |  |  |\n",
       "| hidden_size |  |  |  |\n",
       "| n_classes |  |  |  |\n",
       "| contrastive | bool | False |  |\n",
       "| module | str | GRU |  |\n",
       "| linear_dropout | float | 0.0 |  |\n",
       "| rnn_dropout | float | 0.0 |  |\n",
       "| num_rnn_layers | int | 1 |  |\n",
       "| act | str | gelu |  |\n",
       "| pool | str | average | 'average' or 'max' or 'majority' |\n",
       "| temperature | float | 2.0 | only used if pool='majority' |\n",
       "| n_linear_layers | int | 1 |  |\n",
       "| predict_every_n_patches | int | 1 |  |\n",
       "| bidirectional | bool | True |  |\n",
       "| affine | bool | False |  |\n",
       "| shared_embedding | bool | True |  |\n",
       "| augmentations | NoneType | None |  |\n",
       "| augmentation_mask_ratio | float | 0.0 |  |\n",
       "| augmentation_dims_to_shuffle | list | [1, 2, 3] |  |\n",
       "| norm | NoneType | None | one of [None, 'pre', 'post'] |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RNNProbingHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### RNNProbingHeadExperimental\n",
       "\n",
       ">      RNNProbingHeadExperimental (c_in, input_size, hidden_size, n_classes,\n",
       ">                                  contrastive=False, module='GRU',\n",
       ">                                  linear_dropout=0.0, rnn_dropout=0.0,\n",
       ">                                  num_rnn_layers=1, act='gelu', pool='average',\n",
       ">                                  temperature=2.0, predict_every_n_patches=1,\n",
       ">                                  bidirectional=True, affine=False,\n",
       ">                                  augmentations=None,\n",
       ">                                  augmentation_mask_ratio=0.0,\n",
       ">                                  augmentation_dims_to_shuffle=[1, 2, 3],\n",
       ">                                  pre_norm=True, mlp_final_head=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  |  |\n",
       "| input_size |  |  |  |\n",
       "| hidden_size |  |  |  |\n",
       "| n_classes |  |  |  |\n",
       "| contrastive | bool | False | deprecated |\n",
       "| module | str | GRU |  |\n",
       "| linear_dropout | float | 0.0 |  |\n",
       "| rnn_dropout | float | 0.0 |  |\n",
       "| num_rnn_layers | int | 1 |  |\n",
       "| act | str | gelu |  |\n",
       "| pool | str | average | 'average' or 'max' or 'majority' |\n",
       "| temperature | float | 2.0 | only used if pool='majority' |\n",
       "| predict_every_n_patches | int | 1 |  |\n",
       "| bidirectional | bool | True |  |\n",
       "| affine | bool | False |  |\n",
       "| augmentations | NoneType | None |  |\n",
       "| augmentation_mask_ratio | float | 0.0 |  |\n",
       "| augmentation_dims_to_shuffle | list | [1, 2, 3] |  |\n",
       "| pre_norm | bool | True | one of [None, 'pre', 'post'] |\n",
       "| mlp_final_head | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### RNNProbingHeadExperimental\n",
       "\n",
       ">      RNNProbingHeadExperimental (c_in, input_size, hidden_size, n_classes,\n",
       ">                                  contrastive=False, module='GRU',\n",
       ">                                  linear_dropout=0.0, rnn_dropout=0.0,\n",
       ">                                  num_rnn_layers=1, act='gelu', pool='average',\n",
       ">                                  temperature=2.0, predict_every_n_patches=1,\n",
       ">                                  bidirectional=True, affine=False,\n",
       ">                                  augmentations=None,\n",
       ">                                  augmentation_mask_ratio=0.0,\n",
       ">                                  augmentation_dims_to_shuffle=[1, 2, 3],\n",
       ">                                  pre_norm=True, mlp_final_head=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  |  |\n",
       "| input_size |  |  |  |\n",
       "| hidden_size |  |  |  |\n",
       "| n_classes |  |  |  |\n",
       "| contrastive | bool | False | deprecated |\n",
       "| module | str | GRU |  |\n",
       "| linear_dropout | float | 0.0 |  |\n",
       "| rnn_dropout | float | 0.0 |  |\n",
       "| num_rnn_layers | int | 1 |  |\n",
       "| act | str | gelu |  |\n",
       "| pool | str | average | 'average' or 'max' or 'majority' |\n",
       "| temperature | float | 2.0 | only used if pool='majority' |\n",
       "| predict_every_n_patches | int | 1 |  |\n",
       "| bidirectional | bool | True |  |\n",
       "| affine | bool | False |  |\n",
       "| augmentations | NoneType | None |  |\n",
       "| augmentation_mask_ratio | float | 0.0 |  |\n",
       "| augmentation_dims_to_shuffle | list | [1, 2, 3] |  |\n",
       "| pre_norm | bool | True | one of [None, 'pre', 'post'] |\n",
       "| mlp_final_head | bool | False |  |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RNNProbingHeadExperimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 30])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNProbingHeadExperimental(c_in=7, \n",
    "                                pool='average', \n",
    "                                input_size = 384, \n",
    "                                bidirectional=True,\n",
    "                                affine=False, \n",
    "                                hidden_size=1200,\n",
    "                                module='GRU',\n",
    "                                n_classes=4,\n",
    "                                predict_every_n_patches=32,\n",
    "                                rnn_dropout=0.,\n",
    "                                num_rnn_layers=1,\n",
    "                                linear_dropout=0.,\n",
    "                                mlp_final_head=False,\n",
    "                                pre_norm=True)\n",
    "x = torch.randn((4,7,384,960))\n",
    "sequence_padding_mask = torch.zeros(4,960)\n",
    "sequence_padding_mask[:,-32:] = 1\n",
    "m(x, return_softmax=True, sequence_padding_mask=sequence_padding_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 30])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNProbingHead(c_in=7, pool='majority', input_size = 384, contrastive=False, bidirectional=True, affine=True, shared_embedding=False, hidden_size=384, module='GRU', n_classes=4, predict_every_n_patches=32, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1, norm='post')\n",
    "x = torch.randn((4,7,384,960))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNProbingHead(c_in=7, input_size = 512, contrastive=True, bidirectional=True, affine=False, shared_embedding=True, hidden_size=256, module='GRU', n_classes=5, predict_every_n_patches=5, rnn_dropout=0., num_rnn_layers=1, linear_dropout=0., n_linear_layers=1)\n",
    "x = torch.randn((4,7,512*2,3600))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TransformerDecoderProbingHead\n",
       "\n",
       ">      TransformerDecoderProbingHead (c_in, d_model, n_classes,\n",
       ">                                     norm='BatchNorm', dropout=0.0, act='gelu',\n",
       ">                                     d_ff=2048, num_layers=1, n_heads=2,\n",
       ">                                     predict_every_n_patches=1, affine=False,\n",
       ">                                     shared_embedding=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TransformerDecoderProbingHead\n",
       "\n",
       ">      TransformerDecoderProbingHead (c_in, d_model, n_classes,\n",
       ">                                     norm='BatchNorm', dropout=0.0, act='gelu',\n",
       ">                                     d_ff=2048, num_layers=1, n_heads=2,\n",
       ">                                     predict_every_n_patches=1, affine=False,\n",
       ">                                     shared_embedding=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TransformerDecoderProbingHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerDecoderProbingHead(c_in=7, affine=True, shared_embedding=False, d_model=512, n_classes=5, dropout=0., num_layers=1, n_heads=2, predict_every_n_patches=5)\n",
    "x = torch.randn((4, 7, 512, 3600))\n",
    "\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DecoderFeedForward\n",
       "\n",
       ">      DecoderFeedForward (c_in, predict_every_n_patches, num_layers, d_ff,\n",
       ">                          attn_dropout, res_attention, pre_norm, store_attn,\n",
       ">                          n_heads, shared_embedding, affine, n_classes,\n",
       ">                          d_model, norm='BatchNorm', act='gelu', dropout=0.0)\n",
       "\n",
       "*transformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by\n",
       "a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| predict_every_n_patches |  |  | for a given sequence of length m with frequency f, number of predictions |\n",
       "| num_layers |  |  |  |\n",
       "| d_ff |  |  |  |\n",
       "| attn_dropout |  |  |  |\n",
       "| res_attention |  |  |  |\n",
       "| pre_norm |  |  |  |\n",
       "| store_attn |  |  |  |\n",
       "| n_heads |  |  |  |\n",
       "| shared_embedding |  |  |  |\n",
       "| affine |  |  |  |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| d_model |  |  | the dimension of the transformer model |\n",
       "| norm | str | BatchNorm | batchnorm or layernorm between linear and convolutional layers |\n",
       "| act | str | gelu | activation function to use between layers, 'gelu' or 'relu' |\n",
       "| dropout | float | 0.0 | dropout in between linear layers |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DecoderFeedForward\n",
       "\n",
       ">      DecoderFeedForward (c_in, predict_every_n_patches, num_layers, d_ff,\n",
       ">                          attn_dropout, res_attention, pre_norm, store_attn,\n",
       ">                          n_heads, shared_embedding, affine, n_classes,\n",
       ">                          d_model, norm='BatchNorm', act='gelu', dropout=0.0)\n",
       "\n",
       "*transformer decoder with attention for feedforward predictions. This is really just another encoder layer followed by\n",
       "a linear layer + 1d convolution + softmax. However, if used in linear probing, could be useful.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| predict_every_n_patches |  |  | for a given sequence of length m with frequency f, number of predictions |\n",
       "| num_layers |  |  |  |\n",
       "| d_ff |  |  |  |\n",
       "| attn_dropout |  |  |  |\n",
       "| res_attention |  |  |  |\n",
       "| pre_norm |  |  |  |\n",
       "| store_attn |  |  |  |\n",
       "| n_heads |  |  |  |\n",
       "| shared_embedding |  |  |  |\n",
       "| affine |  |  |  |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| d_model |  |  | the dimension of the transformer model |\n",
       "| norm | str | BatchNorm | batchnorm or layernorm between linear and convolutional layers |\n",
       "| act | str | gelu | activation function to use between layers, 'gelu' or 'relu' |\n",
       "| dropout | float | 0.0 | dropout in between linear layers |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DecoderFeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in = 7\n",
    "frequency = 125\n",
    "win_length=750 \n",
    "overlap = 0.\n",
    "hop_length=win_length - int(overlap*win_length)\n",
    "max_seq_len_sec = (6*3600) # for dataloader\n",
    "#seq_len_sec = sample_stride = 3*3600 # for dataloader\n",
    "max_seq_len = max_seq_len_sec*frequency # for model\n",
    "#n_patches = n_fft // 2 + 1\n",
    "n_patches = (max(max_seq_len, win_length)-win_length) // hop_length + 1\n",
    "\n",
    "#patch_len = int((win_length-conv_kernel_stride_size[1])/conv_kernel_stride_size[1] + 1)\n",
    "x = torch.randn(2,c_in,512,n_patches)\n",
    "\n",
    "model = DecoderFeedForward(c_in=c_in,\n",
    "                           predict_every_n_patches=5,\n",
    "                           num_layers=1,\n",
    "                           d_ff = 2048,\n",
    "                           attn_dropout=0.,\n",
    "                           res_attention = False,\n",
    "                           pre_norm = False,\n",
    "                           store_attn = False,\n",
    "                           n_heads=2,\n",
    "                           affine=False,\n",
    "                           shared_embedding=False,\n",
    "                           n_classes=5,\n",
    "                           d_model=512,\n",
    "                           norm='BatchNorm',\n",
    "                           act='gelu',\n",
    "                           dropout=0.\n",
    "                           )\n",
    "\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeDistributedConvolutionalFeedForward\n",
       "\n",
       ">      TimeDistributedConvolutionalFeedForward (c_in, frequency,\n",
       ">                                               predict_every_seconds,\n",
       ">                                               n_classes, win_length, d_model,\n",
       ">                                               affine=False,\n",
       ">                                               shared_embedding=True)\n",
       "\n",
       "*Convolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension.\n",
       "Then, a convolutional transpose is used to extrapolate the data to its original form.\n",
       "Finally, a final convolution is used to predict the classes.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| frequency |  |  | the frequency of the original channels |\n",
       "| predict_every_seconds |  |  | for a given sequence of length m with frequency f, number of predictions |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| win_length |  |  | the convolved patch length, the first step in this is to do a linear layer to this dimension |\n",
       "| d_model |  |  | the dimension of the transformer model |\n",
       "| affine | bool | False |  |\n",
       "| shared_embedding | bool | True |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeDistributedConvolutionalFeedForward\n",
       "\n",
       ">      TimeDistributedConvolutionalFeedForward (c_in, frequency,\n",
       ">                                               predict_every_seconds,\n",
       ">                                               n_classes, win_length, d_model,\n",
       ">                                               affine=False,\n",
       ">                                               shared_embedding=True)\n",
       "\n",
       "*Convolutional feed forward head that first uses a linear feed forward network to project features into the original convolutional dimension.\n",
       "Then, a convolutional transpose is used to extrapolate the data to its original form.\n",
       "Finally, a final convolution is used to predict the classes.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| frequency |  |  | the frequency of the original channels |\n",
       "| predict_every_seconds |  |  | for a given sequence of length m with frequency f, number of predictions |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| win_length |  |  | the convolved patch length, the first step in this is to do a linear layer to this dimension |\n",
       "| d_model |  |  | the dimension of the transformer model |\n",
       "| affine | bool | False |  |\n",
       "| shared_embedding | bool | True |  |"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TimeDistributedConvolutionalFeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LinearProbingHead\n",
       "\n",
       ">      LinearProbingHead (c_in, predict_every_n_patches, n_classes, input_size,\n",
       ">                         n_layers, num_patch, shared_embedding=True,\n",
       ">                         affine=True, norm='BatchNorm', act='gelu',\n",
       ">                         dropout=0.0)\n",
       "\n",
       "*A linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels in the original input |\n",
       "| predict_every_n_patches |  |  | for a given sequence of length m with frequency f, number of predictions |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| input_size |  |  | the dimension of the transformer model |\n",
       "| n_layers |  |  | the number of linear layers to use in the prediciton head, with RELU activation and dropout |\n",
       "| num_patch |  |  |  |\n",
       "| shared_embedding | bool | True | whether or not to have a dense layer per channel or one layer per channel |\n",
       "| affine | bool | True | include learnable parameters to weight predictions |\n",
       "| norm | str | BatchNorm | batchnorm or layernorm between linear and convolutional layers |\n",
       "| act | str | gelu | activation function to use between layers, 'gelu' or 'relu' |\n",
       "| dropout | float | 0.0 | dropout in between linear layers |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LinearProbingHead\n",
       "\n",
       ">      LinearProbingHead (c_in, predict_every_n_patches, n_classes, input_size,\n",
       ">                         n_layers, num_patch, shared_embedding=True,\n",
       ">                         affine=True, norm='BatchNorm', act='gelu',\n",
       ">                         dropout=0.0)\n",
       "\n",
       "*A linear probing head (with optional MLP), assumes that the d_model corresponds to a particular segment of time and will make a prediction per patch per channel, and average the results*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels in the original input |\n",
       "| predict_every_n_patches |  |  | for a given sequence of length m with frequency f, number of predictions |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| input_size |  |  | the dimension of the transformer model |\n",
       "| n_layers |  |  | the number of linear layers to use in the prediciton head, with RELU activation and dropout |\n",
       "| num_patch |  |  |  |\n",
       "| shared_embedding | bool | True | whether or not to have a dense layer per channel or one layer per channel |\n",
       "| affine | bool | True | include learnable parameters to weight predictions |\n",
       "| norm | str | BatchNorm | batchnorm or layernorm between linear and convolutional layers |\n",
       "| act | str | gelu | activation function to use between layers, 'gelu' or 'relu' |\n",
       "| dropout | float | 0.0 | dropout in between linear layers |"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(LinearProbingHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LinearProbingHead(c_in=7, \n",
    "                      input_size = 512, \n",
    "                      predict_every_n_patches=5,\n",
    "                      n_classes=5,\n",
    "                      n_layers=3,\n",
    "                      shared_embedding=True,\n",
    "                      affine=True,\n",
    "                      num_patch=3600,\n",
    "                      dropout=0.1)\n",
    "\n",
    "x = torch.randn((4,7,512,3600))\n",
    "\n",
    "m(x, return_softmax=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeDistributedFeedForward\n",
       "\n",
       ">      TimeDistributedFeedForward (c_in, n_classes, n_patches, d_model,\n",
       ">                                  pred_len_seconds, n_linear_layers,\n",
       ">                                  conv_kernel_stride_size, dropout=0.0)\n",
       "\n",
       "*Feed forward head that uses a convolutional layer to reduce channel dimensionality\n",
       "Followed by a feedforward network to make*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| n_patches |  |  | the number of stft or time patches |\n",
       "| d_model |  |  | the dimension of the transformer model |\n",
       "| pred_len_seconds |  |  | the sequence multiclass prediction length in seconds |\n",
       "| n_linear_layers |  |  | the number of linear layers to use in the prediciton head, with RELU activation and dropout |\n",
       "| conv_kernel_stride_size |  |  | the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here. |\n",
       "| dropout | float | 0.0 | dropout in between linear layers |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeDistributedFeedForward\n",
       "\n",
       ">      TimeDistributedFeedForward (c_in, n_classes, n_patches, d_model,\n",
       ">                                  pred_len_seconds, n_linear_layers,\n",
       ">                                  conv_kernel_stride_size, dropout=0.0)\n",
       "\n",
       "*Feed forward head that uses a convolutional layer to reduce channel dimensionality\n",
       "Followed by a feedforward network to make*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| n_classes |  |  | the number of classes to predict (for sleep stage - there are 6) |\n",
       "| n_patches |  |  | the number of stft or time patches |\n",
       "| d_model |  |  | the dimension of the transformer model |\n",
       "| pred_len_seconds |  |  | the sequence multiclass prediction length in seconds |\n",
       "| n_linear_layers |  |  | the number of linear layers to use in the prediciton head, with RELU activation and dropout |\n",
       "| conv_kernel_stride_size |  |  | the 1d convolution kernel size and stride, in seconds. If you want every 30 second predicitons, put 30 here. |\n",
       "| dropout | float | 0.0 | dropout in between linear layers |"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TimeDistributedFeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ConvBiGRU\n",
       "\n",
       ">      ConvBiGRU (input_size, hidden_sizes, kernel_sizes, n_layers, d_model,\n",
       ">                 predict_every_n_patches, n_classes)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ConvBiGRU\n",
       "\n",
       ">      ConvBiGRU (input_size, hidden_sizes, kernel_sizes, n_layers, d_model,\n",
       ">                 predict_every_n_patches, n_classes)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConvBiGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ConvGRU1D\n",
       "\n",
       ">      ConvGRU1D (input_size, hidden_sizes, kernel_sizes, n_layers)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ConvGRU1D\n",
       "\n",
       ">      ConvGRU1D (input_size, hidden_sizes, kernel_sizes, n_layers)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConvGRU1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ConvGRU1DCell\n",
       "\n",
       ">      ConvGRU1DCell (input_size, hidden_size, kernel_size)\n",
       "\n",
       "*Generate a convolutional GRU cell*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ConvGRU1DCell\n",
       "\n",
       ">      ConvGRU1DCell (input_size, hidden_size, kernel_size)\n",
       "\n",
       "*Generate a convolutional GRU cell*"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ConvGRU1DCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 720])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4,7,512,3600))\n",
    "\n",
    "convgru = ConvBiGRU(input_size=7, hidden_sizes=32, kernel_sizes=3, n_layers=1, d_model=512, predict_every_n_patches=5, n_classes=5)\n",
    "\n",
    "out = convgru(x)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
