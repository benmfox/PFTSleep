{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Potentially helpful layers for your models\n",
    "output-file: layers.html\n",
    "title: Layers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers for Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L23){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PatchEncoder\n",
       "\n",
       ">      PatchEncoder (c_in, patch_len, d_model, shared_embedding)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| c_in | the number of input channels |\n",
       "| patch_len | the length of the patches (either stft or interval length) |\n",
       "| d_model | the dimension of the initial linear layers for inputting patches into transformer |\n",
       "| shared_embedding | indicator of whether to project each channel individually or together |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L23){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PatchEncoder\n",
       "\n",
       ">      PatchEncoder (c_in, patch_len, d_model, shared_embedding)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| c_in | the number of input channels |\n",
       "| patch_len | the length of the patches (either stft or interval length) |\n",
       "| d_model | the dimension of the initial linear layers for inputting patches into transformer |\n",
       "| shared_embedding | indicator of whether to project each channel individually or together |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PatchEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PositionalEncoding\n",
       "\n",
       ">      PositionalEncoding (num_patch, d_model)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| num_patch | number of patches of time series or stft in input |\n",
       "| d_model | dimension of patch embeddings |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L60){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PositionalEncoding\n",
       "\n",
       ">      PositionalEncoding (num_patch, d_model)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| num_patch | number of patches of time series or stft in input |\n",
       "| d_model | dimension of patch embeddings |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PositionalEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tAPE\n",
       "\n",
       ">      tAPE (d_model:int, seq_len:int, scale_factor=1.0)\n",
       "\n",
       "*time Absolute Position Encoding\n",
       "Adapted from tsai*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| d_model | int |  | the embedding dimension |\n",
       "| seq_len | int |  | the max. length of the incoming sequence or num patches |\n",
       "| scale_factor | float | 1.0 | dropout:float=0., # dropout value |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tAPE\n",
       "\n",
       ">      tAPE (d_model:int, seq_len:int, scale_factor=1.0)\n",
       "\n",
       "*time Absolute Position Encoding\n",
       "Adapted from tsai*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| d_model | int |  | the embedding dimension |\n",
       "| seq_len | int |  | the max. length of the incoming sequence or num patches |\n",
       "| scale_factor | float | 1.0 | dropout:float=0., # dropout value |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(tAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask and Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L115){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Mask\n",
       "\n",
       ">      Mask (mask_type, mask_ratio, return_mask=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L115){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Mask\n",
       "\n",
       ">      Mask (mask_type, mask_ratio, return_mask=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 0.0000,  0.0000,  0.0000,  1.0975,  0.0000,  1.4248,  0.0000, -0.1104,\n",
       "           1.1865]),\n",
       "  tensor(8)),\n",
       " (tensor([ 1.1462, -1.8379, -0.2368,  2.0488,  0.0000,  0.0000, -0.3927, -1.0523,\n",
       "           0.1294]),\n",
       "  tensor(4)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(125)\n",
    "m = Mask(mask_type='jitter_zero', mask_ratio=0.5)\n",
    "x = torch.randn((9))\n",
    "\n",
    "m(x), m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0000,  1.0000,  1.9390, -0.0338]),\n",
       " tensor([-1.7385, -0.5780,  1.9390, -0.0338]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4))\n",
    "x_aug = x.clone()\n",
    "mask = torch.tensor([True,True,False,False])\n",
    "\n",
    "x_aug[mask] = 1\n",
    "x_aug, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PatchAugmentations\n",
       "\n",
       ">      PatchAugmentations (augmentations=['patch_mask', 'jitter_zero_mask',\n",
       ">                          'reverse_sequence', 'shuffle_channels'],\n",
       ">                          patch_mask_ratio=0.0, jitter_zero_mask_ratio=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PatchAugmentations\n",
       "\n",
       ">      PatchAugmentations (augmentations=['patch_mask', 'jitter_zero_mask',\n",
       ">                          'reverse_sequence', 'shuffle_channels'],\n",
       ">                          patch_mask_ratio=0.0, jitter_zero_mask_ratio=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PatchAugmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3600, 7, 750])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,3600,7,750)\n",
    "\n",
    "s=PatchAugmentations(patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\n",
    "s(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EmbeddingAugmentations\n",
       "\n",
       ">      EmbeddingAugmentations (augmentations=['shuffle_dims',\n",
       ">                              'jitter_zero_mask', 'patch_mask'],\n",
       ">                              dims_to_shuffle=[1, 2, 3], patch_mask_ratio=0.0,\n",
       ">                              jitter_zero_mask_ratio=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EmbeddingAugmentations\n",
       "\n",
       ">      EmbeddingAugmentations (augmentations=['shuffle_dims',\n",
       ">                              'jitter_zero_mask', 'patch_mask'],\n",
       ">                              dims_to_shuffle=[1, 2, 3], patch_mask_ratio=0.0,\n",
       ">                              jitter_zero_mask_ratio=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EmbeddingAugmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 512, 3600])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,7,512,3600)\n",
    "\n",
    "s = EmbeddingAugmentations(augmentations=['jitter_zero_mask'], dims_to_shuffle=[1], patch_mask_ratio=0.1, jitter_zero_mask_ratio=0.1)\n",
    "s(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch and Fourier Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L212){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Patch\n",
       "\n",
       ">      Patch (patch_len, stride, max_seq_len=None)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L212){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Patch\n",
       "\n",
       ">      Patch (patch_len, stride, max_seq_len=None)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L224){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### STFT\n",
       "\n",
       ">      STFT (n_fft, win_length, hop_length, stft_norm, decibel_scale,\n",
       ">            channel_stft_means=None, channel_stft_stds=None,\n",
       ">            pad_win_length_to_nfft=True, pad_mode='reflect', center=False,\n",
       ">            return_complex=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L224){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### STFT\n",
       "\n",
       ">      STFT (n_fft, win_length, hop_length, stft_norm, decibel_scale,\n",
       ">            channel_stft_means=None, channel_stft_stds=None,\n",
       ">            pad_win_length_to_nfft=True, pad_mode='reflect', center=False,\n",
       ">            return_complex=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(STFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FFT\n",
       "\n",
       ">      FFT (dim=-1, norm='backward')\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dim | int | -1 | dimension to calculate fft over |\n",
       "| norm | str | backward | \"forward\" - normalize by 1/n, \"backward\" - no normalization, \"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FFT\n",
       "\n",
       ">      FFT (dim=-1, norm='backward')\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dim | int | -1 | dimension to calculate fft over |\n",
       "| norm | str | backward | \"forward\" - normalize by 1/n, \"backward\" - no normalization, \"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reversible Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L294){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RevIN\n",
       "\n",
       ">      RevIN (num_features:int, eps=1e-05, dim_to_reduce=-1, affine=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_features | int |  | the number of channels or features in the input |\n",
       "| eps | float | 1e-05 | added to avoid division by zero errors |\n",
       "| dim_to_reduce | int | -1 | the dimension to reduce, |\n",
       "| affine | bool | True | learning affine parameters bias and weight per channel |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L294){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RevIN\n",
       "\n",
       ">      RevIN (num_features:int, eps=1e-05, dim_to_reduce=-1, affine=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_features | int |  | the number of channels or features in the input |\n",
       "| eps | float | 1e-05 | added to avoid division by zero errors |\n",
       "| dim_to_reduce | int | -1 | the dimension to reduce, |\n",
       "| affine | bool | True | learning affine parameters bias and weight per channel |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RevIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 1000]),\n",
       " torch.Size([4, 7, 1000]),\n",
       " torch.Size([4, 7, 1000]),\n",
       " torch.Size([4, 7, 1]),\n",
       " torch.Size([4, 7, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,7,1000)\n",
    "\n",
    "revin = RevIN(x.shape[1], dim_to_reduce=-1, affine=True)\n",
    "x_norm = revin(x, mode=True)\n",
    "x_denorm = revin(x_norm, mode=False)\n",
    "\n",
    "x.shape, x_norm.shape, x_denorm.shape, revin.mean.shape, revin.stdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L344){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MultiheadFlashAttention\n",
       "\n",
       ">      MultiheadFlashAttention (d_model:int, n_heads:int, qkv_bias:bool=True,\n",
       ">                               is_causal:bool=False, attn_dropout:float=0.0,\n",
       ">                               proj_dropout:float=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L344){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MultiheadFlashAttention\n",
       "\n",
       ">      MultiheadFlashAttention (d_model:int, n_heads:int, qkv_bias:bool=True,\n",
       ">                               is_causal:bool=False, attn_dropout:float=0.0,\n",
       ">                               proj_dropout:float=0.0)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MultiheadFlashAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 100, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiheadFlashAttention(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0.)\n",
    "x = torch.randn(2*7,100,512) # [bs * n_vars x n_patches x d_model]\n",
    "key_padding_mask = torch.zeros(2*7, 100, dtype=torch.bool)\n",
    "key_padding_mask[:, -2:] = True  # mask last 2 positions\n",
    "output = mha(x, key_padding_mask=key_padding_mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L431){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (d_model, n_heads, attn_dropout=0.0,\n",
       ">                                 res_attention=False, lsa=False)\n",
       "\n",
       "*Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
       "(Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
       "by Lee et al, 2021)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L431){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (d_model, n_heads, attn_dropout=0.0,\n",
       ">                                 res_attention=False, lsa=False)\n",
       "\n",
       "*Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
       "(Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
       "by Lee et al, 2021)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ScaledDotProductAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L494){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MultiheadAttentionCustom\n",
       "\n",
       ">      MultiheadAttentionCustom (d_model, n_heads, d_k=None, d_v=None,\n",
       ">                                res_attention=False, attn_dropout=0.0,\n",
       ">                                proj_dropout=0.0, qkv_bias=True, lsa=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L494){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MultiheadAttentionCustom\n",
       "\n",
       ">      MultiheadAttentionCustom (d_model, n_heads, d_k=None, d_v=None,\n",
       ">                                res_attention=False, attn_dropout=0.0,\n",
       ">                                proj_dropout=0.0, qkv_bias=True, lsa=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MultiheadAttentionCustom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttentionCustom(\n",
       "  (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (sdp_attn): ScaledDotProductAttention(\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (to_out): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_attn = MultiheadAttentionCustom(d_model=512, n_heads=8, attn_dropout=0., proj_dropout=0., res_attention=False)\n",
    "mha_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom output shape: torch.Size([2, 10, 64])\n",
      "Flash output shape: torch.Size([2, 10, 64])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "def test_attention_equivalence():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "    \n",
    "    # Create input tensor (only need one since we're using self-attention)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create key padding mask\n",
    "    key_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)\n",
    "    key_padding_mask[:, -2:] = True  # mask last 2 positions\n",
    "\n",
    "    # Initialize both implementations\n",
    "    custom_mha = MultiheadAttentionCustom(d_model=d_model, n_heads=n_heads)\n",
    "    flash_mha = MultiheadFlashAttention(d_model=d_model, n_heads=n_heads)\n",
    "    \n",
    "    # Set both models to eval mode to disable dropout\n",
    "    custom_mha.eval()\n",
    "    flash_mha.eval()\n",
    "    \n",
    "    # Copy weights to ensure identical parameters\n",
    "    # Combine QKV weights from custom implementation into single matrix for flash attention\n",
    "    combined_weight = torch.cat([\n",
    "        custom_mha.W_Q.weight,\n",
    "        custom_mha.W_K.weight,\n",
    "        custom_mha.W_V.weight\n",
    "    ], dim=0)\n",
    "    combined_bias = torch.cat([\n",
    "        custom_mha.W_Q.bias,\n",
    "        custom_mha.W_K.bias,\n",
    "        custom_mha.W_V.bias\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Copy combined weights to flash attention\n",
    "    flash_mha.c_attn.weight.data = combined_weight\n",
    "    flash_mha.c_attn.bias.data = combined_bias\n",
    "    \n",
    "    # Output projection weights\n",
    "    flash_mha.c_proj.weight.data = custom_mha.to_out[0].weight.data.clone()\n",
    "    flash_mha.c_proj.bias.data = custom_mha.to_out[0].bias.data.clone()\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        custom_output, custom_attn = custom_mha(x, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        flash_output = flash_mha(x, attn_mask=key_padding_mask)\n",
    "    \n",
    "    # Compare outputs\n",
    "    print(f\"Custom output shape: {custom_output.shape}\")\n",
    "    print(f\"Flash output shape: {flash_output.shape}\")\n",
    "    \n",
    "    output_close = torch.allclose(custom_output, flash_output, rtol=0, atol=0)\n",
    "    print(f\"Outputs match: {output_close}\")\n",
    "    \n",
    "    if not output_close:\n",
    "        print(\"\\nOutput differences:\")\n",
    "        print(f\"Max difference: {(custom_output - flash_output).abs().max().item()}\")\n",
    "        print(f\"Mean difference: {(custom_output - flash_output).abs().mean().item()}\")\n",
    "    \n",
    "    return custom_output, flash_output\n",
    "\n",
    "custom_output, flash_output = test_attention_equivalence()\n",
    "#: 8.940696716308594e-08\n",
    "#Mean difference: 1.0550138540565968e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input shape: torch.Size([1, 7, 10799, 500])\n",
      "Projected X shape to d_model: torch.Size([1, 7, 10799, 512])\n",
      "Reshape for attention: torch.Size([7, 10799, 512])\n",
      "\n",
      "Testing MHA and SDA attention, with just 50 elements.\n",
      "MHA attention output shape: torch.Size([7, 50, 512]), mha attn weight shape: torch.Size([7, 8, 50, 50])\n",
      "Q shape: torch.Size([1, 8, 75593, 64]), K shape: torch.Size([1, 8, 64, 75593]), V shape: torch.Size([1, 8, 75593, 64])\n",
      "Attn output shape torch.Size([1, 50, 512]), attn weight shape: torch.Size([1, 8, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "d_model=512\n",
    "n_heads=8\n",
    "d_k = d_v = d_model // n_heads\n",
    "attn = ScaledDotProductAttention(d_model=d_model, n_heads=n_heads)\n",
    "mha_attn = MultiheadAttentionCustom(d_model, n_heads)\n",
    "\n",
    "W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "X,_,_ = ds[0]\n",
    "\n",
    "X = create_patch(X, patch_len=(10*50), stride=(5*50), constant_pad=True)\n",
    "\n",
    "patch_len = X.shape[-1]\n",
    "\n",
    "X = X[None, ...].permute(0,2,1,3)  # simulate batch size of 1 [bs x n_vars x num_patch x patch_len]\n",
    "\n",
    "print(f'X input shape: {X.shape}')\n",
    "W_P = nn.Linear(patch_len, d_model)\n",
    "\n",
    "X = W_P(X) # project to d_model\n",
    "print(f\"Projected X shape to d_model: {X.shape}\")\n",
    "\n",
    "X = torch.reshape(X, (X.shape[0]*X.shape[1],X.shape[2],X.shape[3]))\n",
    "print(f\"Reshape for attention: {X.shape}\")\n",
    "\n",
    "# test multihead attention\n",
    "print(\"\\nTesting MHA and SDA attention, with just 50 elements.\")\n",
    "mha_output, mha_attn_weights = mha_attn(Q=X[:,:50,:])\n",
    "print(f\"MHA attention output shape: {mha_output.shape}, mha attn weight shape: {mha_attn_weights.shape}\")\n",
    "\n",
    "# test scaled dot product attn\n",
    "K = Q = V = X\n",
    "\n",
    "# # Linear (+ split in multiple heads)\n",
    "bs = 1 # 1 * 16\n",
    "q_s = W_Q(Q).reshape(bs, -1, n_heads, d_k).transpose(1, 2)\n",
    "k_s = W_K(K).reshape(bs, -1, n_heads, d_k).permute(0, 2, 3, 1)\n",
    "v_s = W_V(V).reshape(bs, -1, n_heads, d_v).transpose(1, 2)\n",
    "print(f\"Q shape: {q_s.shape}, K shape: {k_s.shape}, V shape: {v_s.shape}\")\n",
    "\n",
    "to_out = nn.Linear(n_heads * d_v, d_model)\n",
    "output, attn_weights = attn(q_s[:,:,:50,:],k_s[:,:,:,:50], v_s[:,:,:50,:])\n",
    "output = output.transpose(1, 2).contiguous().view(bs, -1, n_heads * d_v)\n",
    "print(f\"Attn output shape {output.shape}, attn weight shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L546){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Attention_Rel_Scl\n",
       "\n",
       ">      Attention_Rel_Scl (d_model:int, n_heads:int, seq_len:int, d_k:int=None,\n",
       ">                         d_v:int=None, res_attention:bool=False,\n",
       ">                         attn_dropout:float=0.0, lsa:bool=False,\n",
       ">                         proj_dropout:float=0.0, qkv_bias:bool=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| d_model | int |  | Embedding dimension |\n",
       "| n_heads | int |  | number of attention heads |\n",
       "| seq_len | int |  | sequence length or num patches |\n",
       "| d_k | int | None | key dimension |\n",
       "| d_v | int | None | value dimension |\n",
       "| res_attention | bool | False | whether to use residual attention |\n",
       "| attn_dropout | float | 0.0 | dropout for attention |\n",
       "| lsa | bool | False | whether to use LSA, trainable paramater for scaling |\n",
       "| proj_dropout | float | 0.0 | dropout for projection |\n",
       "| qkv_bias | bool | True | bias for q, k, v |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L546){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Attention_Rel_Scl\n",
       "\n",
       ">      Attention_Rel_Scl (d_model:int, n_heads:int, seq_len:int, d_k:int=None,\n",
       ">                         d_v:int=None, res_attention:bool=False,\n",
       ">                         attn_dropout:float=0.0, lsa:bool=False,\n",
       ">                         proj_dropout:float=0.0, qkv_bias:bool=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| d_model | int |  | Embedding dimension |\n",
       "| n_heads | int |  | number of attention heads |\n",
       "| seq_len | int |  | sequence length or num patches |\n",
       "| d_k | int | None | key dimension |\n",
       "| d_v | int | None | value dimension |\n",
       "| res_attention | bool | False | whether to use residual attention |\n",
       "| attn_dropout | float | 0.0 | dropout for attention |\n",
       "| lsa | bool | False | whether to use LSA, trainable paramater for scaling |\n",
       "| proj_dropout | float | 0.0 | dropout for projection |\n",
       "| qkv_bias | bool | True | bias for q, k, v |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Attention_Rel_Scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "d_model=16\n",
    "c_in = 2\n",
    "seq_len = 1*360*10\n",
    "x = torch.randn(4,c_in,seq_len)\n",
    "embed_layer = nn.Sequential(nn.Conv2d(1, d_model*4, kernel_size=[1, 7], padding='same'), nn.BatchNorm2d(d_model*4), nn.GELU())\n",
    "embed_layer2 = nn.Sequential(nn.Conv2d(d_model*4, d_model, kernel_size=[c_in, 1], padding='valid'), nn.BatchNorm2d(d_model), nn.GELU())\n",
    "abs_position = tAPE(d_model, seq_len=seq_len)\n",
    "x_emb = embed_layer2(embed_layer(x.unsqueeze(1))).squeeze(2)\n",
    "x_emb = x_emb.permute(0,2,1)\n",
    "x_emb_pos = abs_position(x_emb)\n",
    "\n",
    "model = Attention_Rel_Scl(d_model=d_model,\n",
    "        n_heads=2, # number of attention heads\n",
    "        seq_len=seq_len, # sequence length or num patches\n",
    "        )\n",
    "\n",
    "out, attn_weights = model(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## test w patches [bs *c_in x num_patches x d_model]\n",
    "d_model=512\n",
    "c_in = 2\n",
    "num_patches = 10\n",
    "x_emb = torch.randn(4*c_in,num_patches, d_model)\n",
    "abs_position = tAPE(d_model, seq_len=num_patches)\n",
    "x_emb_pos = abs_position(x_emb)\n",
    "\n",
    "model = Attention_Rel_Scl(d_model=d_model,\n",
    "        n_heads=2, # number of attention heads\n",
    "        seq_len=num_patches, # sequence length or num patches\n",
    "        )\n",
    "\n",
    "out, attn_weights = model(x_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L672){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MaskedAutogressionFeedForward2\n",
       "\n",
       ">      MaskedAutogressionFeedForward2 (d_model, patch_len, n_layers, dropout)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L672){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MaskedAutogressionFeedForward2\n",
       "\n",
       ">      MaskedAutogressionFeedForward2 (d_model, patch_len, n_layers, dropout)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MaskedAutogressionFeedForward2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L635){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MaskedAutogressionFeedForward\n",
       "\n",
       ">      MaskedAutogressionFeedForward (c_in, patch_len, d_model,\n",
       ">                                     shared_recreation=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| patch_len |  |  | the length of the patches (either stft or interval length) |\n",
       "| d_model |  |  | the dimension of the initial linear layers for inputting patches into transformer |\n",
       "| shared_recreation | bool | True | indicator of whether to project each channel individually or together |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L635){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MaskedAutogressionFeedForward\n",
       "\n",
       ">      MaskedAutogressionFeedForward (c_in, patch_len, d_model,\n",
       ">                                     shared_recreation=True)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| c_in |  |  | the number of input channels |\n",
       "| patch_len |  |  | the length of the patches (either stft or interval length) |\n",
       "| d_model |  |  | the dimension of the initial linear layers for inputting patches into transformer |\n",
       "| shared_recreation | bool | True | indicator of whether to project each channel individually or together |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MaskedAutogressionFeedForward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L696){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Identity\n",
       "\n",
       ">      Identity ()\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L696){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Identity\n",
       "\n",
       ">      Identity ()\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L712){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_activation_fn\n",
       "\n",
       ">      get_activation_fn (activation)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L712){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_activation_fn\n",
       "\n",
       ">      get_activation_fn (activation)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(get_activation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L703){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Transpose\n",
       "\n",
       ">      Transpose (*dims, contiguous=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/benmfox/PFTSleep/blob/main/pftsleep/layers.py#L703){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Transpose\n",
       "\n",
       ">      Transpose (*dims, contiguous=False)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Transpose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
